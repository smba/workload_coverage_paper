\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{blindtext}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\usepackage{mdframed}

\newcommand{\greybox}[1]{
	\begin{mdframed}[backgroundcolor=black!0,linewidth=1pt,innerleftmargin=5pt,innertopmargin=5pt]
		%\noindent
		#1
	\end{mdframed}
}

%\setlength\parindent{0pt}
\begin{document}

\title{Workload Performance Variation meets\\ Dynamic White-Box Code Analysis}

\author{

%\IEEEauthorblockN{Stefan Mühlbauer, Norbert Siegmund}
%\IEEEauthorblockA{\textit{Leipzig University} \\
%	Leipzig, Germany}
%\and
%\IEEEauthorblockN{Florian Sattler, Christian Kaltenecker, Sven Apel}
%\IEEEauthorblockA{\textit{Saarland University,} \\%
%	\textit{Saarland Informatics Campus}\\
%	Saarbrücken, Germany}
}
\maketitle

\begin{abstract}
\ldots
\end{abstract}

%\begin{IEEEkeywords}
%component, formatting, style, styling, insert
%\end{IEEEkeywords}

\section{Introduction}
\subsection{Context}
Most software systems in today‘s age are be configured to meet user demands. The selection of configuration options can enable or disable desired functionality as well as tweak non-functional aspects of a software system, such as improving performance or energy consumption. All too often, software performance bugs can be linked to configuration options provided by a software system\cite{han_empirical_2016}. As performance bugs are conceived harder to fix than purely functional bugs, they increase the overall cost of missed or deferred performance regression testing. The relationship of configuration choices and resulting effects on non-functional properties has been extensively studied in recent years. The main focus here is set on estimating non-functional properties for arbitrary configurations~\cite{siegmund_performance-influence_2015,haDeepPerf2019,guo__2013,guo_2018_data,sarkarCostEfficientSamplingPerformance} as well as finding configurations which optimize one or more non-functional properties~\cite{nairUsingBadLearners2017,nairFlash18,ohFindingNearoptimalConfigurations2017}. Practitioners with \emph{representative} predictive models at hand can make informed decisions when integrating or deploying configurable software systems. Given that software performance is an often overlooked, but nonetheless decisive factor for software quality, predictive models are only useful to practitioners if predictions hold for a large number of application scenarios.

\subsection{Motivation}
Learning predictive models for software performance usually bases on a training set of configuration-specific observations. Such experiments employ a single workload that corresponds to a specific application scenario. While existing work has yielded effective and efficient learning approaches, in essence, predictive models obtained this way can only target workload-specific performance~\cite{siegmund_performance-influence_2015,haDeepPerf2019,guo__2013,guo_2018_data,sarkarCostEfficientSamplingPerformance}. The choice of a single workload can introduce additional bias as the predictive model is only as useful as close as the underlying workload resembles different use cases in in the wild. Pereira et al. illustrate this fact with an analysis of the video transcoder \textsc{x264} and different workloads~\cite{alves_sampling_2020}. The authors observed a wide variation in the performance behavior across a large set of different configurations when encoding different workloads. In case a changed workload is the main factor varied in an execution environment, such bias can be explicitly learned~\cite{jamishidi_transfer_2017,jamshidi_learning_2018,jamshidi_transfer_gp_2017,ding_bayesian_2020}. Transformed performance-influence models, however, only rather adapt to a specific environment change exploiting similarities between corresponding performance prediction models.

\subsection{Problem Statement}
We aim at opening a new perspective on choosing a workload for performance prediction modeling, as so far, the view on workload variation is a black-box perspective, where existing approaches require explicit measurements~\cite{jamishidi_transfer_2017,jamshidi_learning_2018,jamshidi_transfer_gp_2017} or are entirely agnostic of potential worklaod bias~\cite{liao_2020_using_emse}. To circumvent the costly and tedious task of performance measurement, we ask the following question: Can the effect of different benchmarks on performance prediction models can be \emph{inferred} based on analysis of code artifacts --- a task widely used in CI/CD tool chains, for instance, in the form of code coverage testing?

In this paper, we propose a lightweight approach that estimates potential bias of workload on different selectable software features. We are using lightweight, dynamic methods known under the umbrella term \emph{software reconnaissance} to identify feature-specific code and correlate differences in performance, as well as modeled performance influence to workload-specific differences in feature code coverage. To this end, we aim at answering the following research questions:

\begin{enumerate}
	\item Do workloads bias the \emph{expressiveness} of performance prediction models for software configuration options?
	\item Do workloads bias the \emph{feature code coverage} within configurable software systems?
	\item Can we \emph{infer} benchmark-specific bias from differences in feature code coverage?
\end{enumerate}


\section{Scope and Background}

\subsection{Performance Prediction Modeling}
\begin{itemize}
	\item performance prediction\cite{siegmund_performance-influence_2015,haDeepPerf2019,guo__2013,guo_2018_data,sarkarCostEfficientSamplingPerformance}
	\item performance optimization~\cite{nairUsingBadLearners2017,nairFlash18,ohFindingNearoptimalConfigurations2017}
\end{itemize}
\subsection{Workload Variation}
\begin{itemize}
	\item Micro-benchmarks?
	\item \textsc{x264}~\cite{alves_sampling_2020}, varying workloads for prediction~\cite{liao_2020_using_emse}
\end{itemize}
\subsection{Feature Location}
\begin{itemize}
	\item Static code analysis\cite{velez_2020_configcrusher_jase,lillack_2018_lotrack_tse,luo_2019_cova}
	\item Dynamic code analysis \cite{bell_phosphor_2014,velez_comprex_2021}
	\item Static program slicing and dynamic slicing (dicing)
	\item Software reconnaissance ~\cite{wong_integrated_2005,wilde_early_1996,agrawal_fault_1995,simmons_industrial_2006,chen_dynamic_nodate,sherwood_reducing_nodate}
\end{itemize}

\section{Mapping Code to Features and Workloads}
\begin{itemize}
	\item Software features are \emph{intrinsic} variability, often encoded as traceable program options, whereas workloads are \emph{extrinsic} factors, which cannot be explicitly traced in a software system.
	\item Extrinsic factors exclude the use of code analysis tracing data and control-flow. Intrinsic factors, however can be traced using static or dynamic code analysis. 
	We choose software reconnaissance as a white-box approximation to \emph{feature location}, because we can incorporate variability of both sources into one framework at the price of precision. Moreover, code analysis only works so far as program features are mapped to variables. For options with more semantic depth, for instance numerical options or \emph{fast/slow}-type options, software systems need to be refactored for more complex program analysis to be applicable.
	\item We investigate this trade-off in the discussion and review existing feature location approaches in the related work section.
\end{itemize}


\section{Inferring Worklaod Bias}
\subsection{Identifying }
We use dynamic methods described under the umbrella term \emph{software reconnaissance}~\cite{wilde_reconnaissance_1995}.



\section{Evaluation}
To evaluate our approach, we ask the following research questions.

\subsection{Research Questions}

\subsubsection{Do workloads bias the expressiveness of performance prediction models for software configuration options?}
\subsubsection{Do workloads bias the \emph{feature code coverage} within configurable software systems?}
\subsubsection{Can we infer benchmark-specific bias from differences in feature code coverage?}

\subsection{Subject Systems}
\begin{table}
	\centering
	\caption{Subject systems}
	\begin{tabular}{llrr}
		\toprule
		\textbf{Software System} & \textbf{Type} & \textbf{\# Options} & \textbf{\# Configurations} \\
		\midrule
		\textsc{h2} & Database & 16 & $2^{16}$\\
		\textsc{hsqldb} & Database & 16 & $2^{16}$\\
		\textsc{Apache Derby} & Database & 16 & $2^{16}$\\
		%\textsc{trinoDB} & Database System & 16 & $2^{16}$\\
		\textsc{BerkelyDB} & Database & 16 & $2^{16}$\\
		\textsc{prevayler} & Database & 16 & $2^{16}$\\
		\midrule
		\textsc{JCode} & Transcoder & 16 & $2^{16}$\\
		\textsc{OWASP} & Transcoder & 16 & $2^{16}$\\
		\midrule
		\textsc{Kanzi} & Compressor & 16 & $2^{16}$\\
		\textsc{xz-java} & Compressor & 16 & $2^{16}$\\
		\textsc{MiGz} &  Compressor & 16 & $2^{16}$\\
		\bottomrule
	\end{tabular}
\end{table}

\subsection{Subject Workloads}
OLTP-Bench collection~\cite{difallah_oltp_2013}

\subsection{Results}

\section{Discusison}
\section{Threats to Validity}
\subsection{Internal Validity}
\subsection{External Validity}
\subsection{Construct Validity}
\section{Related Work}
\subsection{White-Box Performance Models}
ConfigCrusher~\cite{velez_2020_configcrusher_jase} and Comprex~\cite{velez_comprex_2021}

Method-level white-box performance models~\cite{weber_white_2021}
\subsection{Transfer Learning}
Differences between versions\cite{muehlbauer_identifying_2020}

Differences between environments~\cite{jamishidi_transfer_2017,jamshidi_learning_2018,jamshidi_transfer_gp_2017,muehlbauer_identifying_2020}
\subsection{Workload Variation}
Performance under varying workloads~\cite{liao_2020_using_emse}
\section{Conclusion}

\clearpage
\bibliographystyle{IEEEtran}
\bibliography{literature.bib}


\end{document}
