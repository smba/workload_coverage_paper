\begin{abstract}
Most modern software systems provide configuration
options to customize functionality to meet user demands. Even
from a small number of configuration options, the space of
possible configurations grows large due to combinatorial explosion.
Hence, it is impractical to assess software quality exhaustively for
all configurations. Non-functional properties such as software
performance often depend on configuration choices and can be
learned from a sample set of configurations using performance
models. Existing performance modeling approaches often do
not consider variation beyond configurations, most
importantly, the choice of \emph{workload}. 
Recent work has demonstrated that varying workloads introduces further variation in the observed performance behavior, which makes it non-trivial to learn performance models that generalize to a wider range of possible scenarios. In practice, this limits the \emph{representativeness} of such performance models and raises the question of how different workloads influence the performance behavior.

In this paper, we address this question and conduct an empirical study of a multitude of configurations and workloads across a selection of configurable software systems.
Aside from black-box performance measurements, we enrich our observations with statement coverage data to obtain deeper insights into the relation of different configurations executions and the performance behavior emerging from it. {\color{red} We found that the software systems studied exhibit \ldots. }
With this work, we aim at raising awareness of multi-factor influences on software performance and pave an avenue to selecting more representative worklaods and, ultimately, enabling practicioners to learn more robust and generalizable performance models.

\end{abstract}
