\begin{abstract}
Most modern software systems exhibit configuration options to customize functionality to meet user demands. 
Even from a small number of configuration options, the space of possible configurations grows large due to combinatorial explosion. Hence, it is infeasible to assess software quality exhaustively for all configurations. 
Non-functional properties such as software performance often depend on configuration choices and can be learned from a sample set configurations using performance models. Existing performance modeling approaches often do not consider further variation beyond configurations, most importantly, the choice of workload.
For code sections that are not stressed by a workload, one cannot determine the influence they might have on performance under different workloads. This leaves performance models susceptible to bias by the choice of the workload, so that a model can over-approximate used during learning and may not generalize to real-world use cases.

In this paper, we explore the influence of different workloads on {\color{Red}four} different non-functional properties across {\color{Red}seven} configurable Java software systems. We augment performance measurements with dynamic code analysis to assess the feature coverage and representativeness of performance models in the presence of varying workloads. We devise a novel strategy to interpret performance models via lightweight code analysis and aim to raise awareness of multi-factor influences on software performance.
\end{abstract}


\section{Introduction}
\subsection*{Performance models are important}
Most software systems can be customized via configuration options to meet user demands. The selection of configuration options can enable desired functionality (features) or tweak non-functional aspects of a software system, such as improving performance or energy consumption. 
All too often, software performance bugs can be linked to configuration options~\cite{han_empirical_2016}. 
The relationship of configuration choices and their influence on non-functional properties has been extensively studied in recent years. The main focus was on the estimation non-functional properties, such as performance, for arbitrary configurations~\cite{dorn2020,siegmundPerformanceinfluenceModelsHighly2015,haDeepPerf2019,perfAL,guoVariabilityawarePerformancePrediction2013,sarkarCostEfficientSamplingPerformance,guo_2018_data,fourier_learning_2015,perLasso} or on finding configurations with near-optimal non-functional properties~\cite{nairUsingBadLearners2017,nairFlash18,ohFindingNearoptimalConfigurations2017}. 
The backbone of a performance estimate is a model that maps a given configuration to the estimated performance value. 
In general, performance prediction models are only useful to practitioners if predictions hold for a large number of application scenarios.

\subsection*{Performance models can be workload-biased}
Learning performance models for software performance usually relies on a training set of configuration-specific performance measurements. 
Typically, established approaches employ a single workload (e.g., a benchmark or test suite) to measure performance, which usually aims at emulating a real-world application scenario. 
However, as the collected measurements represent only the one application scenario, the resulting performance model can  estimate performance for this single scenario reliably. 
The choice of a single workload can introduce bias as the predictive model is only as useful as the underlying workload resembles a wide range of use cases in the real world. 
Pereira et al. illustrate this fact with an analysis of the video encoder \textsc{x264} under different workloads~\cite{alves_sampling_2020}. 
For two different workloads, they observed vastly different performance distributions across a large set of configurations.
While this particular observation can be an outlier, in general, one cannot assume that performance behavior under a single workload is generalizable, and therefore consistent in every real-world setting.
In this vein, a recent exploratory study by Jamshidi et al.\cite{jamishidi_transfer_2017} illustrates that performance models of a configurable software system often exhibit differences in estimated performance influences when learned in different environments, including different versions, hardware setups, and workloads.
Performance  influences across different environments remain largely congruent and differences only affect small numbers of configuration options, such that differences can be learned efficiently via transfer learning~\cite{jamshidi_transfer_gp_2017,jamshidi_learning_2018}. 
Nonetheless, under a single workload, we are left with performance models whose estimations are potentially inaccurate for different environments.

\subsection*{Towards Estimating Representativeness}
Given not one, but a set of performance models based on different workloads, the question arises: Which performance model of the set is most general, whose estimations are more likely to \emph{represent} real-world scenarios? 
To assess which model's estimations to trust most, we require information about whether and how different workloads interact with the software system under test. 
To this end, we extend the black-box perspective of existing approaches~\cite{dorn2020,siegmundPerformanceinfluenceModelsHighly2015,haDeepPerf2019,perfAL,guoVariabilityawarePerformancePrediction2013,sarkarCostEfficientSamplingPerformance,guo_2018_data,fourier_learning_2015,perLasso} with code coverage analysis, where the question usually is, what code is covered by a test case.
If the execution of performance-relevant code is determined by a specific workload, one cannot measure the influence such code would contribute to performance otherwise.
In addition, if such interactions affect option-specific code, the influence of configuration option is more susceptible to potential bias. 

\subsection*{Approach, Results \& Implications}
In this paper, we conduct an exploratory study of {\color{red}7} configurable Java systems across a multitude of configurations and workloads to assses the influence of workloads on performance models as well as code coverage. 
Our study results show that workload-specific differences are prevalent across different domains, both with regard to performance behavior and code coverage. We explore how code coverage could serve as a lightweight, but effective indicator to identify potential workload bias and, thus, select representative \emph{representative} workloads a priori. Our results show that\ldots.


\section{Background and Related Work}
\subsection{Performance Models for Configurable Software}
Configurable software systems are an umbrella term for any kind of software system that exhibits configuration options to customize functionality. 
While the primary purpose of configuration options is to select (categorical or binary options) and tune (numerical options) functionality, each configuration choice may also have implications on non-functional properties---be it intentional or unintentional. Finding configurations with optimal performance~\cite{nairUsingBadLearners2017,nairFlash18,ohFindingNearoptimalConfigurations2017} and estimating the performance for arbitrary configurations is an established line of research\cite{siegmundPerformanceinfluenceModelsHighly2015,haDeepPerf2019,perfAL,guoVariabilityawarePerformancePrediction2013,sarkarCostEfficientSamplingPerformance,guo_2018_data,fourier_learning_2015,perLasso}. 

For the latter, machine learning techniques are used to learn \emph{performance models} that approximate non-functional properties, such as execution time or memory usage, as a function of software configurations $c \in C$, formally $\Pi: C \rightarrow \mathbb{R}$.
Performance models can be obtained using a variety of machine learning techniques, including probabilistic programming~\cite{dorn2020}, multiple linear regression~\cite{siegmundPerformanceinfluenceModelsHighly2015}, classification-and-regression trees~\cite{sarkarCostEfficientSamplingPerformance,guo_2018_data}, Fourier learning~\cite{fourier_learning_2015,perLasso}, and deep neural networks~\cite{haDeepPerf2019,perfAL}.
The set of configurations for training can be sampled from the configuration space using a variety of different sampling techniques. All sampling strategies aim at yielding a representative sample, either by covering main effects of configuration options and interactions among them~\cite{siegmundPredictingPerformanceAutomated2012}, or sampling uniformly from the configuration space~\cite{ohFindingNearoptimalConfigurations2017,kaltenecker_distance-based_2019}.

Most approaches share the perspective of treating a configurable software system as a black-box and model application-level performance agnostic of any code. Besides, recent work has incorporated feature location techniques to guide sampling effort towards relevant configuration options~\cite{velez_2020_configcrusher_jase,velez_comprex_2021} or model non-functional properties similarly, but at finer granularity~\cite{weber_white_2021}.

\subsection{Performance under Varying Workloads}
\improvement{define benchmark and workload}
Differences in the performance behavior of configurable software systems have been documented before~\cite{jamishidi_transfer_2017,alves_sampling_2020}. Jamshidi et al. observe that performance influences across different environments remain largely congruent and differences only affect small numbers of configuration options, such that differences can be learned efficiently via transfer learning~\cite{jamishidi_transfer_2017,jamshidi_learning_2018,jamshidi_transfer_gp_2017,ding_bayesian_2020}. Here, the bias is explicitly learned to adapt an already existing â€“ and potentially also biased -- performance model. The benefit of using more than one specific workload for performance models is illustrated by work of Liao et al.~\cite{liao_2020_using_emse}. For two different versions of a software system, they learned performance models based on disjoint sets of workloads. From the comparison of both performance models, they were able to identify performance shifts as the workload bias was averaged out.
Closely related to our work, aspects of workload variability and bias have been studied for micro-benchmarks, such as test suites for performance testing. Laaber et al. use static analysis to estimate the stability (convergence after repeated measurements) of a given micro-benchmark based on usage of specific code features~\cite{laaber_emse_2021}. Similarly to our problem, yet for micro-benchmarks instead of system-level workloads, Grambow et al. use static analysis to infer redundant or missing coverage~\cite{grambow_peerj_2021}. With this approach, they are able to condense benchmark suites to select only relevant ones and also recommend code sections to test with further workloads.

\subsection{Associating Code and Features}\label{sec:feature_location}
The problem of determining what code sections implement certain functionality in a software system is known as feature location. To reason about the degree to what a software feature (i.e., code conditioned by a configuration option) is covered under a workload, knowledge of corresponding code regions is necessary and can be obtained from program analysis techniques.
In this vein, most approaches either employ static or dynamic analysis to infer an association between code and features. For static analyses, variables that encode configuration options are tainted and from following data-flow and control-flow one can infer code sections tainted by such variables~\cite{velez_2020_configcrusher_jase,lillack_2018_lotrack_tse,luo_2019_cova}.
While static analyses can yield precise results, scalability is often limited by the exploration of possible execution paths. To mitigate this shortcoming, dynamic taint analysis taints variables similarly to the above works, but only follows one execution path~\cite{bell_phosphor_2014,velez_comprex_2021,splat_kim_2013}. From few executions of different configurations one can then extract feature-specific code from the code that was covered. A related, less precise, but also less computationally expensive approach is \emph{differential coverage testing}~\cite{wong_integrated_2005,wilde_early_1996,agrawal_fault_1995,simmons_industrial_2006,chen_dynamic_nodate,sherwood_reducing_nodate}. Here, code that is covered when an option is selected, but not covered otherwise, is associated with that option.

\section{Problem Statement}
\improvement{Re-iterate the overall idea}
Performance models are often learned from observations under a single workload. While the implications of potential bias have been described to some extent in related work, we want to revisit these with an introductory example in Listing~\ref{lst:intro}. The Python method computes a dictionary of word frequencies in a given file and can write results to or retrieve results from a cache, which is enabled by a configuration option. We highlighted all code regions that are dependent on either the configuration option. The three highlighted code sections depend on the selection of the caching option, the workload size, or both: The first part will increase execution time if the target file has already been cached, the second part will only be executed for files that contains more than 1024 characters, and the last part will cache a large file for later re-use to save on execution time. That is, depending on the workload and configuration, one can expect different performance behavior. 

\begin{lstlisting}[caption={Illustrative code example with one configuration option (line~1), option-dependent behavior (line~10), workload-dependent behavior (lines~14--17), and mixed-dependency behavior (line~21).},label=lst:intro, escapechar=\%,float]
CACHING = parse_options()
	
def calc_word_frequency(self, path: String)
	workload = open(path, 'r').read()
	workload_size = len(workload)
	frequencies = dict()
	
	# option-dependent
	if CACHING and path in self.cache:
%\cova%		return self.cache[path]}
	
	# workload-dependent
	if workload_size > 1024:	
%\covb%		for word in workload.split(' '):
%\covb%			if word not in frequencies: 
%\covb%				frequencies[word] = 0
%\covb%			frequencies[word] += 1
	
	# option- and workload-dependent
	if CACHING and workload_size > 65536:
%\covc%		self.cache[path] = frequencies
	
	return frequencies
\end{lstlisting}

\subsection{Exploring Workload-specific Differences}
Workload selection can determine the coverage of code regions, but this does not necessarily translate to observable shifts in performance and vice versa. Nonetheless, both metrics, code coverage and performance, in isolation are used to make statements about the quality of a software system. We argue that, even without drawing a connection between both, exploring the impact that the selection of different workloads can have is worthwhile, for instance to select for stable workloads or robust configurations.
In the first part of our contribution, hence, we explore the influence that different workloads can have on performance behavior, as well as code coverage and ask the following two research questions:

\begin{enumerate}[align=left,label=RQ1.\arabic*]
	\item\textit{Are there interactions between workloads and configuration performance of a configurable software systems?}\label{rq:1.1}
	\item\textit{Are there workload-specific differences in code coverage for configurations of a software system?}\label{rq:1.2}
\end{enumerate}

We address the first two research questions with an empirical study of configurable software systems across a multitude of configurations and workloads (cf. Section~\ref{sec:study}). Given the presence of workload-specific differences in both code coverage and performance of a configurable software system, we subsequently explore whether such differences could be attributed to workloads as a common factor of variation. To this end, we ask the following research question:

\begin{enumerate}[align=left,label=RQ1.\arabic*]
	\setcounter{enumi}{2}
	\item\textit{Do workload-specific differences in code coverage correspond to differences in performance?}\label{rq:1.3}
\end{enumerate}


\subsection{Code Coverage and Representativeness}
As the main target of our efforts, in the second part of our efforts, we now focus on how workload variability influences performance models. To further see, whether workload-specific coverage differences affect not only the performance of individual configurations (cf.~\ref{rq:1.1}), but the performance influence of each configuration option (or interaction among them), we ask the following research question:

\begin{enumerate}[align=left,label=RQ2.\arabic*]
	\item\textit{Do workload-specific differences in code coverage correspond to differences in the performance influence of configuration options?}\label{rq:2.1}
\end{enumerate}

Drawing a connection between differences in both code coverage and performance influences, depending on the workload, is a first approximation of how susceptible a performance model is to workload variability. To obtain a clearer picture of how susceptible individual configuration options and interactions among them are to workload bias, we require information about what code is related to one or more configuration option. To achieve this, we aim to infer such locations from using different workloads via differential coverage testing (cf. Section~\ref{sec:feature_location}) and ask the following research question:

\begin{enumerate}[align=left,label=RQ2.\arabic*]
	\setcounter{enumi}{1}
	\item\textit{Can we use different workloads to identify option-specific code sections via superimposition?}\label{rq:2.2}
\end{enumerate}

To ultimately assess, whether we can estimate the bias for individual performance influences, we apply the findings from \ref{rq:2.2} as a baseline for feature location and correlate them with observed differences in performance influences to answer the final research question:

\begin{enumerate}[align=left,label=RQ2.\arabic*]
	\setcounter{enumi}{2}
	\item\textit{Can we use code coverage or option-code coverage as an indicator for representativeness?}\label{rq:2.3}
\end{enumerate}


\section{Empirical Study (RQ)}~\label{sec:study}

\begin{table*}[ht!]
	\centering
	\caption{Experiment Characteristics}
	\input{tables/subject_systems.tex}
	\label{tab:subject_systems}
\end{table*}


\subsection{Experiment Setup}
Table~\ref{tab:subject_systems}\\
Database benchmarks were run using OLTP-Bench\cite{difallah_oltp_2013}\\
\noindent\sosy{kanzi}\footnote{\url{https://github.com/flanglet/kanzi/}} is a compression tool.\\
\sosy{jump3r}\footnote{\url{https://github.com/Sciss/jump3r/}} is a compression tool.\\\
\sosy{density-converter}\footnote{\url{https://github.com/patrickfav/density-converter/}} is a compression tool.\\
\sosy{batik}\footnote{\url{https://xmlgraphics.apache.org/batik/tools/rasterizer.html}} is a compression tool.\\
\sosy{jadx}\footnote{\url{https://github.com/skylot/jadx/}} is a compression tool.\\
\sosy{h2}\footnote{\url{https://github.com/h2database/h2database/}} is a compression tool.\\
\sosy{hsqldb}\footnote{\url{https://github.com/ryenus/hsqldb/}} is a compression tool.\\
\subsubsection*{Configuration Sampling}
For all subject systems, we sampled a set of configurations since exhaustive assessment of the configuration space is infeasible due to combinatorial explosion. We combine several coverage-based sampling strategies and uniform random sampling into an \emph{ensemble} approach: to study the influence of single configuration options, we employ feature-wise and negative feature-wise sampling, where each feature is enabled once, or all except one, respectively. In the same way, pair-wise sampling applies this schema to study influences of two-way interactions between configuration options. Interactions of higher degree can be found accordingly, however, is computationally expensive. Instead, we augment our sample set with a random sample that is at least twice the size of the coverage-based sample subset. To achieve a uniform random sample, we used \emph{distance-based sampling}~\cite{kaltenecker_distance-based_2019}, which ensures uniform coverage of the configuration space. The variability models as well as sample sets can be found at the paper's companion web site.


\subsubsection*{Hardware Setup}
All experiments were conducted on compute clusters with an identical hardware setup, either Intel~Core~i5-8259U CPUs at 2.3~GHz (\sosy{jump3r}, \sosy{kanzi}, \sosy{jump3r}, \sosy{h2}, and \sosy{hsqldb}) or Intel~Core~i7-8559U CPUs at 2.7~GHz (\sosy{density-converter}, \sosy{batik}, and \sosy{jadx}) with 32~GB of RAM, respectively. The machines run a headless Debian 10 installation (kernel version \mbox{4.19.0-14}). To minimize measurement noise, no additional user process were running in the background and no other than necessary packages were installed.
For all data points, we report the median across five repetitions (except for \sosy{h2} and \sosy{hsqldb}), which has shown to be a good trade-off between variance and measurement effort. The reported coefficient of variation was below {\color{Red}10\,\%}. For the two database systems, we omitted the repetitions as in a pre-study, we found that across all benchmarks the throughput coefficient of variation (standard deviation divided by the arithmetic mean) was consistently below 5\,\%.

\subsection{Performance Ranking of Configurations (RQ1.1)}
{\color{teal}
\begin{itemize}
	\item Rank configurations by performance for each workload
	\item Compare Spearman rank correlation for each pair of workloads
	\item Expectation: Low overall, but high local variation
	\item Discuss, how results correspond to~\cite{jamishidi_transfer_2017}
\end{itemize}
}
\subsection{Coverage Set Similarity (RQ1.2)}
{\color{teal}
	\begin{itemize}
		\item Ra
	\end{itemize}
}
\subsection{Correlation Analysis (RQ1.3)}

\section{Estimating Representativeness}~\label{sec:metric}

\subsection{Approximated Feature Regions}

\subsection{Representativeness Scoring}

\section{Discussion}

\section{Threats to Validity}
\subsection{Internal Validity}enum:rq1.1
\begin{itemize}
	\item dedicated hardware to mitigate measurement noise, 
	\item five repetitions per data point, 
	\item database experiments robustness assessed in pre-study
\end{itemize}
\subsection{External Validity}
\begin{itemize}
	\item Java only, but different application domains,
	\item consistent with observations from related work
\end{itemize}
\section{Conclusion}
