%\clearpage
\section{Introduction}
\input{sections/introduction.tex}
\begin{comment}
Most software systems can be customized via configuration options to meet user demands. The selection of configuration options can enable desired functionality (features) or tweak non-functional aspects of a software system, such as improving performance or energy consumption. 
The relationship of configuration choices and their influence on performance has been extensively studied in literature.
The backbone of performance estimation is a model that maps a given configuration to the estimated performance value. 
Learning performance models usually relies on a training set of configuration-specific performance measurements. 
To measure performance, established approaches~\cite{dorn2020,siegmundPerformanceinfluenceModelsHighly2015,haDeepPerf2019,perfAL,guoVariabilityawarePerformancePrediction2013,sarkarCostEfficientSamplingPerformance,guo_2018_data,fourier_learning_2015,perLasso} usually employ only a single workload (e.g., a benchmark or test suite), which aims at emulating a specific real-world application scenario.

Conversely, for performance and load testing in practice, workloads are a widely considered as a key factor for performance variation~\cite{ceesay2020,papadopoulos2021}.
In the domain of cloud computing applications~\cite{papadopoulos2021} and data-intense applications~\cite{ceesay2020}, methodological principles emphasize the importance of using representative workloads or testing against a set of workloads that reflect the system under test and account for workload-specific behavior. Methods for finding such representative workloads are known under the umbrella term \emph{workload characterization}~\cite{calzarossa2016} and are widely used alongside mixes of different workloads in practice~\cite{jiang2015survey}. 

Both configuration-dependent and workload-dependent performance behavior have been studied mostly in isolation. While there are some attempts to gain insights on their combination for specific application scenarios~\cite{alves_sampling_2020}, under limited variability~\cite{liao_2020_using_emse}{\color{purple}, and for a specific problem domain~\cite{koc_satune_2021}}, to the best of our knowledge, we are the first to systematically explore the intersection of both factors of variation {\color{purple} across multiple domains}. We aim at providing a clear picture of how prevalent and severe workload-specific differences in configuration-dependent performance are and how they emerge in the wild. 
To illustrate, how both factors may interact, consider the introductory example of an imaginary database system in Figure~\ref{fig:intro}. The method in this example handles an array of insertions. The workload determines how configuration-dependent code is utilized, such as the execution of \colorbox{duplicatecheck}{Lines 2--6} for configuration option \texttt{DUPLICATE\_CHECK}. A performance model that considers this dependency as an input factor might be more representative than a performance model that was trained on a single workload. If a workload, however, conditions the execution of configuration-specific source code, such as \colorbox{autocommit}{Lines 12--14} for configuration option \texttt{AUTOCOMMIT}, one can derive a more representative performance modes from selecting workloads that increase overall code coverage, similar to test coverage.

%In absence of a systematic study that sheds light on whether and how configuration options and workload choices interact with regard to performance, we address this isssue in this paper. 
We have conducted an empirical study of 29\,014 configurations and 55 workloads across 6 configurable software systems to provide a broad picture of the interaction of configuration and workload when learning performance models and estimating a configuration's performance (i.e., response time). Aside from studying the sole effects of workload variation on performance behavior and performance model influences, we explore \emph{how} both factors interact. To this end, we enrich performance observations with corresponding statement coverage data to understand workload variation at a finer grain. This way, we reveal whether workload-specific effects can be attributed to specific code segments or rather depend on workload-specific code usage.

Our findings suggest that the workload-specific
execution of covered option-specific code segments plays a substantial role in how workloads interact with configurations and is most likely accountable for workload-specific performance variation. We conclude that identifying performance-relevant workload characteristics and incorporating them as independent variables is
a promising avenue towards obtaining generalizable and representative performance models.

To summarize, we make the following contributions: 
	
\begin{compactitem}
	\item An empirical study of 29\,014 configurations and 55 workloads across 6 configurable software systems of whether and how the interaction of workloads with configuration options influences performance;
	
	\item An evaluation combining insights from observations of workload-specific differences of configurations, individual configuration options' influence on performance, and workload-specific coverage differences of option-specific code segments;

	\item A companion Web site\footnote{\url{https://github.com/anonyms-2021/submission-448/}} with supplementary material including performance and coverage measurements, experiment workloads and configurations, as well as additional visualizations left out due to space limitations.
\end{compactitem}
\end{comment}


\section{Background and Problem Statement}
\subsection{Performance Prediction Models}~\label{sec:perfmodels}
Configurable software systems are an umbrella term for any kind of software system that exhibits configuration options to customize functionality. While the primary purpose of configuration options is to select and tune functionality, each configuration choice may also have implications on non-functional properties---be it intentional or unintentional. 
There are different approaches to capture the relationship between configuration options and performance indicators, most basically either \textit{analytical} or \textit{empirical} in nature. All share the objective to approximate non-functional properties, such as execution time or memory usage, as a function of software configurations  $c \in C$, formally $\Pi: C \rightarrow \mathbb{R}$. 

Analytic models incorporate existing knowledge about the operations of a software system, comparable to estimating an algorithmâ€™s complexity~\cite{analytic_model_2000,analytic_model_2011}. Here, one deliberately includes or excludes configuration options as predictors and selects a model structure following the current understanding of the software system. While it avoids ambiguity in terms of feature selection and explainability, analytic approaches do not guarantee to cover unanticipated idiosyncrasies or interactions between configuration options.

Empirical performance models, by contrast, do not rely on an understanding of the software system, but on a set of configuration-specific observations. In this vein, finding configurations with optimal performance~\cite{nairUsingBadLearners2017,nairFlash18,ohFindingNearoptimalConfigurations2017} and estimating the performance for arbitrary configurations of the configuration space is an established line of research~\cite{siegmundPerformanceinfluenceModelsHighly2015,haDeepPerf2019,perfAL,guoVariabilityawarePerformancePrediction2013,sarkarCostEfficientSamplingPerformance,guo_2018_data,fourier_learning_2015,perLasso}.
Empirical performance models can be obtained using a variety of machine-learning techniques, including probabilistic programming~\cite{dorn2020}, multiple linear regression~\cite{siegmundPerformanceinfluenceModelsHighly2015}, classification and regression trees~\cite{guoVariabilityawarePerformancePrediction2013,sarkarCostEfficientSamplingPerformance,guo_2018_data}, Fourier learning~\cite{fourier_learning_2015,perLasso}, and deep neural networks~\cite{haDeepPerf2019,perfAL}.
The set of configurations for training can be sampled from the configuration space using a variety of different sampling techniques~\cite{kaltenecker_interplay_2020}. All sampling strategies aim at yielding a representative sample, either by covering the main effects of configuration options and interactions among them~\cite{siegmundPredictingPerformanceAutomated2012}, or sampling uniformly from the configuration space~\cite{ohFindingNearoptimalConfigurations2017,kaltenecker_distance-based_2019}.
Most approaches share the perspective of treating a configurable software system as a black-box model at application-level granularity. Recent work has incorporated feature location techniques to guide sampling effort towards relevant configuration options~\cite{velez_2020_configcrusher_jase,velez_comprex_2021} or model non-functional properties at finer granularity~\cite{weber_white_2021}.

\subsection{Varying Workloads}\label{sec:varying_workloads}
%\subsubsection{Whats is a workload?}
When assessing the performance of a software system, we ask how well a certain \textit{operation} is executed, or, phrased differently, how well an \textit{input fed to the software system} is processed. Such inputs, commonly called workloads, are essential to assessing performance, even detached from the specific context of configurable software systems. By nature, the workload of a software system is application-specific, such as a series of queries and transactions fed to a database system, a set of raw image files for video encoding, or an arbitrary file for data compression etc. Workloads can often be distinguished by characteristics (dimensions) they exhibit, such as their file size in general, or, the  type of data to be compressed (text, binary data) for instance.

%\subsubsection{Workload Characterization}\label{sec:workload-characterization}
A useful workload for assessing performance or benchmarking should, in practice, closely represent the real-world scenario that the system under test is deployed in. To achieve this, a well-defined and widely employed technique in performance engineering is workload characterization~\cite{ceesay2020,papadopoulos2021}. To select a representative workload, it is imperative to explore workload characteristics and validate a workload with real-world observations. This can be achieved by constructing workloads, among others, from usage patterns~\cite{calzarossa2016}, or by increasing the workload coverage by using a mix of different workloads rather than a single one~\cite{jiang2015survey}.

While workload characterization and benchmark construction is domain-specific, there are numerous examples of this task being driven by community efforts instead of individuals. For instance, the non-profit organizations Standard Performance Evaluation Corporation (SPEC) and Transaction Processing Performance Council (TPC) provide large bodies of benchmarks for data-centric applications or across different domains, respectively.

\subsection{Representative Estimations?}\label{sec:generalizability}
While the notion of representative workloads is ubiquitous in an industry setting, we face a different situation in research on empirical performance models (cf. Section~\ref{sec:perfmodels}): Most approaches provide accurate performance estimations, yet are based on observations gained by varying configurations while keeping the workload the same. Clearly, this can limit the model's generalizability to different workloads, especially if the training workload poorly represents real-world scenarios. While the configuration-specific behavior might be congruent across different workloads (i.e, a configuration scoring comparably for different workloads), we cannot rely on such assumptions in practice, where different workloads can indeed result in entirely different configuration-specific performance behavior.

Revisiting an observation by \citeauthor{alves_sampling_2020}~\cite{alves_sampling_2020}, the following example illustrates that even seemingly small differences in the composition of a workload can induce performance behavior that is hard to anticipate: We tested the performance (throughput: transactions per second) of a number of configurations for the database system \htwo across two different workloads. Both workloads are instances of the database benchmark \textsf{TPC-C}, consisting of a fixed-ratio mix of transactions (inserts, updates, selects) for a specific database schema. The only difference was varying the scale factor, which controls for the number of warehouses modeled in the schema. The resulting performance distributions are given in Figure~\ref{fig:h2_intro}.
\begin{figure}
	\centering
	\includegraphics[width=0.85\linewidth]{images/h2_motivation.eps}
	\caption{Performance distributions of the database system \htwo run the \textsf{TPC-C} benchmark at different scale factors.}
	\label{fig:h2_intro}
\end{figure}
While one might first expect that a more complex workload results in lower throughput, this is indeed the case, but does not hold for all configurations. The median throughput decreases for the larger scale factor, yet the shape and spread of the distribution are entirely different. Most notably, the maximum throughput for the greater scale factor is higher than for the smaller scale factor, indicating that some configurations indeed do not follow the general trend. This illustrates that the effect of varying the workload for some configurations cannot be captured by a linear transformation (constant shift or scaling).


Some aspects of this \emph{input sensitivity} have been observed and documented before in the literature~\cite{liao_2020_using_emse,alves_sampling_2020,jamishidi_transfer_2017} and raise questions, such as: \textit{Which options are input sensitive? What are the driving factors for input sensitivity? Can we estimate which options are input-sensitive?} We set out to answer these questions in this paper.
	
\subsection{Workloads and Performance Prediction} ~\label{sec:strategies}
To some aspects of the said questions, different approaches have been proposed to tackle the problem of input sensitivity.

\paragraph{Workload-aware Performance Modeling}\label{sec:workload-aware}
Extending on workload characterization (cf.~Section~\ref{sec:varying_workloads}), a strategy that embraces workload diversity is to incorporate workload characteristics into the problem space of a performance prediction model. Here, performance is modeled as a function of both the configuration options exhibited by the software system as well as the workload characteristics, formally $\Pi: C \times W \rightarrow \mathbb{R}$.
The combined problem space enables learning performance models that generalize to workloads that exhibit characteristics denoted by $W$ since we can screen for performance-relevant combinations of options and workload characteristics. Although this strategy is highly application-specific, it has been successfully applied to different domains, such as program verification~\cite{koc_satune_2021}. However, its main disadvantages are twofold: The combined problem space (configuration and workload dimension) requires substantially more observations to screen for identifying performance-relevant options, characteristics, and combinations thereof. In addition, previous work  found that that only few configuration options are input sensitive~\cite{jamishidi_transfer_2017} when varying the workload. That is, the problem of identifying meaningful, but sparse predictors is exacerbated since one must not only identify performance-relevant configuration options, but also input-sensitive ones.

\paragraph{Transfer Learning for Performance Models}\label{sec:transfer}
Another strategy that builds on the fact that, across different workloads, only few configuration options are in fact input sensitive~\cite{jamishidi_transfer_2017}. Here one first trains a model on a standard workload and, subsequently, adapts it to different workloads. Contrary to a generalizable workload-aware model, transfer learning strategies focus on approximating a transfer function that, without characterizing the workload, encodes the information of which configuration options are sensitive to differences between a source and target pair of workloads. Training a workload-specific model and adapting it on occasion provides an effective means to reuse performance models, which is not limited to workloads~\cite{jamshidi_learning_2018}, but has successfully been applied to different hardware setups~\cite{ding_bayesian_2020,valov_transferring_performance_2017} and across versions~\cite{martin_transfer_2021}. The main shortcoming of transfer learning approaches is that they do not generalize to arbitrary workloads, since a transfer function is tailored to a specific target workload. Here, one trades off generalizability and measurement cost as learning a transfer function requires substantially fewer training samples.

While both directions are effective means to handle input sensitivity, to the best of our knowledge, there is no \textit{systematic} assessment of the factors that drive the interaction between configuration options and workloads with regard to performance. Understanding scenarios that are associated with or even cause incongruent performance influences across workloads can help practitioners to employ established analysis techniques more effectively and can motivate researchers to devise analysis techniques dedicated to such scenarios.

\section{Study Design}~\label{sec:study}
In what follows, we describe the general experiment setup and study design as well as research questions. We make all performance measurement data, configurations, workloads, and learned performance models available on the paper's companion Web site.
\subsection{Research Questions}
Our first two research questions shed light on the input sensitivity of the performance behavior of the studied software systems. We first take a look at systems as a whole~(\RQref{1}) with regard to a large set of configurations and, subsequently, consider individual configuration options~(\RQref{2}). Extending on the results of \RQref{2}, we explore possible driving factors and indicators for workload-specific performance variation~(\RQref{3}).

\paragraph{Performance Variation Across Workloads}
It is well known that performance variation can arise from differences in the workload~\cite{benchmarking_book}. In a practical setting, the question arises whether, and if so, to what extent an existing workload-specific performance model is representative of the performance behavior of other workloads. 
That is, can a model estimating performance of different configurations be reused for the same software system but run with a different workload? Depending on the degree of similarity of the performance behavior across workloads, we obtain a clearer picture of the prevalence of input sensitivity and to what extent the strategies outlined in  Section~\ref{sec:strategies} might be applicable.
To this end, we formulate the following research question: 

\RQ{1}{To what extent does \textit{performance behavior} vary across workloads?}
\paragraph{Option Influence Across Workloads}
At large, performance behavior is the resulting effect arising from multiple configuration optionsâ€™ and combinationsâ€™ respective influences. To understand which configuration options are driving performance variation, in general, and which are input sensitive, in particular, we formulate the following research question:

\RQ{2}{To what extent do \textit{influences of individual configuration options} depend on the workload?}
\paragraph{Causes of Input Sensitivity}
The first two research questions describe the performance behavior of our subject systems: Based on the results of related work, we expect configuration options to be, at least, to some extent input sensitive. To contextualize our findings, we switch our perspective to the code level. The goal is to understand the relationship between input sensitivity (i.e., variation in the performance influence of configuration options) and the execution of the subject system under varying workloads. We hypothesize that executions under different workloads also exhibit variation with respect to what code sections are executed and how this code is used. Using code coverage analysis---an easy to understand and widely employed technique---we are interested in how far one could infer or explain performance influence variation just based on code. 

\RQ{3}{Does the variation in configuration options' performance influence across workloads correlate with differences in the respective execution footprint?}
\subsection{Experiment Setup}\label{sec:setup}
\paragraph{Subject Systems}
For our study, we select six configurable software systems implemented in Java. We decided to use Java because it is is one of the most widely used programming languages. We discuss this decision in more detail in Section~\ref{sec:threats}. 
All subject systems operate by processing a domain-specific input fed to them
For all software systems, except for \htwo, we report performance as the execution time. For \htwo, we report throughput instead. 

\begin{table}
	\footnotesize
	\centering
	\caption{Subject System Characteristics}
	\input{tables/subject_systems.tex}
	\label{tab:subject_systems}
\end{table}
\paragraph{Workload Selection}
We aim at covering a variety of workload characteristics to make use of different functionality of a subject system. For the selection of workloads, we considered general characteristics (e.g., workload size to probe for scaling and utilization factors) and domains-specific characteristics that reflect the specific application domain (e.g., different valid input file types). As relevant workload characteristics are not always explicitly known, we cannot guarantee that our selection covers all functionality.
Due to space limitations, we provide here only an overview in Table~\ref{tab:workloads}. The workloads along with a more extensive description is available at the companion Web site. 

\begin{table}
	\footnotesize
	\centering
	\caption{Overview of workloads used for our subject systems.}
	\begin{tabular}{lp{7cm}}
		\toprule
		\textbf{System} & \textbf{Workload Description} \\
		\midrule
		
		%\rowcolor{black!5!white}
		\jumper & Different WAVE audio files varying in sampling rate, the number of channels, and length. \\
		
		%\rowcolor{black!10!white}
		\midrule
		\kanzi  & Common file compression benchmarks, including the Canterbury corpus (mixed file types), and single-type files. \\
		
		%\rowcolor{black!5!white}
		\midrule
		\dconvert  & Graphic and project files varying in size and file type (JPEG, PNG, PSD, SVG) \\
		
		%\rowcolor{black!10!white}
		\midrule
		\htwo & Standardized OLTP benchmarks (TPC-H, SmallBank, Yahoo! Cloud Serving Benchmark, and Voter) at two different scale factors from \textsf{benchbase}~(formerly \textsf{OLTPBench})~\cite{difallah_oltp_2013}. \\
		
		%\rowcolor{black!5!white}
		\midrule
		\batik  & Different SVG vector graphic files varying in size. \\
		
		%\rowcolor{black!10!white}
		\midrule
		\jadx  & APK packages of popular Android applications varying in size and application type (utilities, games etc.). \\
		\bottomrule
	\end{tabular}
\label{tab:workloads}
\end{table}

\paragraph{Configuration Sampling}\label{sec:sampling}
All subject systems (except \htwo) exhibit both categorical (binary) and numerical configuration options. 
For each subject system, we sampled a set of configurations. As exhaustive coverage of the configuration space is infeasible due to combinatorial explosion~\cite{henardCombining2015}, for binary configuration options, we combine several coverage-based sampling strategies and uniform random sampling into an \emph{ensemble} approach: 
To study the influence of single configuration options, we employ option-wise and negative option-wise sampling~\cite{siegmundPerformanceinfluenceModelsHighly2015}, where each option is enabled once (i.e., in, at least, one configuration), or all except one, respectively. In addition, we used pairwise sampling, where two-way combinations of configuration options are systematically selected. Interactions of higher degree could be found accordingly, however, it is computationally prohibitively expensive~\cite{henardCombining2015}. 
Last, we augment our sample set with a random sample that is, at least, twice the size of the coverage-based sample. To achieve a uniform random sample, we used \emph{distance-based sampling}~\cite{kaltenecker_distance-based_2019}. Numeric configuration options have been varied across, at least, two levels to account for their effect. All variability models and sample sets can be found on the companion Web site.
	
\paragraph{Coverage Profiling}\label{sec:profiling}
To measure code coverage, we use the on-the-fly profiler \textsc{JaCoCo}. Unlike profilers that use source code instrumentation, on-the-fly instrumentation does not alter the executable binary. From a practical perspective, on-the-fly instrumentation is more flexible and easier to accommodate. Nonetheless, either choice introduces some performance overhead. To avoid this overhead for our performance measurements, we conducted our coverage analysis in a separate run, for which no performance data was collected. 	
	
\paragraph{Hardware Setup}
All experiments were conducted on three different compute clusters, with each subject system was exclusively run on a single cluster. All machines in a compute cluster had the identical hardware setup: either with Intel~Core~i5-8259U CPUs at 2.3~GHz (\jumper and \kanzi),  Intel~Core~i7-8559U CPUs at 2.7~GHz (\dconvert, \batik, and \jadx) and 32~GB of RAM, respectively, or Intel~Xeon~E5-2630~v4 CPUs at 2.2~GHz with 256~GB of RAM (\htwo). All clusters ran a headless Debian 10 installation, the first two with kernel version \mbox{\texttt{4.19.0-14}}, the latter with version \mbox{\texttt{4.19.0-17}}. 
To minimize measurement noise, we used a controlled environment, where no additional user processes were running in the background, and no other than necessary packages were installed.	For all data points, we report the median across five repetitions (except for \htwo), which has shown to be a good trade-off between variance and measurement effort. For \htwo, we omitted the repetitions as, in a pre-study, running on the identical cluster setup, we found that across all benchmarks the coefficient of variation (standard deviation divided by the arithmetic mean) of the throughput was consistently below~5\,\%.

\section{Study Results}~\label{sec:results}
\subsection{Comparing Performance Distributions ($RQ_1$)}\label{sec:rq1}
\subsubsection{Operationalization}
We answer \RQref{1} by pairwisely comparing the performance distributions from different workloads (cf. the comparison in Figure~\ref{fig:h2_intro}) and by determining whether any two distributions are similar or, if not, can be transformed into each other. For the latter case, we are specifically interested in what type of transformation is necessary as this determines \textit{how} complex a workload interacts with configuration options. Specifically, we categorize each pair of workloads with respect to the following aspects: 
\begin{compactenum}
	\item \textit{Similarity}: To test whether varying the workload has any effect at all, we employ statistical tests. Since performance distributions are often multi-modal or long-tailed~\cite{curtsinger_stabilizer_2013,maricq2018taming}, they fail to meet the requirements for parametric methods. 
	Thus, we resort to a non-parametric approach: We use the Wilcoxon signed-rank test~\cite{lovric_international_2010} to assess whether varying the workload affects configuration performance. If we can reject the null hypothesis $H_0$ at significance level $\alpha~=~0.95$, we consider the distributions dissimilar. To account for account for overpowering due to high and different sample sizes (cf.~Table~\ref{tab:subject_systems}), we report effect sizes to asses whether workload effect is negligible or not. We use Cliff's $\delta$~\cite{Cliff1993DominanceSO} to identify non-negliglible differences ($\vert \delta \vert > 0.147$) following the guidelines for interpretation from \citeauthor{romano2006exploring}.~\cite{romano2006exploring}.
	
	\item \textit{Linear Correlation}: To test whether both performance distributions are shifted by a constant value or scaled by a constant factor, we compute for each pair of distributions Pearson's correlation coefficient $r$. To discard the sign of relationship, we use the absolute value and consider $\vert r\vert >0.6$ indicates a strong linear relationship. 
	
	\item \textit{Monotone Correlation}: Finally, we test whether there exists a monotonous relationship between the two performance distributions. We use Kendall's rank correlation coefficient $\tau$~\cite{kendall1938new} and consider $\vert\tau\vert > 0.6$ a strong monotonous relationship.
\end{compactenum}
{\color{black} Based on these three tests and metrics, we composed four categories that each pair of performance distributions can be categorized into. If we cannot reject $H_0$, we consider them identical and as similar distributions (\textsf{\colorbox{cs-color}{SD}}). If both distributions exhibit a strong linear relationship, we classify them as linearly transformable (\textsf{\colorbox{lt-color}{LT}}). If we observe a strong monotonous, but not a linear relationship, we classify such pairs as exclusively monotonously transformable into a separate category (\textsf{\colorbox{xmt-color}{XMT}}). Last, if the comparison yields no monotonous relationship, we can only transform them using non-monotonous methods (\textsf{\colorbox{nmt-color}{NMT}}). We summarize the category criteria as well as the category counts in Table~\ref{tab:categorization}. 

\begin{table}
	\footnotesize
	\caption{Four disjoint categories of relationships between pairs of workload-specific performance distributions and their respective criteria.}
	\centering
\begin{tabular*}{\linewidth}{lp{3.89cm}p{3cm}}	
	\toprule
	 \textbf{Abbrev.} & \textbf{Category} & \textbf{Criteria}\\
	 \midrule
	 \cellcolor{cs-color}\textsf{SD} & {Statistically similar distributions} & {$H_0$ not rejected} and $\delta > 0.147$ \\
	 \cellcolor{lt-color}\textsf{LT} & {Strictly linear transformation} & $r^* \geq 0.6$ \\
	\cellcolor{xmt-color}\textsf{XMT} & {Non-linear, monotonous transformation} & $r^* < 0.6 $ and $ \tau^* \geq 0.6$ \\
	\cellcolor{nmt-color}\textsf{NMT} & {Non-monotonous transformation}  & (otherwise) \\%$p < 0.05$ and $\tau < 0.6$ \\
	\bottomrule
\end{tabular*}
\label{tab:categorization}
\end{table}
\begin{table}
	\footnotesize
	\caption{Frequency of each category (cf. Table~\ref{tab:categorization}) for each software system studied.}
\begin{tabular}{lrrrrrrrr}	
	\toprule
	\textbf{Subject System} & \multicolumn{2}{c}{\textbf{\cellcolor{cs-color}\textsf{SD}}} & \multicolumn{2}{c}{\textbf{\cellcolor{lt-color}\textsf{LT}}} & \multicolumn{2}{c}{\textbf{\cellcolor{xmt-color}\textsf{XMT}}} & \multicolumn{2}{c}{\textbf{\cellcolor{nmt-color}\textsf{NMT}}}\\
	 & \textit{abs} & \textit{rel} & \textit{abs} &\textit{rel} & \textit{abs} & \textit{rel}& \textit{abs} & \textit{rel}\\
	\midrule
	
	\jumper & 
	0 & 0\,\% & 
	\cellcolor{lt-color!100!white}15 & \cellcolor{lt-color!100!white}100.0\,\% & 
	0 & 0\,\% & 
	0 & 0\,\%\\
	
	\kanzi & 
	0 & 0\,\% & 
	\cellcolor{lt-color!78!white}28 & \cellcolor{lt-color!78!white}77.8\,\% & 
	\cellcolor{xmt-color!11!white}	4& \cellcolor{xmt-color!11!white}11.1\,\% & 
	\cellcolor{nmt-color!11!white}4 & \cellcolor{nmt-color!11!white}11.1\,\%\\
	
	\dconvert & 
	0 & 0\,\% & 
	\cellcolor{lt-color!43!white}29 & \cellcolor{lt-color!43!white}43.9\,\% & 
	0 & 0\,\% & 
	\cellcolor{nmt-color!56!white}37 & \cellcolor{nmt-color!56!white}56.1\,\%\\
	
	\htwo & 
	0& 0\,\% & 
	\cellcolor{lt-color!47!white}13 & \cellcolor{lt-color!47!white}46.4\,\% & 
	0 & 0\,\% & 
	\cellcolor{nmt-color!53!white}15 & \cellcolor{nmt-color!53!white}53.6\,\%\\
	
	\batik & 
	0 & 0\,\% & 
	\cellcolor{lt-color!50!white}28 & \cellcolor{lt-color!50!white}50.9\,\% & 
	\cellcolor{xmt-color!14!white}8 & \cellcolor{xmt-color!14!white}14.6\,\% & 
	\cellcolor{nmt-color!34!white}19 & \cellcolor{nmt-color!34!white}34.6\,\%\\
	
	\jadx & 
	0 & 0\,\% & 
	\cellcolor{lt-color!100!white}120 & \cellcolor{lt-color!100!white}100.0\,\% & 
	0 & 0\,\% & 
	0 & 0\,\%\\
	
	\bottomrule
	\end{tabular}
	\label{tab:categorization_counts}
\end{table}
\subsubsection{Results}
We summarize the results of our classification in Table~\ref{tab:categorization_counts}. For all of the six software systems, varying the workloads has a noticable effect on the performance distribution. All software systems, at least, in part, exhibit performance distributions which can be transformed into one another using an linear  transformation, such shifting by a constant value or scaling by a constant factor. In particular, for \jumper and \jadx, we did  observe only such behavior. This finding corresponds to experimental insights~\citeauthor{jamishidi_transfer_2017}, who encoded differences between performance distributions using linear functions. For four software systems, we obtained a more diverse picture: For \kanzi and \batik, a few performance distributions require transformations that are non-linear, but still monotonous. For \dconvert and \htwo, the majority of performance distribution pairs cannot be described by a monotonous relationship. In total, four out of the six software systems exhibit \emph{non-monotonous} relationships across, at least, one workload.

The observed range of relationship types across six software systems had no type prevail across all software systems. The large number of linear relationships suggests that varying the workload does not pose a general obstacle to learning or adapting performance models However, the presence of non-monotonous relationships besides other ones emphasizes that (a) there exist indeed cases where adapting performance models across workloads is challenging. That is, adressing and handling non-monotinicity require more information about what factors (workload characteristics and configuration options) are driving workload-dependent effects.
\vspace{1mm}
\greybox{\textbf{Summary} (\RQref{1}): Varying the workload causes a substantial amount of variation among performance distributions. Across workloads, we observed \textit{mostly linear}, but to a large extent, also \textit{non-monotonous} differences.
}

\subsection{Input Sensitivity of Options ($RQ_2$)}\label{sec:rq2}

\subsubsection{Operationalization}
To address \RQref{2}, we need to determine the configuration optionsâ€™ influence on performance and assess their variation across workloads. 

\paragraph*{Explanatory Model}
To obtain accurate and interpretable performance influences per option, we learn an explanatory performance model using the entire sample set that is based on multiple linear regression~\cite{dorn2020,siegmundPerformanceinfluenceModelsHighly2015,perLasso}. Here, each variable in the linear model corresponds to an option and each coefficient represents the corresponding option's influence on performance. 
We limit the set of independent variables to individual options rather than including higher-order interactions to be consistent with the feature location used for \RQref{3} where we determine option-specific, yet not interaction-specific code segments.
\paragraph*{Standardization}
To facilitate the comparison of regression coefficients across workloads, we follow common practice in machine learning and standardize our dependent variable by subtracting the populationâ€™s mean performance and divide the result by the respective standard deviation. Henceforth, we refer to these standardized regression coefficients as \textit{relative performance influences}. A beneficial side effect of standardization is that the observed variation of regression coefficients for each configuration option cannot be attributed to shifting or scaling effects (affine transformation, class \colorbox{lt-color}{\textsf{LT}} in Table~\ref{tab:categorization}). This way, we can pin down  the non-linear or explicitly non-monotonous effect that workloads may exercise on performance.
\paragraph*{Handling Multicollinearity} Multicollinearity is a standard problem in statistics and emerges when features are correlated~\cite{Daoud_2017}. This can, for instance, arise from groups of mutually exclusive configuration options and result in distorted regression coefficients~\cite{dorn2020}. Although the model's prediction accuracy remains unaffected, we cannot trust and interpret the calculated coefficients. To mitigate this problem and, in particular, to ensure that the obtained performance influences remain interpretable, we follow best practices and remove specific configuration options from the sample that cause multicollinearity~\cite{dorn2020}. For the training step, we exclude all mandatory configuration options since these, by definition, cannot contribute to performance variation. In addition, for each group of mutually exclusive configuration options, we discard one group member. 

From the comparison of the relative performance influences, we can answer \RQref{2} in detail and assess how many configurations are sensitive to varying the workload, what characteristic traits describe the performance influences, and whether we can identify patterns.
\subsubsection{Results}\label{sec:results2}
We illustrate the results of training explanatory performance models for each subject system in Figure~\ref{fig:results_influence}. Each row shows the distribution of the relative performance influence of a configuration option across the set of tested workloads. For this visualization, we made some tweaks to highlight a few properties: First, we show each regression coefficient as an individual black rug (vertical bar). Second, we highlight the greatest positive influence and smallest minimum influence in red and green, respectively, to illustrate both the range of influences and possible opposing influences (i.e., a performance degrading option becomes performance improving or vice versa). 
\begin{figure}
	\centering
	\includegraphics[width=0.95\linewidth]{images/out_annotated2.pdf}
	\caption{Relative performance influences (\textit{standardized regression coefficients}) for all configuration options across all workloads. Each black bar denotes a workload-specific performance influence. For each configuration option, we highlight the range of observed influences. }
	\label{fig:results_influence}
\end{figure}
{\color{orange!0!black}
We have identified three characteristic (but non-exclusive) traits, by which we can describe the distributions of regression coefficients: 
\begin{compactitem}	
\item[\circled{1}] \emph{Spread} of performance influences: Some distributions scatter over a wide range, while others are concentrated around a single value. Consider \htwo, for which we observe a relative performance difference of 200\,\% for option \textsf{MVSTORE}, meaning that the performance influence of turning on this option can be twice as high depending on the workload. 

\item[\circled{2}] \emph{Opposing influences}: Some distributions exhibit both positive and negative coefficients, while others remain consistently either positive or negative. For \dconvert, we observe both positive and negative influences for option \textsf{floor} for two workloads, while for the remaining workloads, the option has neglible influence.

\item[\circled{3}] \emph{Conditional influences}: For some models, the majority of coefficients is negligible, but for few workloads we observe options having an influence. For example, for \kanzi, we observe for option \textsf{BWT} we observe a negative influence only for a specific workload, whereas for all others the option has no influence.
\end{compactitem}
	
These criteria, the spread (\circled{1}) or concentration, opposing influences (\circled{2}), and options becoming influential only on occasions (\circled{3}), allow us to group each configuration option into a specific category, as presented in Table~\ref{tab:option_classification}. We omitted all configuration options with low spread and a neglible performance influence since these are not interacting with the workload either. 
}
\begin{table}[ht!]
	\centering
	\footnotesize
	\caption{Classification of relative performance influence distributions with respect to \textit{spread}, \textit{opposing influence}, and \textit{conditional influence.}}
	\begin{tabular}{p{1.2cm}p{0.7cm}rp{4.9cm}} % h2 batik jadx
		\toprule
		\textbf{Category} & \textbf{System} & \textbf{\#} & \textbf{Options} \\
		\midrule
		
		\multirow{5}{*}{\parbox{1.3cm}{ \centering \circled{1}\\\vspace{1mm} High Spread}} & \kanzi & 1 & \textsf{TPAQX}\\
		\cmidrule{2-4}
		& \dconvert & 6 & \textsf{androidIncludeLdpiTvdpi, gif, win, ios, threads, web} \\
		\cmidrule{2-4}
		& \htwo & 3 & \textsf{\mbox{RECOMPILE\_ALWAYS, MVSTORE}, \mbox{COMPRESS}} \\
		\cmidrule{2-4}
		& \batik & 4 & \textsf{resolution, png, pdf, validate}\\
		\cmidrule{2-4}
		& \jadx & 2 & \textsf{no-res, no-src}\\
		\midrule
		
		\multirow{5}{*}{\parbox{1.3cm}{\centering \circled{2}\\\vspace{1mm} Opposing Influences}} & \kanzi & 2 & \textsf{Huffman, RLT} \\
		\cmidrule{2-4}
		& \dconvert & 3 & \textsf{threads, androidIncludeLdpiTvdpi, floor} \\
		\cmidrule{2-4}
		& \htwo & 1 & \textsf{MVSTORE}\\
		\cmidrule{2-4}
		& \batik & 1 & \textsf{pdf}\\
		\midrule
		
		\multirow{5}{*}{\parbox{1.3cm}{\centering \circled{3}\\\vspace{1mm}Conditional Influences}} & \jumper & 3 & \textsf{Mono, MidSideStereo, JointStereo}\\
		\cmidrule{2-4}
		 & \kanzi & 4 & \textsf{BWT, skip, TPAQ, TPAQX} \\
		 \cmidrule{2-4}
		 & \dconvert & 7 & \textsf{floor, png, round, threads, web, ios, win} \\
		 \cmidrule{2-4}
		 & \htwo & 2 & \textsf{DEFRAG\_ALWAYS, COMPRESS} \\
		 \cmidrule{2-4}
		 & \batik & 3 & \textsf{onload, tiff, png}\\
		 
		\bottomrule
	\end{tabular}	
\label{tab:option_classification}
\end{table}

With two exceptions, we see that all software systems have configuration options, for which, at least, one of the three characteristic traits apply. For \jumper, we found only conditional influences for three options, whereas for \jadx, we observed only three options with high spread of relative performance influences. As these differences arise from varying the workload, we conclude that input sensitivity of a configuration option can manifest in different ways, as shown by the three characteristics. Of particular interest are conditional and opposing influences. We conjecture that the factors leading up to these characteristics are likely easier to identify than those causing high spread.
\vspace{1mm}
\greybox{\textbf{Summary} (\RQref{2}): Configuration options are profoundly input sensitive: We observe high performance variantions, non-mo\-no\-to\-nic behavior, conditional influence, and even diametrically opposed influence for a single option.}

%\subsection{Exploring Categories of Sensitivity}
\subsection{Code Coverage and Performance ($RQ_3$)}\label{sec:rq3}\label{sec:categories}
From the findings of \RQref{2}, we have learned that input sensitivity of configuration options is specific to certain configuration options and can be diverse along multiple dimensions. With regard to the characteristic traits from~\RQref{2}, we conjecture that workload-specific sign flipping or conditional influences are driving factors for non-monotonicity across entire performance distributions (cf.~\RQref{1}). From these findings, the question arises: What causes these different aspects of input sensitivity? For a practical setting, one might, in addition, ask whether it is possible to identify input-sensitive configuration options without an assessment resembling our case study.
To shed light on possible explanations for input sensitivity, in general, and, possibly, different shades (cf.~\RQref{2}), we switch to the code level and analyze the relation of performance influences inferred for each configuration option to code coverage information. 

We augment our performance observations with code coverage information to assess differences in the execution under different workloads. Specifically, we are interested in code sections that implement option-specific functionality (i.e., functionality that is only used if the configuration option is selected). From comparing the coverage information of option-specific code, we can formulate different hypothetical scenarios explaining input sensitivity. 

First, if we observe that the  coverage of option-specific code is conditioned by the presence of some workload characteristic, we expect that such an option is only influential under respective workloads. This scenario would enables us (to some extent) to use code coverage as a cheap-to-compute proxy for estimating the representativeness of a workload and, by extension, resulting performance models: For options that are known to condition code sections we can maximize option-code coverage to elicit all option-specific behavior and, thus, performance influence. For instance, a database system could cache a specific view only if a minimum number of queries are executed. Here, the effect of any caching feature would be conditioned by the number of transactions resulting from the workload.
Second, if we observe performance variation across workloads in spite of similar or identical option-specific code coverage, we draw a different picture. Here, we cannot attribute performance variation to code coverage, yet have to consider differences in the workloadsâ€™ characteristics as potential cause: The presence of a workload characteristic may influence not {what} code sections are executed, but \emph{how} code sections are executed. For instance, in a simple case, a software systemâ€™s performance may scale with the input size. In a more complex case, the presence of a characteristic may determine how frequently an operation is repeated, as is the case for compiling a \LaTeX document with a bibliography (three times) and without (only once).

\subsubsection{Locating Configuration-dependent Code}
To reason about option-specific code, we require a mapping of configuration options to code. 
The problem of determining which code section implements which functionality in a software system is known as \emph{feature location}~\cite{rubin_feature_2013}. 
While there are a number of approaches based on static~\cite{velez_2020_configcrusher_jase,lillack_2018_lotrack_tse,luo_2019_cova} and dynamic taint analysis~\cite{bell_phosphor_2014,velez_comprex_2021,splat_kim_2013}, we employ a more light-weight,  but also less precise approach that uses code coverage information, such as execution traces.
The rationale is that, by exercising feature code, for instance via enabling configuration options or running corresponding tests, its location can be inferred from differences in code coverage. 
Applications of such an approach have been studied not only for feature location~\cite{wong_integrated_2005,sulir_annotation_2015,michelon_spectrum_2021,perez_framing_2016}, but root work on in program comprehension~\cite{wilde_early_1996,wilde_reconnaissance_1995,sherwood_reducing_nodate,perez_diagnosis_2014,castro_pangolin_2019} and fault localization~\cite{agrawal_fault_1995,wong_faultloc_2016}. 
}
Specifically, we follow  a strategy akin to  \textit{spectrum-based feature location}~\cite{michelon_spectrum_2021}:
First, we obtain a baseline of \textit{all} option code in the scope of the entire workload selection. For each workload $w \in W$, we compute the set of code lines that depend on any option $o \in O$. 
Let $C_{o}$ be the set of configurations with option $o$ selected, and $C_{\neg o}$ with option $o$ deselected. To obtain the code sections specific to option $o$ under workload $w$, $S_{w, o}$, we subtract the set of the code lines covered under $C_{\neg o}$ from those of $C_{o}$:
\begin{equation}%\todo{example from Figure~\ref{fig:intro}}
	S_{w, o} = \bigcup_{p \in C_{o}} S_{w}(p) ~ \setminus ~ \bigcup_{q \in C_{\neg o}} S_{w}(q)
\end{equation}
While $S_{w, o}$ yields an approximation of option-dependent code for a single workload, we aggregate the approximations for each workload $w\in W$ to obtain the set of lines that depend on a configuration option $o$ and are executed in, at least, one workload,~$S_{o}$: 
\begin{equation}
	S_{o} = \bigcup_{w \in W} S_{w, o}
\end{equation}
While this aggregated set is not a ground truth per se, it enables us to reason about differences in option-dependent code in the scope of our selected workloads. That is, the expressiveness of this baseline depends on the diversity of the workloads in question. From the ratio of option-specific code per workload to option-specific code across workloads, $\mid S_{w_1, o}\mid/~{\mid S_{w_2, o}\mid}$, we can estimate the coverage of option-dependent code. 

\subsubsection{Comparing Execution Footprints}
From (a) the information about which code sections are specific to a configuration option and (b) how much of these sections is actually covered under different workloads, we can compare the workload-specific execution footprint for each option. By comparing the sets $S_{w_1, o}$ and $S_{w_2, o}$ for any two workloads $w_1$ and $w_2$, we can estimate similarity between the option-code coverage via the Jaccard set similarity index. A Jaccard similariy of zero implies that there is no overlap in the lines covered under each workload, whereas a Jaccard similarity of 1 implies that the exact same code was covered. Based on this pairwise similarity metric $\text{sim}_o(w_1, w_2)$, we can compute a distance $d_o(w_1, w_2) = 1 - sim(w_1, w_2)$ and cluster all workload-specific execution profiles. We use agglomerative hierarchical clustering with full linkage to construct dendrograms, as shown in Figure~\ref{fig:results_rq3}. In this bottom-up approach, we iteratively add execution footprints to clusters and merge sub clusters into larger ones depending on their Jaccard similarity  to each other. The vertical bars with respect to the x-axis denote the Jaccard distance between merged clusters or, for initial clusters, constituent execution footprints. Finally, we compare (a) the clustering of workload-specific execution profiles for each option with (b) the distribution of relative performance influences for the respective option. As a recap, the distribution of performance influences refers to individual rows in Figure~\ref{fig:results_influence}. In essence, we ask whether variation in the observed distribution of relative performance influences correspond to similarities or differences in what, or what portions of, option-specific code is executed. Of special interest are the patterns identified in Section~\ref{sec:results2} and the involved configuration options from Table~\ref{tab:option_classification}.

\subsubsection{Results}
We inspected manually the relation of the coverage similarity across workloads for each option and the observed workload-specific performance influences.  For the majority of cases, the results suggest that the workload determines how the code is executed since we did not observe a strong relationship between performance variation and differences in code coverage. However, we have identified seven configuration options across three software systems (\jumper, \kanzi, and \dconvert)  where code coverage is likely the driving factor for performance variation. We present illustrative examples for both scenarios in Figure~\ref{fig:results_rq3}.
\begin{figure}
	\centering
	\begin{subfigure}{0.99\linewidth}
		\begin{minipage}{0.5\linewidth}
			\includegraphics[width=\linewidth]{images/plots/jump3r_Mono_influences.pdf}
		\end{minipage}
		\begin{minipage}{0.5\linewidth}
			\includegraphics[width=0.8\linewidth]{images/plots/jump3r_Mono_workloads.pdf}
		\end{minipage}
		\caption{Configuration option \guillemotleft\textsf{Mono}\guillemotright~of \jumper}
		\label{fig:results_rq3_jump3r}
	\end{subfigure}
	
	\begin{subfigure}{0.99\linewidth}
		\begin{minipage}{0.5\linewidth}
			\includegraphics[width=\linewidth]{images/plots/kanzi_skip_influences.pdf}
		\end{minipage}
		\begin{minipage}{0.5\linewidth}
			\includegraphics[width=0.8\linewidth]{images/plots/kanzi_skip_workloads.pdf}
		\end{minipage}
		\caption{Configuration option \guillemotleft\textsf{skip}\guillemotright~ of \kanzi}
		\label{fig:results_rq3_kanzi}
	\end{subfigure}

	\begin{subfigure}{0.99\linewidth}
		\begin{minipage}{0.5\linewidth}
			\includegraphics[width=\linewidth]{images/plots/dconvert_web_influences.pdf}
		\end{minipage}
		\begin{minipage}{0.5\linewidth}
			\includegraphics[width=0.8\linewidth]{images/plots/dconvert_web_workloads.pdf}
		\end{minipage}
		\caption{Configuration option \guillemotleft\textsf{web}\guillemotright~ of \dconvert}
		\label{fig:results_rq3_dconvert}
	\end{subfigure}
	
	\begin{subfigure}{0.99\linewidth}
		\begin{minipage}{0.5\linewidth}
			\includegraphics[width=\linewidth]{images/plots/h2_MVSTORE_influences.pdf}
		\end{minipage}
		\begin{minipage}{0.5\linewidth}
			\includegraphics[width=0.8\linewidth]{images/plots/h2_MVSTORE_workloads.pdf}
		\end{minipage}
		\caption{Configuration option \guillemotleft\textsf{COMPRESS}\guillemotright~ of \htwo}
		\label{fig:results_rq3_h2}
	\end{subfigure}
	\caption{Distribution of relative performance influences across workloads (left) and cluster dendrogram of workload-specific option-code executions (right).}
	\label{fig:results_rq3}
\end{figure}

For the scenario of a workload characteristic conditioning code coverage, \jumper is a good illustrative example in Figure~\ref{fig:results_rq3_jump3r}: We have identified two workloads that exhibit multiple audio channels (\textsf{helix.wav} and \textsf{dual-channel.wav}) under which configuration options (\textsf{Mono, MidSideStereo and JointStereo}) become influential. This is supported by the coverage information, where we see that both workloads result in similar code sections covered, whereas such code sections are not covered under other workloads. We observe similar effects for \kanzi and \dconvert. For \kanzi, we find two distinct clusters of code coverage whose workloads show opposing influences for option \textsf{skip} (cf. Figure~\ref{fig:results_rq3_kanzi}). For \dconvert, under workload \textsf{svg-large} no option-specific code is executed, resulting in little influence for options \textsf{web, ios, and gif} (cf. Figure~\ref{fig:results_rq3_dconvert}). These three examples suggest that a workload characteristic can indeed determine whether a configuration option's code section is executed and, thus, determine whether this option is influential or whether its performance influence is positive or negative. 

By contrast, the majority of cases we inspected did not show such a relationship. We illustrate one example that highlights that non-linear shifts in performance influence can arise even from little differences in the workload. While for the other subject systems, we selected workloads without knowledge of workload characteristics, for \htwo, we pursued a rather controlled experiment. Here, we only vary the scale factor for four different standard benchmarks. Thus, we can expect some inherent similarity across these pairs. In Figure~\ref{fig:results_rq3_h2}, we show the comparison of performance influences and the corresponding dendrogram for the configuration option \textsf{COMPRESS}. While we see that the execution footprints for the pairs of benchmarks form clusters and are quite similar (i.e., the distance is below 0.3 among all workloads), we still observe performance variation, most notably, in the case of workload \textsf{ycsb-600}. So contrary to our expectation, varying only one characteristic (here, the scale factor) can introduce further variation that cannot be plausibly explained by workload-specific code coverage. 

For the other subject systems and remaining configuration options, we could not find any relationship between workload-dependent performance variations and code coverage. We identified both cases, where differences in code coverage did not correspond to variation in the relative performance influences and vice versa. That is, for the majority of configuration options, input sensitivity cannot be explained by a varying option-specific code coverage. Instead, workload characteristics most likely account for variation in how covered option specific-code is executed, including the frequency and depth of method calls as well as variation arising from method arguments.
\vspace{1mm}
\greybox{\textbf{Summary} (\RQref{3}): Varying the workload can condition the execution of option-specific code  (\textit{code coverage}) and cause performance differences. However, there is no single driving factor: \textit{Code utilization} depending on workload characteristics is a likely factor accounting for the majority of performance influence variation.  
}

\section{Discussion}\label{sec:discussion}
In this section, we summarize the implications learned from our study and discuss threats to validity and limitations of our study.
\subsection{The Problem of Input Sensitivity}\label{sec:discusssion_challenge}
With \RQref{1} and \RQref{2}, we have addressed the problem of input sensitivity: The key is that not all, but only a few of configuration options are susceptible to varying the workload. In addition, despite our small selection of workloads, we observed differences in performance influence across even similar workloads, as the example of \htwo illustrates (cf.~Figure~\ref{fig:results_rq3_h2}). That is, the challenge that input sensitivity poses to performance modeling is twofold: First, it is key to identify characteristics by which a workload can be described and which are relevant to performance. While workload characterization is a well established practice~\cite{ceesay2020,papadopoulos2021}, one cannot guarantee a complete and exhaustive description of workloads. Moreover, workload characterization is often done only for specific applications, such as high-performance computing. Second, to account for input sensitivity, we require knowledge of which configuration options have an influence on performance and are susceptible to varying the workload. In essence, configuration-dependent performance modeling in the presence of varying workload gives rise to a more complex and inflated problem space, where characterizing workloads requires knowledge of both the application under test and the workloads in question.

Adding to the increased complexity, we observed a significant number of cases, where varying the workload resulted in non-mo\-no\-to\-nous differences, meaning transforming one workload-specific performance distribution to another workload requires a non-mono\-tonous transfer function. While learning non-mo\-no\-to\-nous relationships is possible with established machine learning techniques, it is limited by the resulting model's interpretability. 

With the exception of the dedicated approaches described in Section~\ref{sec:workload-aware}, approaches that aim at learning and tuning performance of configurable software systems~\cite{siegmundPerformanceinfluenceModelsHighly2015,haDeepPerf2019,perfAL,guoVariabilityawarePerformancePrediction2013,sarkarCostEfficientSamplingPerformance,guo_2018_data,fourier_learning_2015,perLasso} do not consider the workload as a factor contributing to performance and, thus, we have to question their overall effectiveness beyond using a single workload. Our results highlight the necessity of a more holistic approach when modeling performance and motivate further research on feature selection and sensitivity analysis: identifying performance-relevant workload characteristics as well as input-sensitive configuration options.

\subsection{Shades of Input Sensitivity}
Our study allows us to attribute workload-specific performance variation to different factors, as demonstrated in Section~\ref{sec:categories}: option-code coverage (whether option-specific code sections are executed at all) and option-code usage (how option-specific code sections are executed). For code coverage, consider option \textsf{Mono} of \jumper: This configuration option's code sections are executed only under specific workloads, which, in this case, can be explained by a workload characteristic: the presence or absence of multiple audio channels. For cases where we observed performance influence variation in spite of similar option-code coverage, we conjecture that workload characteristics (not as apparent as for \jumper) determine how the option-specific code sections are executed and, subsequently, shape the optionâ€™s performance influence. 
This differentiation provides a more detailed and nuanced picture of input sensitivity of configuration options. It implies that, to adequately handle input sensitivity, different analyses techniques are necessary. While this complicates the challenge outlined in Section~\ref{sec:discusssion_challenge} in principle (since there cannot be a silver bullet to handle all shades of input sensitivity), the call for different analyses can make it easier overall as we can address different driving factors individually and in a stepwise manner.

Our results suggest that, to improve representativeness, an additional notion of input sensitivity has to be considered. We envision future performance prediction models that indicate sources of input sensitivity, as illustrated in Equation~\ref{eq:cool_model}, to specifically refine the representativeness of options.
\begin{equation}
	\Pi(c) = {\color{red!40!black}
		\underbrace{\beta_1 \cdot c_1 + ~~\ldots}_{\text{\parbox{1.4cm}{\centering Not input-sensitive}}}
	} + {\color{green!40!black}
		\underbrace{\beta_2 \cdot c_2 +  ~~\ldots}_{\text{\parbox{2.55cm}{\centering Influence conditioned by option-code coverage}}}
	} + ~~
	{\color{blue!50!black}
		\underbrace{\beta_3 \cdot c_3 + ~~\ldots}_{\text{\parbox{2.5cm}{\centering Influence determined by option-code utilization}}}
	}
	\label{eq:cool_model}
\end{equation}

The benefit of such a representation is that developers are now aware that certain influences are prone to workload variations and may need to be re-fitted when the application scenario changes.
In our empirical study, we have employed feature location techniques and code coverage analysis to identify cases where option-code coverage conditioned the performance influence of individual configuration opinions. This approach is a first step to classify configuration options and increase overall performance model representativeness. Coverage testing is already an established practice in functional regression testing, so maximizing option-code coverage could be easily adopted to testing in a non-functional context. 
As our empirical study aimed at identifying driving factors of input sensitivity, methods for assessing code utilization remain an open challenge. Existing work on comparing the execution detailat finer granularity, for instance via log differencing~\cite{bao_logdiff_ase_2019}, provide a promising avenue towards obtaining representative performance prediction models.

\subsection{Threats to Validity}\label{sec:threats}
Threats to \emph{internal validity} include measurement noise which may distort our classification into categories (Section~\ref{sec:rq1}) and model construction (Section~\ref{sec:rq2}). We mitigate these threats by repeating each experiment five times and reporting the median as a robust measure. %For \htwo, we confirmed a negligible measurement variation in a separate pre-study.
Moreover, the coverage analysis with \mbox{\textsc{JaCoCo}} entails a noticeable instrumentation overhead, which may distort performance observations. We mitigate this threat by separating the experiment runs for coverage assessment and performance measurement. In the case of \htwo, the load generator of the \textsf{benchbase} framework (formerly \textsf{OLTPBench})~\cite{difallah_oltp_2013} ran on the same machine as the database since we were testing an embedded scenario with only negligible overhead.

The selection of subject systems, all written in Java, poses a threat to \emph{external validity}. While our motivation for this selection is primarily practical (cf. Section~\ref{sec:setup}), we mitigate this threat by selecting subject systems from a variety of application domains (cf.~Table~\ref{tab:subject_systems}). Nonetheless, one cannot necessarily conclude that our results hold for other domains and programming languages. 

\section{Conclusion}\label{sec:conclusion}
Most modern software systems exhibit configuration options to customize behaviors and meet user demands. Configuration choices, however, can also affect the performance of a software system.
State-of-the-art approaches model configuration-dependent software performance, yet overlook variation due to the workload.Until now, there exists no \textit{systematic} assessment of what is driving the effect that input sensitivity of individual configuration optionsâ€™ influence on performance. We have conducted an empirical study of 29 347 configurations and 55 workloads across six configurable software systems to characterize the effects that varying the workload can have on configuration-specific performance. We compare performance measurements with code coverage data to identify possible factors that drive input sensitivity. We found that the interaction between options and workloads are driven by workload characteristics conditioning the execution of option-specific code sections as well as determining how option-specific code sections are executed. Our findings highlight the necessity to consider input sensitivity when modeling configuration-dependent performance as varying the workload resulted in a substantial number of non-monotonous relationships, which limits a performance model's representativeness. Code analysis can provide an effective strategy to differentiate between sources of input sensitivity to obtain more representative performance models.

