%\clearpage
\section{Introduction}
\subsection*{Motivation}
Most software systems can be customized via configuration options to meet user demands. The selection of configuration options can enable desired functionality (features) or tweak non-functional aspects of a software system, such as improving performance or energy consumption. 
All too often, software performance bugs can be linked to configuration options~\cite{han_empirical_2016}. 
The relationship of configuration choices and their influence on non-functional properties has been extensively studied in recent years. Most work focuses on the prediction of non-functional properties\footnote{In this paper, we use the terms \textit{non-functional properties} and \textit{performance} interchangably, although the latter term is more restricive. In the context of this work a performance model describes any model that predicts a non-functional property.}, such as execution time or memory utilization, for arbitrary configurations~\cite{dorn2020,siegmundPerformanceinfluenceModelsHighly2015,haDeepPerf2019,perfAL,guoVariabilityawarePerformancePrediction2013,sarkarCostEfficientSamplingPerformance,guo_2018_data,fourier_learning_2015,perLasso}, or on finding configurations that are near-optimal with regard to a non-functional property~\cite{nairUsingBadLearners2017,nairFlash18,ohFindingNearoptimalConfigurations2017}. 
The backbone of performance estimation is a model that maps a given configuration to the estimated performance value. 

Learning performance models usually relies on a training set of configuration-specific performance measurements. 
To measure performance, established approaches~\cite{dorn2020,siegmundPerformanceinfluenceModelsHighly2015,haDeepPerf2019,perfAL,guoVariabilityawarePerformancePrediction2013,sarkarCostEfficientSamplingPerformance,guo_2018_data,fourier_learning_2015,perLasso} employ only a single workload (e.g., a benchmark or test suite), which usually aims at emulating a specific real-world application scenario.
Conversely, for performance and load testing in practice, workloads are a widely considered and called for factor of variation~\cite{ceesay2020,papadopoulos2021}.
In the domain of cloud computing applications~\cite{papadopoulos2021} and data-intense applications~\cite{ceesay2020}, methodological principles emphasize the importance of using representative workloads or testing against a set of workloads that reflect the system under test and account for workload-specific behavior. Methods for finding such representative workloads, among other based on observed usage patterns, are known under the umbrella term \emph{workload characterization}~\cite{calzarossa2016} and are widely used alongside mixes of different workloads in practice~\cite{jiang2015survey}. To illustrate the relevance and effectiveness of varying workloads for performance assessment, consider the following example by \citeauthor{liao_2020_using_emse}: For two different versions of a software system,
they learn performance models based on disjoint sets of workloads.
From the comparison of both performance models, they were able
to identify performance shifts as the workload bias was averaged
out~\cite{liao_2020_using_emse}.


While both configuration-dependent performance and performance under varying workloads have been studied in isolation, to the best of our knowledge, we are the first to systematically explore the intersection of both factors of variation. As configuration-dependent performance are learned under under one workload, the question arises whether such performance models can generalize to different workloads. To some degree, with transfer learning it is possible to adapt existing models and learn workload-specific differences~\cite{jamishidi_transfer_2017,jamshidi_learning_2018,jamshidi_transfer_gp_2017}. Yet, we do not have a clear picture of how prevalent and severe workload-specific differences in configuration-dependent performance are and how they emerge. For instance, if workloads merely determine how option-dependent code is utilized, performance models that consider this dependency as input factors might be more representative. If workloads, however, condition the execution of option-specific code, one can derive more representative performance models from selecting workloads that increase overall code coverage.

%subsection*{Practical Relevance }
\begin{comment}
Recent work on varying workloads for configurable software systems~\cite{alves_sampling_2020,jamishidi_transfer_2017} has, to some extent, illustrated the influence that varying the workload for learning performance models can elicit: \citeauthor{alves_sampling_2020} show that for two different workloads of the video trancoder x264 the performance observed performance of a set of configurations (henceforth called \emph{performance distribution}) can differ substantially~\cite{alves_sampling_2020}. \citeauthor{jamishidi_transfer_2017} explore the similarity of performance influences of configuration options under different workloads~\cite{jamishidi_transfer_2017,jamshidi_transfer_gp_2017} and exploit similarities to adopt workload-specific performance models to other workloads using transfer learning~\cite{jamshidi_learning_2018}. Both cases, dissimilar performance distributions as well as performance influence similarities over different workloads are plausible. Yet, to the best of our knowledge, there is no systematic study of if and how workload choices can influence configuration-specific performance behavior. That is, one cannot state whether existing performance modeling approaches are \emph{representative} of real-world behavior. Moreover, the practicality of the proposed transfer learning technique~\cite{jamshidi_transfer_gp_2017} depends on how susceptible the performance influence of configuration options is to workload variation.
\end{comment}
%\subsection*{What we do}
In absence of a systematic study that sheds light on if and how configuration options and workload choices interact with regard to performance, we address this isssue in this paper. We conducted an empirical study of X configurations and Y workloads across six configurable software systems to provide a broader picture of the interaction of configurations and workloads when learning performance models. Aside from studying the sole effects of workload variation on performance behavior and performance model influences, we assess the quality of such effects. To this end, we enrich our performance observations with corresponding statement coverage data to understand workload variation at a finer grain and see whether workload-specific effects can be attributed to specific code segments or rather depend on workload-specific code utilization.

{\color{blue}Our results show that… This indicates that… In summary, we contribute an empirical study of X configurations and Y workloads across Z configurable software systems as well as a companion website\footnote{\url{https://github.com/anonyms-2021/submission-448/}} with supplementary material including performance and coverage measurements, experiment workloads and configurations, as well as additional visualizations left out due to space limitations.}


%\clearpage
\section{Background and Related Work}

\begin{figure}
	\begin{subfigure}[l]{0.63\linewidth}
		\lstinputlisting[escapechar=\%,label=lst:intro]{examples/introduction.cc}
	\end{subfigure}
	\hfill
	\begin{subfigure}[l]{0.35\linewidth}
		\includegraphics[width=1\linewidth]{images/influences.pdf}
	\end{subfigure}
	\caption{Illustrative example (left) with workload-dependent performance influences of configuration options \texttt{DUPLICATE\_CHECK} and \texttt{AUTOCOMMIT} (right).}
	\label{fig:intro}
\end{figure}

\subsection{Configuration-dependent Performance Modeling}
Configurable software systems are an umbrella term for any kind of software system that exhibits configuration options to customize functionality. 
While the primary purpose of configuration options is to select (categorical or binary options) and tune (numerical options) functionality, each configuration choice may also have implications on non-functional properties --- be it intentional or unintentional. Finding configurations with optimal performance~\cite{nairUsingBadLearners2017,nairFlash18,ohFindingNearoptimalConfigurations2017} and estimating the performance for arbitrary configurations of the configuration space is an established line of research~\cite{siegmundPerformanceinfluenceModelsHighly2015,haDeepPerf2019,perfAL,guoVariabilityawarePerformancePrediction2013,sarkarCostEfficientSamplingPerformance,guo_2018_data,fourier_learning_2015,perLasso}. 
Take as an example of a configurable software system the method \texttt{insertRows} in Figure~\ref{fig:intro} from an exemplary database system. It receives as an argument an array of strings, each a row to insert. 
The method exhibits two configuration options, \texttt{DUPLICATE\_CHECK} and  \texttt{AUTOCOMMIT}. If \texttt{DUPLICATE\_CHECK} is enabled, the passed array of insertion statements is checked for duplicates by  inserting all array items into a \texttt{HashSet} (\colorbox{duplicatecheck}{Lines 2--6}).
If \texttt{AUTOCOMMIT} is enabled, each insertion is treated as a single transaction (\colorbox{autocommit}{Lines 12–14}), wheres the method \texttt{commit} might otherwise be called  after  all rows have been inserted. By enabling or disabling such configuration options, the method's behavior and, thus, its execution time changes.


Machine-learning techniques have been used to learn \emph{performance models} that approximate non-functional properties, such as execution time or memory usage, as a function of software configurations $c \in C$, formally $\Pi: C \rightarrow \mathbb{R}$.
Performance models can be obtained using a variety of machine-learning techniques, including probabilistic programming~\cite{dorn2020}, multiple linear regression~\cite{siegmundPerformanceinfluenceModelsHighly2015}, classification and regression trees~\cite{sarkarCostEfficientSamplingPerformance,guo_2018_data}, Fourier learning~\cite{fourier_learning_2015,perLasso}, and deep neural networks~\cite{haDeepPerf2019,perfAL}.
The set of configurations for training can be sampled from the configuration space using a variety of different sampling techniques. All sampling strategies aim at yielding a representative sample, either by covering main effects of configuration options and interactions among them~\cite{siegmundPredictingPerformanceAutomated2012}, or sampling uniformly from the configuration space~\cite{ohFindingNearoptimalConfigurations2017,kaltenecker_distance-based_2019}.

Most approaches share the perspective of treating a configurable software system as a black-box model at application-level granularity. Recent work has incorporated feature location techniques to guide sampling effort towards relevant configuration options~\cite{velez_2020_configcrusher_jase,velez_comprex_2021} or model non-functional properties at finer granularity~\cite{weber_white_2021}.

\subsection{Workload-dependent Performance Modeling}

In performance and load testing, it its a common practice to select a workload that is reflective of the software system in question~\cite{ceesay2020,papadopoulos2021}. This can be achieved by constructing workloads, among others, from usage patterns~\cite{calzarossa2016}, or by increasing the workload coverage by using a mix of different workloads rather than a single one~\cite{jiang2015survey}.
The benefit of using more than one specific workload for performance models is illustrated by work of \citeauthor{liao_2020_using_emse}~\cite{liao_2020_using_emse}. For two different versions of a software system, they learn performance models based on disjoint sets of workloads. From the comparison of both performance models, they were able to identify performance shifts as the workload bias was averaged out.

Workload-specific differences in the performance behavior of configurable software systems have been documented before~\cite{jamishidi_transfer_2017,alves_sampling_2020}. \citeauthor{jamishidi_transfer_2017} observe that performance influences across different environments remain largely congruent and differences only affect small numbers of configuration options, such that differences can be learned efficiently via transfer learning~\cite{jamishidi_transfer_2017,jamshidi_learning_2018,jamshidi_transfer_gp_2017,ding_bayesian_2020}. Here, the bias is explicitly learned to adapt an already existing --- and potentially also biased --- performance model. \citeauthor{alves_sampling_2020} have observed dissimilar performance distributions for the video transcoder \textsc{x264} over different  workloads.

To illustrate the influence that workloads can have on
configuration performance, let us revisit the introductory example from Figure~\ref{fig:intro}. In addition to being dependent on the configuration, the execution of \texttt{insertRows} also depends on its workload,  the array of insertion statements. If the array size exceeds 50 (line~50), by design all insertions are handled by another method, \texttt{insertBulkRows}. Consequently, the code section depending on option \texttt{AUTOCOMMIT} also depends on the workload size as it is skipped for arrays with more than 50 items. 
To model the performance of any execution of method \texttt{insertRows}, we have to consider the influence of the configuration options, the workload, and the interactions between both. If we assume that each insertion takes a constant amount of time, we expect the method execution time to be proportional to the number of insertion statements. For the individual configuration options, however, the workload size can distort this conception. We illustrate this facet in Figure~\ref{fig:intro}~(right), where the performance influence of both configuration options is shown as a function of the number of insertions. Here, the influence of option \texttt{AUTOCOMMIT} becomes negligible for workloads with more than 50 insertion statements. That is, any configuration  with this option enabled will behave differently depending on the workload. At large, a performance model for this database system learned with a workload of either less or more than 50 insertions cannot generalize to the respective other scenario.


\subsection{Mapping Features to Code}\label{sec:feature_location}
The problem of determining which code implements which functionality in a software system is known as \emph{feature location}~\cite{rubin_feature_2013}. To reason about the degree to what a software feature (e.g., code conditioned by a configuration option) is covered under a workload, a mapping from features to code is necessary. 
%and can be obtained from data and control-flow analysis as well as dynamic analyses.

Most feature location approaches either employ static or dynamic program analysis to infer a mapping between features and code. As for static analyses, variables that encode configuration options (e.g., \texttt{AUTOCOMMIT} in Listing~\ref{fig:intro}) are tainted and, from tracing dataflow and controlflow, one can infer code sections tainted by such variables~\cite{velez_2020_configcrusher_jase,lillack_2018_lotrack_tse,luo_2019_cova}.
While static analyses can yield precise results, scalability is often limited by the exploration of all possible execution paths. To mitigate this shortcoming, dynamic taint analysis taints variables similarly to the static approaches, but only follows one execution path~\cite{bell_phosphor_2014,velez_comprex_2021,splat_kim_2013}. From few executions of different configurations, one can extract feature-specific code. In the context of the example in Listing~\ref{fig:intro}, two runs with \texttt{DUPLICATE\_CHECK} either enabled or disabled, respectively, would allow us to determine the lines of code depending on this option.


Aside from program analysis, a more tame, but also less precise approach is to use code coverage information, such as execution traces.
The rationale is that by exercising feature code, for instance via enabling configuration options or running corresponding tests, its location can be inferred from code coverage differences. Applications of such an approach have been studied not only for feature location~\cite{wong_integrated_2005,sulir_annotation_2015,michelon_spectrum_2021,perez_framing_2016}, but root in program comprehension~\cite{wilde_early_1996,wilde_reconnaissance_1995,sherwood_reducing_nodate,perez_diagnosis_2014,castro_pangolin_2019} and fault localization~\cite{agrawal_fault_1995,wong_faultloc_2016}. In the context of our example from Figure~\ref{fig:intro}, the feature code for options \texttt{DUPLICATE\_CHECK} (\colorbox{duplicatecheck}{Lines 2--6}) and \texttt{AUTOCOMMIT} (\colorbox{autocommit}{Lines 12–14}) could be inferred from the diff between execution traces where each option is once enabled and once disabled, respectively.


\section{Empirical Study}~\label{sec:study}
\todo{glue code}
\subsection{Experiment Setup}\label{sec:setup}
\paragraph*{Subject Systems and Workloads}
For our study, we have selected seven configurable software systems implemented in Java. While this limits the potential corpus of subject systems, we decided to use Java for mainly two reasons: (1) practically, because we can forgo code modifications for instrumentation by using off-line instrumentation (cf.~Section~\ref{sec:profiling}) for code coverage measurement, and (2) strategically, because Java is one of the most widely used programming languages. We discuss this decision in more detail in the Section~\ref{sec:threats}. 
The full list of our subject systems and characteristics is presented in Table~\ref{tab:subject_systems}. Due to space limitations, we provide a more extensive description of the used workloads at the paper's companion Web site~\footnote{\url{paper web site}}.
	
\jumper\footnote{\url{https://github.com/Sciss/jump3r/}} is a re-implementation of the LAME audio codec for MP3 in Java. In total, we have selected six WAVE audio files as a workload to encode to MP3. To diversify the selection of workloads, we indcluded audio files with different characteristics (sampling rate, number of channels, length etc.). 
\kanzi\footnote{\url{https://github.com/flanglet/kanzi/}} is a file compression tool. For this subject system, we selected nine workloads, including, among others, dedicated compression benchmarks (sets of files of different types) as well as binary and text files at different scales: a binary of the Linux kernel, a dump of its repository, XML and CSV data. 
\dconvert\footnote{\url{https://github.com/patrickfav/density-converter/}} is a Java utility to scale images for use in Android apps into different formats and different resolutions. For our experiments, we selected a range of workloads, varying in file type (JPEG, PNG, PSD, and SVG) and size.
\batik is a Java utility to rasterize vector graphics, for which we selected a range of workloads varying in size.
\jadx is a decompiler and deobfuscator for Android applications, for which we selected a range of popular APK packages as workloads, varying in application domain and size.
\htwo is a relational database which can be integrated into Java applications or serve as a standalone. We used a selection of application-level benchmarks from the \textsc{OLTPBench}~\cite{difallah_oltp_2013}, a load generator for databases that allows for testing benchmarks, such as \textsc{TPC-C}. Overall, the selection of the workloads was driven by diversity to make use of different functionality of a subject system and by size to probe for scaling and utilization factors.
	
\begin{table*}[ht!]
		\centering
		\caption{Subject system characteristics}
		\input{tables/subject_systems.tex}
		\label{tab:subject_systems}
	\end{table*}

\paragraph*{Configuration Sampling}\label{sec:sampling}
For each subject system, we sampled a set of configurations. Exhaustive coverage of the configuration space is infeasible due to combinatorial explosion~\cite{henardCombining2015}. We combine several coverage-based sampling strategies and uniform random sampling into an \emph{ensemble} approach: to study the influence of single configuration options, we employ option-wise and negative option-wise sampling, where each option is enabled once, or all except one, respectively. In the same way, pair-wise sampling applies this schema to study influences of two-way interactions between configuration options. Interactions of higher degree can be found accordingly, which however, is computationally expensive. Instead, we augment our sample set with a random sample that is, at least, twice the size of the coverage-based sample. To achieve a uniform random sample, we used \emph{distance-based sampling}~\cite{kaltenecker_distance-based_2019}. The variability models as well as sample sets can be found on the paper's companion Web site.
	
\paragraph*{Coverage Profiling}\label{sec:profiling}
	To measure code coverage, we opted for the on-the-fly profiler \textsc{JaCoCo}\footnote{\url{https://github.com/jacoco/jacoco/}}. Unlike profilers that use source code instrumentation, with on-the-fly instrumentation, the executable binary remains unchanged. This way, we avoid altering and re-compiling the source code or injecting instrumentation code post compilation. From a practical perspective, on-the-fly instrumentation is more flexible and easier to accommodate. Nonetheless, either choice introduces some performance overhead. To avoid this overhead distorting our performance measurements, we conducted our coverage analysis in a separate run, for which no performance data was collected. 	
	
	\paragraph*{Hardware Setup}
	All experiments were conducted on three different compute clusters and each subject system was exclusively run on a single cluster. All machines in a compute cluster had the identical hardware setup: either with Intel~Core~i5-8259U CPUs at 2.3~GHz (\jumper and \kanzi),  Intel~Core~i7-8559U CPUs at 2.7~GHz (\dconvert, \batik, and \jadx) and 32~GB of RAM, respectively, or Intel~Xeon~E5-2630~v4 CPUs at 2.2~GHz with 256~GB of RAM (\htwo). All clusters ran a headless Debian 10 installation, the first two with kernel version \mbox{\texttt{4.19.0-14}}, the latter with version \mbox{\texttt{4.19.0-17}}. 
	To minimize measurement noise, no additional user processes were running in the background and no other than necessary packages were installed.
	For all data points, we report the median across five repetitions (except for \htwo), which has shown to be a good trade-off between variance and measurement effort. For \htwo, we omitted the repetitions as in a pre-study since we found that across all benchmarks the throughput coefficient of variation (standard deviation divided by the arithmetic mean) was consistently below~5\,\%. 


\subsection{Performance Distribution Similarity}\label{sec:rq1}
Previous work has illustrated that performance variation can be attributed to differences in the workload. While the performance distributions (i.e., the set of performance observations over a selection of configurations) of \textsc{x264} observed by \citeauthor{alves_sampling_2020} exhibit little to no similarity to each other~\cite{alves_sampling_2020}, for another series of subject systems, the influences of options (or their order) remained mostly stable, while only few shifted~\cite{jamshidi_transfer_gp_2017,jamishidi_transfer_2017}.  The similarity between workload-specific performance distributions observed by \citeauthor{jamshidi_learning_2018} is key to adapt existing performance models to a new workload~\cite{jamshidi_learning_2018}. 

In a practical setting, the question arises whether, and if so, to what extent an existing workload-specific performance model is representative of the performance behavior of other workloads as well. 
That is, can a model estimating performance of different configuration be reused for the same software system, but learned under a different workload?
Depending on the degree of similarity, such a performance model could be either re-used, transformed at a reasonable cost or must be re-learned entirely, which entails a substantial measurement cost. To provide some context for the feasibility of model reuse or transformation, we formulate the following research question: 

\RQ{1}{To what extent do performance distributions over configurations differ across workloads?}

\paragraph*{Operationalization}
We address this research question by comparing workload-specific performance distributions obtained from our experiments (cf. Section~\ref{sec:study}). To this end, we investigate whether any two distributions are identical, can be transformed into each other (e.g., by scaling or shifting), or are not related in any way. As no single-valued metric can reliably describe the above properties, we employ an aggregate of multiple metrics: First, to assess whether two distributions are identical, we use the paired-sample \emph{Wilcoxon signed-rank test}. If we cannot reject the null hypothesis ($H_0$) that the distributions come from the same same distribution (at significance level $\alpha=0.95$), we do not consider any further metric. 
Otherwise, we explore whether the distributions exhibit any degree of similarity (i.e., whether they can be transformed into each other). If the distributions can be transformed using linear transformation, such as scaling and shifting, we expect a high linear correlation, which we measure by Pearson’s correlation coefficient (\emph{Pearson's~r}). As an additional metric, we use Kendall’s rank correlation coefficient (\emph{Kendall's~$\tau$}) to test for further possible transformations. As the rank correlation usually subsumes linear correlation, a low linear but high rank correlation indicates that only a non-linear transformation is possible. In the case where rank correlation does not indicate a relationship, we consider the distributions dissimilar. From these three metrics, we classify the pairs of workload-specific distributions into four different categories:
\vspace{1mm}

\begin{tabular}{p{3.9cm}l}
	 \textbf{Category} & \textbf{Criteria} \\
	{similar} & $p \geq 0.05$ \\
	{linearly transformable} & $p < 0.05$ and $r \geq 0.6$ \\
	{monotonically transformable} & $p < 0.05$, $r < 0.6$, and $\tau \geq 0.6$ \\
	{dissimilar}  & (otherwise) \\%$p < 0.05$ and $\tau < 0.6$ \\
\end{tabular}

\paragraph*{Results} {\color{blue} Heatmaps with Kendall's $\tau$ + Categories count Figure~\ref{fig:diff_performance_distribution}}  and Table~\ref{tab:categorization}\\

\greybox{\textbf{Summary} (\RQref{1}): \ldots}


\subsection{Performance Influence Similarity}\label{sec:rq2}
Our categorization of the differences between workload-specific performance distributions does help understand the degree of workload variability but leaves out the role of software configuration options. From a practical perspective, it is important to ask whether all configurations of a software system are equally likely to be affected by a workload choice or whether the selection of some configuration option increases or decreases this likelihood. If some configuration options show to be more sensitive to workload choices, practitioners can incorporate this knowledge into selecting configurations for regression testing or to adapt existing performance models~\cite{jamshidi_learning_2018}. To provide a clearer picture of the influence of workload variability in the presence of configuration options, we ask the following research question:

\RQ{2}{To what extent do influences of individual options (and pairwise interactions) depend on the workload choice?}

\paragraph*{Operationalization}
We address this research question by learning and comparing performance models from the measurements of our experiments. As performance models effectively estimate the performance influence of one or more options on a configuration, we construct models not to predict unseen configurations but simply to explain differences observed in the performance distributions. So, for the construction of our performance models, we deliberately learn performance models using the entire sample set. To correct for multicollinearity~\cite{Daoud_2017}, for the training step we exclude all mandatory configuration options. In addition, for each group of mutually exclusive configuration options, we discard one randomly selected group member.

Many different machine techniques have been used to obtain performance models, among others, based on linear regression~\cite{perLasso,siegmundPerformanceinfluenceModelsHighly2015} and classification-and-regression-trees~\cite{sarkarCostEfficientSamplingPerformance,guo_2018_data}. In this vein, we construct both a simple linear model using the least-squares method and a non-linear model based on random forests (both based on the Python implementation in the \texttt{sklearn} library). For both models, we allow single options and pairwise combinations, since our ensemble sampling technique (cf. Section~\ref{sec:sampling}) covered pairwise interactions and additional configurations selected randomly. This way, we can both ensure that, to some degree, higher-order interactions are considered and that the observations are not simply memorized by the explanatory performance model. For the linear model, we report and compare all its coefficients rankings. We report rankings rather than the coefficients themselves, to account for possible scaling effects between two distributions. For the random forest, we report and compare its feature importance measures, a measure of how much each feature (i.e., configuration option) helps to divide the data. 

From the comparison of model parameters, we can answer the research question in detail and assess (1) how many configuration options and interactions, on average, change their influence/importance, and (2) how many configuration options, in total, change their influence/importance.

\paragraph*{Results} {\color{blue} Differences between performance models Figure~\ref{fig:diff_performance_similarity}}\\

\begin{table}
	\caption{Categorization of performance distribution similarity of workload pairs (highest category count highlighted)}
	\begin{tabular}{p{1.7cm}rrrr}
		\toprule
		\textbf{Software System} & similar & LT & MT & dissimilar\\
		\midrule
		\jumper & 0 (0\,\%)& 7 (46.6\,\%) & \cellcolor{nicegreen!20}8 (53.3\,\%)& 0 (0\,\%)\\
		\kanzi & 0 (0\,\%)& \cellcolor{nicegreen!20}28 (77.8\,\%)& 4 (11.1\,\%) & 4 (11.1\,\%)\\
		\dconvert & 0 (0\,\%)& 29 (43.9\,\%) & 0 (0\,\%) & \cellcolor{nicegreen!20}37 (56.0\,\%)\\
		\htwo & 0 (0\,\%)& 11 (39.3\,\%) & 0 (0\,\%) & \cellcolor{nicegreen!20}17 (60.7\,\%)\\
		\batik & 0 (0\,\%)& \cellcolor{nicegreen!20}28 (50.9\,\%) & 8 (14.5\,\%) & 19 (34.5\,\%)\\
		\jadx  & 0 (0\,\%)& \cellcolor{nicegreen!20}120 (100\,\%) & 0 (0\,\%) & 0 (0\,\%)\\
		\bottomrule
	\end{tabular}
	\label{tab:categorization}
\end{table}

\begin{figure*}
	\centering
	\begin{subfigure}{0.33\textwidth}
		\centering
		\includegraphics[width=\linewidth]{images/rq1/kendall_batik.pdf}
		\caption{\batik}
	\end{subfigure}
	\begin{subfigure}{0.33\textwidth}
		\centering
		\includegraphics[width=\linewidth]{images/rq1/kendall_dconvert.pdf}
		\caption{\dconvert}
	\end{subfigure}
	\begin{subfigure}{0.33\textwidth}
		\centering
		\includegraphics[width=\linewidth]{images/rq1/kendall_h2.pdf}
		\caption{\htwo}
	\end{subfigure}
	\begin{subfigure}{0.33\textwidth}
		\centering
		\includegraphics[width=\linewidth]{images/rq1/kendall_jump3r.pdf}
		\caption{\jumper}
	\end{subfigure}
	\begin{subfigure}{0.33\textwidth}
		\centering
		\includegraphics[width=\linewidth]{images/rq1/kendall_jadx.pdf}
		\caption{\jadx}
	\end{subfigure}
	\begin{subfigure}{0.33\textwidth}
		\centering
		\includegraphics[width=\linewidth]{images/rq1/kendall_kanzi.pdf}
		\caption{\kanzi}
	\end{subfigure}
	\caption{Similarity matrix (Kendall's $\tau$) for performance distributions of different pairs of workloads.}
	\label{fig:diff_performance_similarity}
\end{figure*}

 \greybox{\textbf{Summary} (\RQref{2}) ...}



\subsection{Feature Execution and Performance}\label{sec:rq3}
The description of the effects of workload variation so far has taken only black-box measurements into account. To raise the experiment results to practicality, we want to know whether the sensitivity of different configuration options (or interactions) to workload variation manifests itself only in performance differences, or if it can be understood at a finer granularity, at code usage. To draw parallels with software testing, where we determine which code is actually executed, we now extend our perspective on the problem with statement coverage data. 

%The rationale behind this is that performance emerges, among others\footnote{The hardware setup is plausibly a deciding factor for performance behavior as well~\cite{jamishidi_transfer_2017}, but was not varied in this experiment. We discuss this limitation in the threats to validity.}, from configuration and workload choice.

For illustration, let us revisit the introductory example from Listing~\ref{lst:intro}, where the code highlighted in\label{sec:rq3} red depends on the selection of configuration option \texttt{DUPLICATE\_CHECK}, but, once selected, its influence is proportional to the number of queries to handle. By contrast, the invocation of method \texttt{insertBulkRows} later on solely depends on the number of queries exceeding the minimum of 50 queries. While both examples consider workload-specific behavior, the execution in the former depends on the configuration option and the latter example depends on workload properties. 
For practitioners, understanding not only whether, but also, how configuration options interact with workload choices is useful for assessing a performance model with regard to representativeness. 

To start translate our findings of performance variation under varying workloads into actionable strategies, we require insights about where and how workloads and configurations interact. For instance, if a performance model is based on code sections that are covered only under a specific workload, the code’s contribution to performance cannot be accounted for. Similarly, if performance-relevant code is covered under a variety of workloads, but each workload stressed this code segment differently, the performance-influence is likely biased by each individual workload, rendering each workload's performance model unrepresentative. Knowing where (for which options) and how (by coverage or different code utilization) configurations and workloads interact, paves an avenue towards actively searching for workloads that increase utilization or maximize coverage.

With the last part of this evaluation, we shed light on this aspect and explore the relation between the execution of configurable software systems and the observed performance and answer the following research question:

\RQ{3}{To what extent do varying workloads influence both performance behavior and set of covered code lines?}

To approach the research question, we focus on differences between pairs of workloads at two different levels of granularity. First, for each individual configuration $c$, we assess  how its performance and code coverage differ between two workloads $w_1$ and $w_2$. Second, we compare each option's performance influence and option code coverage across pairs of workloads. Hence, we split \RQref{3} into two research questions:

\RQ{3.1}{To what extent are configuration performance and configuration code coverage influenced by varying workloads?}

\RQ{3.2}{To what extent are option's performance influences and option code coverage influenced by varying workloads?}

\paragraph*{Operationalization (RQ~3.1)} For each triple $(c, w_1, w_2)$, we compute the relative performance $\hat{\pi}_{w}(c)$, which is the observed performance $\pi_{w_i}(c)$ divided by the average performance under workload $w_i$ for both $w_1$ and $w_2$. 

\begin{equation}
	\hat{\pi}_{w_i}(c_i) =  \frac{1}{\mid C \mid} \cdot \frac{\pi_{w_i}(c_i)}{\sum_{c\in C} \pi_{w_i}(c)}
\end{equation}

Likewise, for configuration $c$, we compare the sets of covered lines under each of both workloads. From all lines $S_{w_i}(c)$ covered under workload $w_i$ and configuration $c$, we substract the lines that are \emph{core code} of this configuration, which is covered under \emph{all} workloads $W$:

\begin{equation}
\hat{S}_{w_i}(c)	 = S_{w_i}(c) \setminus \bigcap_{w\in W} S_{w}(c)
\end{equation}

For each tuple $(c, w_1, w_2)$ of a software system, we compute the quotient 
$\frac{\hat{\pi}_{w_1}(c) }{ \hat{\pi}_{w_2}(c) }$, where $\hat{\pi}_{w_1}(c) \leq \hat{\pi}_{w_2}(c)$, to indicate performance differences and the Jaccard set similarity of both coverage sets $\hat{S}_{w_1}(c)$ and $\hat{S}_{w_2}(c)$. As a similiarity metric, we use the Jaccard set similarity index defined as

\begin{equation}
	sim (A, B) = \frac{\mid A \cap B\mid}{\mid A\cup B \mid}.
\end{equation} 

We report the relationship between both the relative performance quotient and the coverage similarity to see whether, and if so, how both correlate. If a configuration exhibits little to no performance variation, but shows high variation in its code coverage, this indicates that the option-specific code here does not contribute substantially to performance variation. Conversely, if the performance observations show a high degree of variation while largely similar code is executed, we can infer that, if at all, the code does only contribute to performance variation due to differences in how it is utilized. While the case is unlikely for configurations that exhibit a linear relationship, we can infer that code coverage variation could serve as a proxy for performance variation. 

\paragraph*{Results} {\color{blue} Figure~\ref{fig:diff_config}}\\
\greybox{\textbf{Summary} (\RQref{3.1}): ...}

\begin{figure*}
	\centering
	\begin{subfigure}{0.49\textwidth}
		\centering
		%\includegraphics[width=\linewidth]{images/mockup.png}
		\caption{\batik (execution time)}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		%\includegraphics[width=\linewidth]{images/mockup.png}
		\caption{\dconvert (execution time)}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		%\includegraphics[width=\linewidth]{images/mockup.png}
		\caption{\htwo (throughput)}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		%\includegraphics[width=\linewidth]{images/mockup.png}
		\caption{\jumper (execution time)}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		%\includegraphics[width=\linewidth]{images/mockup.png}
		\caption{\jadx (execution time)}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		%\includegraphics[width=\linewidth]{images/mockup.png}
		\caption{\kanzi (execution time)}
	\end{subfigure}
	\caption{Relationship between differences in configuration code coverage and differences in relative configuration performance for pairs of workloads.}
	\label{fig:diff_config}
\end{figure*}



\paragraph*{Operationalization (RQ~3.2)}
{\color{magenta}From the results in Figure~\ref{fig:diff_config}, we see that a clear relationship between performance variation and code coverage variation might be rare in practice.} However, to deeper investigate the variation in both metrics, we abstract from single configurations to configuration options and relate the performance influences and importances from \RQref{2} with the amount of option-dependent code covered under the respective workloads.

To determine which code can be attributed to a single configuration option or interactions among them, we employ a two-step process. First, we obtain a baseline of all code that depends on a configuration option within the scope of our entire benchmark selection. For each workload $w \in W$, we compute the set of code lines that depend on option $o \in O$. Let $C_{o}$ be the set of configurations with option $o$ selected, and $C_{\neg o}$ with option $o$ deselected, respectively. To obtain $S_{w, o}$, we follow a strategy similar to spectrum-based feature location~\cite{michelon_spectrum_2021} (cf. Section~\ref{sec:feature_location}) and subtract th4 set of the code lines covered under $C_{\neg o}$ from those of $C_{o}$:

\begin{equation}
	S_{w, o} = \bigcup_{p \in C_{o}} S_{w}(p) ~ \setminus ~ \bigcup_{q \in C_{\neg o}} S_{w}(q)
\end{equation}

While $S_{w, o}$ yields an approximation of option-dependent code for a single workload, we aggregate the approximations for each workload $w \in W$ to obtain the set of lines that depend on a configuration option $o$ and are executed in, at least, one workload,~$S_{o}$. 

\begin{equation}
	S_{o} = \bigcup_{w \in W} S_{w, o}
\end{equation}

\todo{maybe code example based on 1}
While this aggregated set should not be mistaken for a ground truth, it enables us to reason about differences in option-dependent code within the scope of our selected workloads. That is, the expressiveness of this baseline depends on the diversity of the workloads in question. We discuss this limitation in Section~\ref{sec:threats}. From the ratio of option-dependent code per workload to option-dependent code across workloads, $\mid S_{w_1, o}\mid/~{\mid S_{w_2, o}\mid}$, we can estimate the coverage of option-dependent code. From comparing the sets $S_{w_1, o}$ and $S_{w_2, o}$ for any two workloads $w_1$ and $w_2$, we can estimate a similarity between the option-code coverage.

\paragraph*{Results}{\color{blue} ... Figure~\ref{fig:diff_performance_option_coverage}}\\

\begin{figure*}
	\centering
	\begin{subfigure}{0.33\textwidth}
		\centering
		(image)
		%\includegraphics[width=\linewidth]{images/mochup_heatmap.png}
		\caption{\batik}
	\end{subfigure}
	\begin{subfigure}{0.33\textwidth}
		\centering
		(image)
		%\includegraphics[width=\linewidth]{images/mochup_heatmap.png}
		\caption{\dconvert}
	\end{subfigure}
	\begin{subfigure}{0.33\textwidth}
		\centering
		(image)
		%\includegraphics[width=\linewidth]{images/mochup_heatmap.png}
		\caption{\htwo}
	\end{subfigure}
	\begin{subfigure}{0.33\textwidth}
		\centering
		(image)
		%\includegraphics[width=\linewidth]{images/mochup_heatmap.png}
		\caption{\jumper}
	\end{subfigure}
	\begin{subfigure}{0.33\textwidth}
		\centering
		(image)
		%\includegraphics[width=\linewidth]{images/mochup_heatmap.png}
		\caption{\jadx}
	\end{subfigure}
	\begin{subfigure}{0.33\textwidth}
		\centering
		(image)
		%\includegraphics[width=\linewidth]{images/mochup_heatmap.png}
		\caption{\kanzi}
	\end{subfigure}
	\caption{Differences in performance influence/importance vs differences in option code coverage}
	\label{fig:diff_performance_option_coverage}
\end{figure*}

\greybox{\textbf{Summary} (\RQref{3.2}):}

\section{Discussion}
\paragraph*{Interpretation of the Results}
\paragraph*{Implications for Transfer Learning}
\paragraph*{Coverage as a Proxy Metric?}
\paragraph*{Workload-aware Performance Models}
\section{Threats to Validity}\label{sec:threats}

\paragraph*{Internal Validity}\label{sec:internal_validity}
Threats to \emph{internal validity} include measurement noise which may distort our classification into categories (Section~\ref{sec:rq1}) and model construction (Section~\ref{sec:rq2}). We mitigate this threat by repeating each experiment five times and reporting the median as a robust measure. For \htwo, we confirmed a negligible measurement variation in a separate pre-study.
Another potential threat is that the coverage analysis with \mbox{\textsc{JaCoCo}} entails a noticable instrumentation overhead, which may can distort performance observations. We mitigate this threat by separating the experiment runs for coverage assessment and performance measurement. In the case of \htwo, the load generator of the \textsc{OLTPBench} framework~\cite{difallah_oltp_2013} ran on the same machine as the database since we were testing an embedded scenario, but thus introduced only negligible variation and overhead.
\paragraph*{External Validity}\label{sec:external_validity}
The selection of subject systems all written in Java poses a threat to \emph{external validity}. While our motivation for this selection is primarily practical in nature (cf. Section~\ref{sec:setup}), we mitigate this threat by selecting subject systems from a large variety of application domains (cf.~Table~\ref{tab:subject_systems}). Nonetheless, one cannot necessarily conclude that our results hold for other domains and programming languages. 
\paragraph{Construct Validity}\label{sec:construct_validity}
The profiler \textsc{JaCoCo} instruments Java byte code instructions rather than statements, which leads to some minor imprecision when mapping covered instructions back to the original statements. Transitively, this also affects the precision of our feature location (cf. Section~\ref{sec:rq3}). This drawback, however, is not limited to this particular profiler, but applies to most profilers operating on byte code. 
Superimposition in our feature location technique is sound, but, by design, incomplete, since no set of workloads is guaranteed to cover all feature code. We selected a variety of different workloads as a counter measure, yet our study is of exploratory nature.

{\color{blue} 
\section{Conclusion}

Software performance emerges from a multitude of factors, of which configuration options have been widely studied and used for learning performance models. As workloads are, so far, an overlooked factor when learning configuration-dependent performance models, we study the influence of workloads as an orthogonal factor, individually and in conjunction with configuration options. 
\todo{why}

In an empirical study of X configurations and Y workloads, we explore both how varying workloads affect the performance behavior and execution across Z software systems. 

We found that \ldots. From our results \ldots.

In further work, we will incorporate further factors of variation, such as hardware variability, but also the evolution of configurable software systems, into our analysis, providing a more holistic picture of performance variation and ultimately enable selecting representative workloads for learning robust and generalizable performance models. 
}