\section{Introduction}
%\subsection*{Performance models are important}
Most software systems can be customized via configuration options to meet user demands. The selection of configuration options can enable desired functionality (features) or tweak non-functional aspects of a software system, such as improving performance or energy consumption. 
All too often, software performance bugs can be linked to configuration options~\cite{han_empirical_2016}. 
The relationship of configuration choices and their influence on non-functional properties has been extensively studied in recent years. Most work focuses on the prediction of non-functional properties\footnote{In this paper, we use the terms \textit{non-functional properties} and \textit{performance} interchangably, although the latter is more restricive. In the context of this work a performance model describes any model that predicts one or more non-functional properties.}, such as execution time or memory utilization, for arbitrary configurations~\cite{dorn2020,siegmundPerformanceinfluenceModelsHighly2015,haDeepPerf2019,perfAL,guoVariabilityawarePerformancePrediction2013,sarkarCostEfficientSamplingPerformance,guo_2018_data,fourier_learning_2015,perLasso} or on finding configurations that are near-optimal with regard to a non-functional property~\cite{nairUsingBadLearners2017,nairFlash18,ohFindingNearoptimalConfigurations2017}. 
The backbone of performance estimation is a model that maps a given configuration to the estimated performance value. 
In general, performance prediction models are only useful to practitioners if predictions hold for a large number of application scenarios.

%\subsection*{Performance models can be workload-biased}
Learning performance models for software performance usually relies on a training set of configuration-specific performance measurements. 
Typically, established approaches employ only a single workload (e.g., a benchmark or test suite) to measure performance, which usually aims at emulating a specific real-world application scenario. 
However, as the collected measurements represent only the one application scenario, the resulting performance model can  estimate performance for this single scenario reliably. 
The choice of a single workload can introduce bias as the predictive model is only as useful as the underlying workload resembles a wide range of use cases in the real world. 
Pereira et al. illustrate this fact with an analysis of the video encoder \textsc{x264} under different workloads~\cite{alves_sampling_2020}. 
For two different workloads, they observed vastly different performance distributions across a large set of configurations.
While this particular observation can be an outlier, in general, one cannot assume that performance behavior under a single workload is generalizable, and therefore consistent in every real-world setting.
In this vein, a recent exploratory study by Jamshidi et al.~\cite{jamishidi_transfer_2017} illustrates that performance models of a configurable software system often exhibit differences in estimated performance influences when learned in different environments, including different versions, hardware setups, and workloads.
Performance  influences across different environments remain largely congruent and differences only affect small numbers of configuration options, such that differences can be learned efficiently via transfer learning~\cite{jamshidi_transfer_gp_2017,jamshidi_learning_2018}. 
Nonetheless, under a single workload, we are left with performance models whose estimations are potentially inaccurate for different environments. A specific workload might miss some lines of code and, hence, it is impossible to get performance information about such code regions.

%\subsection*{Towards Estimating Representativeness}

Given not one, but a set of performance models based on different workloads, the question arises: Which performance model of the set is most general, whose estimations are more likely to \emph{represent} real-world scenarios? 
To assess which model's estimations to trust most, we require information about whether and how different workloads interact with the software system under test. 

Take as an example the method in Listing~\ref{lst:intro} from an imaginary database system. It receives as an argument an array of strings, each a row to insert. The method exhibits two configuration options, of whose selection some code sections depend: if \texttt{DUPLICATE\_CHECK} is enabled, the passed array is checked for duplicates via a separate method invocation (lines 2--4), and if \texttt{AUTOCOMMIT} is enabled, each insertion is treated as a single transaction (lines 10--12). The latter section, however, not only depends on the selection of a configuration option, but also on the number of rows to insert. If the workload (here, the number of rows) is large enough, the individual insertion per row (line 9) is replaced by another insertion method (line 6). This example illustrates, how the execution of a program could depend on both configuration decisions (lines 2--4) and workloads (lines 5--9), or interactions between them (lines 10--12).

\begin{figure}
\begin{subfigure}[l]{0.63\linewidth}
	
\lstinputlisting[caption=Workload-dependent code ,escapechar=\%,label=lst:intro]{examples/introduction.cc}

\end{subfigure}
	\begin{subfigure}[l]{0.35\linewidth}
		\includegraphics[width=1\textwidth]{images/example_workload_influences/linear.pdf}
		\includegraphics[width=1\textwidth]{images/example_workload_influences/linear.pdf}
		\includegraphics[width=1\textwidth]{images/example_workload_influences/linear.pdf}
	\end{subfigure}
	\caption{Illustrative example (left) with workload-dependent performance influences (right)}
	\label{fig:intro}
\end{figure}
\todo{Einbauen}


To now representatively assess the method's performance, one would require not only observations under a set of sample configurations, but also under different workloads to cover and measure performance for workload-dependent code. If dependent code, such as the interaction between \texttt{AUTOCOMMIT} and the workload size is missed, the estimation of an option's performance influence becomes unrepresentative and, thus, the performance model unreliable.

%\subsection*{Approach, Results \& Contribution}
In this paper, we explore how the selection of a workload can influence performance models how prone these are to be distorted. To this end, we extend our perspective from pure black-box measurements~\cite{dorn2020,siegmundPerformanceinfluenceModelsHighly2015,haDeepPerf2019,perfAL,guoVariabilityawarePerformancePrediction2013,sarkarCostEfficientSamplingPerformance,guo_2018_data,fourier_learning_2015,perLasso} with knowledge about the code executed. Specifically, we enrich performance measurements from different configurations under varying workloads with corresponding easy-to-compute code coverage data. 
We conduct an empirical study of seven configurable software systems to explore how the selection of workloads can influence both performance and code coverage, and possibly distort performance models (cf. Section~\ref{sec:study}). {\color{teal}We found that \ldots.} 
Based on our findings, we explore correlations between code coverage and differences in performance influence (Section~\ref{sec:metric}). {\color{teal}	We found that \ldots. We propose to use code coverage as a \ldots and devise a novel method to interpret performance model with regard to code coverage in order to estimate their representativeness.}

In summary, we offer the following contributions:
\begin{compactitem}
	\item A companion Web site\footnote{\url{https://github.com/icse-submitter123/submission}} providing supplementary material including performance measurements, code coverage data, and additional visualizations.
	\item more 
\end{compactitem}

\section{Background and Related Work}
\subsection{Performance Prediction Modeling}
Configurable software systems are an umbrella term for any kind of software system that exhibits configuration options to customize functionality. 
While the primary purpose of configuration options is to select (categorical or binary options) and tune (numerical options) functionality, each configuration choice may also have implications on non-functional properties---be it intentional or unintentional. Finding configurations with optimal performance~\cite{nairUsingBadLearners2017,nairFlash18,ohFindingNearoptimalConfigurations2017} and estimating the performance for arbitrary configurations is an established line of research~\cite{siegmundPerformanceinfluenceModelsHighly2015,haDeepPerf2019,perfAL,guoVariabilityawarePerformancePrediction2013,sarkarCostEfficientSamplingPerformance,guo_2018_data,fourier_learning_2015,perLasso}. 

For the latter, machine-learning techniques are used to learn \emph{performance models} that approximate non-functional properties, such as execution time or memory usage, as a function of software configurations $c \in C$, formally $\Pi: C \rightarrow \mathbb{R}$.
Performance models can be obtained using a variety of machine-learning techniques, including probabilistic programming~\cite{dorn2020}, multiple linear regression~\cite{siegmundPerformanceinfluenceModelsHighly2015}, classification and regression trees~\cite{sarkarCostEfficientSamplingPerformance,guo_2018_data}, Fourier learning~\cite{fourier_learning_2015,perLasso}, and deep neural networks~\cite{haDeepPerf2019,perfAL}.
The set of configurations for training can be sampled from the configuration space using a variety of different sampling techniques. All sampling strategies aim at yielding a representative sample, either by covering main effects of configuration options and interactions among them~\cite{siegmundPredictingPerformanceAutomated2012}, or sampling uniformly from the configuration space~\cite{ohFindingNearoptimalConfigurations2017,kaltenecker_distance-based_2019}.

Most approaches share the perspective of treating a configurable software system as a black box model at application-level granularity. Besides, recent work has incorporated feature location techniques to guide sampling effort towards relevant configuration options~\cite{velez_2020_configcrusher_jase,velez_comprex_2021} or model non-functional properties similarly, but at finer granularity~\cite{weber_white_2021}.

\subsection{Performance under Varying Workloads}
Differences in the performance behavior of configurable software systems have been documented before~\cite{jamishidi_transfer_2017,alves_sampling_2020}. Jamshidi et al. observe that performance influences across different environments remain largely congruent and differences only affect small numbers of configuration options, such that differences can be learned efficiently via transfer learning~\cite{jamishidi_transfer_2017,jamshidi_learning_2018,jamshidi_transfer_gp_2017,ding_bayesian_2020}. Here, the bias is explicitly learned to adapt an already existing – and potentially also biased -- performance model. The benefit of using more than one specific workload for performance models is illustrated by work of Liao et al.~\cite{liao_2020_using_emse}. For two different versions of a software system, they learned performance models based on disjoint sets of workloads. From the comparison of both performance models, they were able to identify performance shifts as the workload bias was averaged out.
Closely related to our work, aspects of workload variability and bias have been studied for micro-benchmarks, such as test suites for performance testing. Laaber et al. use static analysis to estimate the stability (convergence after repeated measurements) of a given micro-benchmark based on usage of specific code features~\cite{laaber_emse_2021}. Similarly to our problem, yet for micro-benchmarks instead of system-level workloads, Grambow et al. use execution logs to infer redundant or missing coverage~\cite{grambow_peerj_2021}. With this approach, they are able to condense benchmark suites to select only relevant ones and also recommend code sections to test with further workloads. In contrast to our work, this does not consider configuration options.
%\pdfcomment[color=green!25]{
%Die lernen aber ein Performance-Model nicht anhand von Konfigurationen so wie wir es tun, %sondern anhand von Logs, um auch Änderungen der Workloads leicht abdecken zu können. Ich finde, dass man das verdeutlichen sollte.
%}


{\color{gray}
\subsection{Associating Code and Features}\label{sec:feature_location}
The problem of determining what code sections implement certain functionality in a software system is known as \emph{feature location}~\cite{rubin_feature_2013}. To reason about the degree to what a software feature (i.e., code conditioned by a configuration option) is covered under a workload, knowledge of corresponding code regions is necessary and can be obtained from data and control-flow analysis as well as dynamic analyses.

Most feature location approaches either employ static or dynamic program analysis to infer an association between code and features. For static analyses, variables that encode configuration options (e.g., \texttt{AUTOCOMMIT} in Listing~\ref{lst:intro}) are tainted and from following data-flow and control-flow, one can infer code sections tainted by such variables~\cite{velez_2020_configcrusher_jase,lillack_2018_lotrack_tse,luo_2019_cova}.
While static analyses can yield precise results, scalability is often limited by the exploration of possible execution paths. To mitigate this shortcoming, dynamic taint analysis taints variables similarly to the above works, but only follows one execution path~\cite{bell_phosphor_2014,velez_comprex_2021,splat_kim_2013}. From few executions of different configurations one can then extract feature-specific code from the code that was covered. In the context of the example in Listing~\ref{lst:intro}, two runs with \texttt{DUPLICATE\_CHECK} either enabled or disabled, respectively, would allow to infer dependent lines of code.


Aside from program analysis, a more tame, but also less precise approach is to use code coverage information, such as execution traces.
The rationale is that by exercising feature code, for instance via enabling configuration options or running corresponding tests, its location can be inferred from code coverage differences. Applications of such an approach have been studied not only for feature location~\cite{wong_integrated_2005,sulir_annotation_2015,michelon_spectrum_2021,perez_framing_2016}, but root in program comprehension~\cite{wilde_early_1996,wilde_reconnaissance_1995,sherwood_reducing_nodate,perez_diagnosis_2014,castro_pangolin_2019} and fault localization~\cite{agrawal_fault_1995,wong_faultloc_2016}. In the context of our example from Listing~\ref{lst:intro}, the feature code for option \texttt{DUPLICATE\_CHECK} could be inferred from the diff between execution traces where that option is once enabled and once disabled.
}

\section{Empirical Study}~\label{sec:study}

{
	\color{black}
	\subsection{Experiment Setup}
	\subsubsection{Subject Systems \& Workloads}
	For our study, we have selected seven configurable software systems implemented in Java. While this limits, among others, the corpus of subject systems to be considered, we decided to use Java for mainly two reasons. Practically, because we can forgo code modifications for instrumentation by using off-line instrumentation (cf.~Section~\ref{sec:profiling}) for code coverage measurement. Strategically, because we can limit the influence of different measurement pipelines. We discuss this decision in more detail in the threats to validity (cf.~Section~\ref{sec:threats}). A full list of our subject systems and characteristics is presented in Table~\ref{tab:subject_systems}, a more extensive description of the worklaods used can be found at the paper's companion Web site.
	\jumper\footnote{\url{https://github.com/Sciss/jump3r/}} is a re-implementation of the LAME audio codec for MP3 in Java. In total, we have selected {\color{red}six} WAVE audio files as a workload to encode to MP3. While the selection of workloads was exploratory, we indcluded audio files with different characteristics, such as sampling rate, number of channels, and length. 
	\kanzi\footnote{\url{https://github.com/flanglet/kanzi/}} is a file compression tool. For this subject system, we selected {\color{red}ten} workloads, including, among others, dedicated compression benchmarks (sets of files of different types) as well as binary and text files at different scales: a binary of the Linux kernel, a dump of its repository, XML and CSV data. 
	\dconvert\footnote{\url{https://github.com/patrickfav/density-converter/}} is a Java utility to scale images for use in Android apps into different formats and different resolutions. For our experiments, we selected a range of workloads, varying in file type (JPEG, PNG, PSD, and SVG) and size.
	\batik is a Java utility to rasterize vector graphics, for which we selected a range of workloads varying in size.
	\jadx is a decompiler and deobfuscator for Android applications, for which we selected a range of popular APK packages as workloads, varying in application domain and size.
	\htwo is a relational database which can be integrated into Java applications or serve as a standalone. We opted for the former operating mode and used a selection of application-level benchmarks from the \textsc{OLTPBench}~\cite{difallah_oltp_2013}, a load generator for various databases that allows for testing benchmarks, such as TPC-C. Simialrly, \hsqldb is a relational database that offers the same operating modes. We stressed \hsqldb with the same selection of benchmarks from \textsc{OLTPBench}.
	
	\begin{table*}[ht!]
		\centering
		\caption{Subject System Characteristics}
		\input{tables/subject_systems.tex}
		\label{tab:subject_systems}
	\end{table*}

	\subsubsection{Configuration Sampling}
	For all subject systems, we sampled a set of configurations since exhaustive assessment of the configuration space is infeasible due to combinatorial explosion. We combine several coverage-based sampling strategies and uniform random sampling into an \emph{ensemble} approach: to study the influence of single configuration options, we employ feature-wise and negative feature-wise sampling, where each feature is enabled once, or all except one, respectively. In the same way, pair-wise sampling applies this schema to study influences of two-way interactions between configuration options. Interactions of higher degree can be found accordingly, however, is computationally expensive. Instead, we augment our sample set with a random sample that is at least twice the size of the coverage-based sample subset. To achieve a uniform random sample, we used \emph{distance-based sampling}~\cite{kaltenecker_distance-based_2019}, which ensures uniform coverage of the configuration space. The variability models as well as sample sets can be found at the paper's companion web site.
	
	\subsubsection{Coverage Profiling}\label{sec:profiling}
	To measure code coverage, we opted for the on-the-fly profiler \textsc{JaCoCo}\footnote{\url{https://github.com/jacoco/jacoco/}}. Unlike profilers that use source code instrumentation, such as \textsc{Clover}\footnote{\url{https://openclover.org/}}, or off-line byte code instrumentation, such as \textsc{Cobertura}\footnote{\url{https://cobertura.github.io/cobertura/}}. With on-the-fly instrumentation the executable binary remains unchanged. That is, we avoid altering and re-compiling the source code or injecting instrumentation code post compilation. From a practical perspective, on-the-fly instrumentation is more flexible and easier to accommodate. Nonetheless, either choice is likely to introduce significant performance overhead. For this reason, we conducted performance measurements and coverage measurements in separate runs.
	
	\subsubsection{Hardware Setup}
	All experiments were conducted on three different compute clusters with an identical hardware setup: either with Intel~Core~i5-8259U CPUs at 2.3~GHz (\jumper, \kanzi, and \hsqldb),  Intel~Core~i7-8559U CPUs at 2.7~GHz (\dconvert, \batik, and \jadx) and 32~GB of RAM, respectively, or Intel~Xeon~E5-2630~v4 CPUs at 2.2~GHz with 256~GB of RAM (\htwo). All clusters ran a headless Debian 10 installation, the first two with kernel version \mbox{4.19.0-14}, the latter with version \mbox{4.19.0-17}. 
	
	To minimize measurement noise, no additional user process were running in the background and no other than necessary packages were installed.
	For all data points, we report the median across five repetitions (except for \hsqldb or \htwo), which has shown to be a good trade-off between variance and measurement effort. For the two database systems, we omitted the repetitions as in a pre-study, we found that across all benchmarks the throughput coefficient of variation (standard deviation divided by the arithmetic mean) was consistently below 5\,\%.
}

\subsection{Effects on Performance and Coverage}
Workload selection can determine the coverage of code regions, but this does not necessarily translate to observable shifts in performance and vice versa. Nonetheless, both metrics, code coverage and performance, in isolation are used to make statements about the quality of a software system. We argue that, even without drawing a connection between both, exploring the influence that the selection of different workloads can have is worthwhile, for instance, to select for stable workloads or robust configurations. Hence, we first explore what influence different workloads can have on performance behavior, as well as code coverage and answer the overall question:

\RQ{1}{Does the workload choice influence the configuration-specific variation of both performance and code coverage?}

Previous work has already observed both notable differences in the distribution of configuration-specific performance~\cite{alves_sampling_2020} as well as the prevalence of some options’ influence remaining congruent across environments. The latter patterns have been exploited adapting performance models to new workloads via transfer learning~\cite{jamishidi_transfer_2017,jamshidi_transfer_gp_2017}, and guiding configuration sampling towards options whose influence is likely influenced by a workload choice~\cite{jamshidi_learning_2018}. 

\subsubsection{Performance Variation}\label{sec:performance_variation}
To provide a clearer picture of \textit{how} the performance of a subject systems in question is affected, we assess workload influence in detail by answering the following research question:

% performance vartiance across workloads
\RQ{1.1}{How similar is configuration-specific performance variation across workloads?}
{\color{blue}
\paragraph{Operationalization} To asses whether, and if so, how the distribution of configuration-specific performance across workloads differs for a subject system, we employ two different statistical measures. As a first measure, to check whether two performance distributions are similar, we use the \textit{Wilcoxon signed-rank test}, a non-parametric paired difference test whose null hypothesis is that two samples come from the same distribution. For distribution pairs, where we can reject the null hypothesis, we use as a further measure Kendall's $\tau$ rank correlation coefficient~\cite{kendall1938new}. This will tell us, whether there is any monotonic relationship between the performance distributions. 

This approach lets us divide pairs of workloads into three different categories, where a) the performance distributions of configurations are virtually identical, b) transformed such that they retain their ranks, or c) not related. For the latter two cases, the distinction is especially important, as a high rank correlation implies that a performance model for either workload retains some generalizability since a well or poorly performing configuration in one distribution is doing so in the respective other distribution.

\paragraph{Results} \ldots
}
\subsubsection{Code Coverage Variation}\label{sec:coverage_spectra}

Similarly to the research question~\RQref{1.1}, we want to know how much the selection of a workload influences the execution of the subject system in question (i.e., what code is covered). Therefore, we ask the following research question:

\RQ{1.2}{How similar is configuration-specific code-coverage variation across workloads?}
{\color{blue}
\paragraph{Operationalization} To see, how configurations' code coverage is affected by the choice of the workload, for each configurations, we compare the code coverage across all workloads. In this context, we express the code coverage by the set of lines covered for a configuration under a specific workload. For each pair of workloads, we can compute the Jaccard set similarity, which is defined as the ratio of common lines and total lines covered under any of the two workloads. For each subject system, we report the distribution of this set similarity. 

\paragraph{Results} \ldots
}

\subsubsection{Performance and Code Coverage}\label{sec:performance_coverage_correlation}
fgfg

\RQ{1.3}{Does workload variance correspond to differences in configuration performance and code coverage?}

{\color{blue}
\paragraph{Operationalization} We compare the workload-specific differences in performance from \RQref{1.1}, expressed as differences in their respective ranks, with the workload-specific differences in code coverage from \RQref{1.2}, expressed as the Jaccard set similarity to check for a possible relationship. To check for any monotic relationship in general, and a linear relationship in particular, we report both the the Kendall's $\tau$ rank correlation coefficient and Pearson's correlation coefficient, respectively. 


}
\subsection{Effects on Performance Models}

\RQ{2}{Does workload variance correspond to differences in configuration performance influence and code coverage?}

\subsubsection{Effect on Performance Influences} j

\RQ{2.1}{Do performance-influence ranks vary across workloads?}

{\color{blue}
\paragraph{Operationalization} 

\begin{compactitem}
	\item Construct performance-influence models for each workload with up to three-way interactions
	\item Report rank correlation coefficients for each pair of workloads
	\item The higher the correlation, the more similar the performance influence models are
\end{compactitem}

\paragraph{Results} \ldots
}

\subsubsection{Effect on Option-Code Coverage} d

\RQ{2.2}{Does option-dependent code coverage vary across workloads?}
{\color{blue}
\paragraph{Operationalization} 

\begin{compactitem}
	\item For each workload, compute the set of option-dependent code (i.e., code only visible if an option \texttt{OPT} is enabled
	\item For each pair of workloads, calculate the set similarity across features
\end{compactitem}

\paragraph{Results} \ldots
}
\subsubsection{Performance Influences and Option-Code Coverage} d

\RQ{2.3}{Does better option-code coverage make performance models more robust?}

{\color{blue}
	\paragraph{Operationalization} 
	
	\begin{compactitem}
		\item Take the performance influence rankings from \RQref{2.1} and the set similarities from \RQref{2.2} 
		\item Correlate the differences in performance-influence ranks (across options) with set similarities in(across options) 
	\end{compactitem}
	 
	\paragraph{Results} \ldots
}


\section{Discussion}
\subsection{Representative Workloads}
\subsection{Representative Performance Models}
\clearpage

\section{Threats to Validity}\label{sec:threats}
\paragraph{Internal Validity}\label{sec:internal_validity}

\begin{compactitem}
	\item dedicated hardware to mitigate measurement noise, 
	\item five repetitions per data point, 
	\item database experiments robustness assessed in pre-study,
	\item separate experiment runs for coverage analysis and performance measurement to avoid profiling averhead (assuming determinism)
	\item load generator overhead for databases, hence omitted memory consumption and other hardware-related performance indicators 
\end{compactitem}

\paragraph{External Validity}\label{sec:external_validity}
%A threat to generalizability is the selection of our subject systems since we only considered software written in Java. 
%While we deliberately opted for this class of subject systems for practical reasons, we address facet by selecting a large range of software systems and workloads from different application domains. 
%While one cannot generalize findings to other programming languages or environments without context, we are confident that domain-specific findings, such as for databases, may hold for further similar software systems.
\begin{compactitem}
	\item Java only, but different application domains,
	\item consistent with observations from related work
\end{compactitem}

\paragraph{Construct Validity}\label{sec:construct_validity}
\begin{compactitem}
	\item \textit{off-line instrumentation} vs. on-line instrumentation
	\begin{compactitem}
		\item avoid compiler optimizations or overhead from instrumented source code
		\item byte code ad-hoc instrumentation could increase overhead in a separate run
		\item byte-code instrumentation with Jacoco less precise than other techniques since mapping byte code instructions back to Java source code is not always trivial (observed before~\cite{luo_2019_cova})
	\end{compactitem}
	\item Coverage analysis for middle- to low-level languages face instrumentation bias
\end{compactitem}

\section{Conclusion}