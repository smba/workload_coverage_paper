%\clearpage
\section{Introduction}
\subsection*{Motivation}
Most software systems can be customized via configuration options to meet user demands. The selection of configuration options can enable desired functionality (features) or tweak non-functional aspects of a software system, such as improving performance or energy consumption. 
The relationship of configuration choices and their influence on performance has been extensively studied in literature.
The backbone of performance estimation is a model that maps a given configuration to the estimated performance value. 
Learning performance models usually relies on a training set of configuration-specific performance measurements. 
To measure performance, established approaches~\cite{dorn2020,siegmundPerformanceinfluenceModelsHighly2015,haDeepPerf2019,perfAL,guoVariabilityawarePerformancePrediction2013,sarkarCostEfficientSamplingPerformance,guo_2018_data,fourier_learning_2015,perLasso} usually employ only a single workload (e.g., a benchmark or test suite), which aims at emulating a specific real-world application scenario.

Conversely, for performance and load testing in practice, workloads are a widely considered as a key factor for performance variation~\cite{ceesay2020,papadopoulos2021}.
In the domain of cloud computing applications~\cite{papadopoulos2021} and data-intense applications~\cite{ceesay2020}, methodological principles emphasize the importance of using representative workloads or testing against a set of workloads that reflect the system under test and account for workload-specific behavior. Methods for finding such representative workloads are known under the umbrella term \emph{workload characterization}~\cite{calzarossa2016} and are widely used alongside mixes of different workloads in practice~\cite{jiang2015survey}. 
%To illustrate the relevance and effectiveness of varying workloads for performance assessment, consider the following example by \citeauthor{liao_2020_using_emse}: For two different versions of a software system,they learn performance models based on disjoint sets of workloads.
%From the comparison of both performance models, they were able
%to identify performance shifts as the workload bias was averaged
%out~\cite{liao_2020_using_emse}.

Both configuration-dependent and workload-dependent performance behavior have been studied mostly in isolation. While there are some attempts to gain insights on their combination for specific application scenarios~\cite{alves_sampling_2020} and under limited variability~\cite{liao_2020_using_emse}, to the best of our knowledge, we are the first to systematically explore the intersection of both factors of variation. We aim at providing a clear picture of how prevalent and severe workload-specific differences in configuration-dependent performance are and how they emerge in the wild. 
To illustrate, how both factors may interact, consider the introductory example of an imaginary database system in Figure~\ref{fig:intro}. The method in this example handles an array of insertions. The workload determines how configuration-dependent code is utilized, such as the execution of \colorbox{duplicatecheck}{Lines 2--6} for configuration option \texttt{DUPLICATE\_CHECK}. A performance model that considers this dependency as an input factor might be more representative than a performance model that was trained on a single workload. If a workload, however, conditions the execution of configuration-specific source code, such as \colorbox{autocommit}{Lines 12--14} for configuration option \texttt{AUTOCOMMIT}, one can derive a more representative performance modes from selecting workloads that increase overall code coverage, similar to test coverage.

%In absence of a systematic study that sheds light on whether and how configuration options and workload choices interact with regard to performance, we address this isssue in this paper. 
We have conducted an empirical study of 29\,014 configurations and 55 workloads across 6 configurable software systems to provide a broad picture of the interaction of configuration and workload when learning performance models and estimating a configuration's performance (i.e., response time). Aside from studying the sole effects of workload variation on performance behavior and performance model influences, we explore \emph{how} both factors interact. To this end, we enrich performance observations with corresponding statement coverage data to understand workload variation at a finer grain. This way, we reveal whether workload-specific effects can be attributed to specific code segments or rather depend on workload-specific code usage.
\todo{results}

{\color{blue}In summary, we contribute 
	
\begin{itemize}
	\item an empirical study of 29\,014 configurations and 55 workloads across 6 configurable software systems of whether and how the interaction of workloads with configuration options influences performance, and
	\item a companion website\footnote{\url{https://github.com/anonyms-2021/submission-448/}} with supplementary material including performance and coverage measurements, experiment workloads and configurations, as well as additional visualizations left out due to space limitations.
	
\end{itemize}
}


%\clearpage
\section{Background and Related Work}

\begin{figure}
	\begin{subfigure}[l]{0.63\linewidth}
		\lstinputlisting[escapechar=\%,label=lst:intro]{examples/introduction.cc}
	\end{subfigure}
	\hfill
	\begin{subfigure}[l]{0.35\linewidth}
		\includegraphics[width=1\linewidth]{images/influences.pdf}
	\end{subfigure}
	\caption{Illustrative example (left) with workload-dependent performance influences of configuration options \texttt{DUPLICATE\_CHECK} and \texttt{AUTOCOMMIT} (right).}
	\label{fig:intro}
\end{figure}

\subsection{Configuration-dependent Performance Modeling}
Configurable software systems are an umbrella term for any kind of software system that exhibits configuration options to customize functionality. 
While the primary purpose of configuration options is to select (categorical or binary options) and tune (numerical options) functionality, each configuration choice may also have implications on non-functional properties --- be it intentional or unintentional. Finding configurations with optimal performance~\cite{nairUsingBadLearners2017,nairFlash18,ohFindingNearoptimalConfigurations2017} and estimating the performance for arbitrary configurations of the configuration space is an established line of research~\cite{siegmundPerformanceinfluenceModelsHighly2015,haDeepPerf2019,perfAL,guoVariabilityawarePerformancePrediction2013,sarkarCostEfficientSamplingPerformance,guo_2018_data,fourier_learning_2015,perLasso}. 
Take as an example of a configurable software system the method \texttt{insertRows} in Figure~\ref{fig:intro} from an exemplary database system. It receives as an argument an array of strings, each a row to insert. 
The method exhibits two configuration options, \texttt{DUPLICATE\_CHECK} and  \texttt{AUTOCOMMIT}. If \texttt{DUPLICATE\_CHECK} is enabled, the passed array of insertion statements is checked for duplicates by  inserting all array items into a \texttt{HashSet} (\colorbox{duplicatecheck}{Lines 2--6}).
If \texttt{AUTOCOMMIT} is enabled, each insertion is treated as a single transaction (\colorbox{autocommit}{Lines 12–14}), wheres the method \texttt{commit} might otherwise be called  after  all rows have been inserted. By enabling or disabling such configuration options, the method's behavior and, thus, its execution time changes.


Machine-learning techniques have been used to learn \emph{performance models} that approximate non-functional properties, such as execution time or memory usage, as a function of software configurations $c \in C$, formally $\Pi: C \rightarrow \mathbb{R}$.
Performance models can be obtained using a variety of machine-learning techniques, including probabilistic programming~\cite{dorn2020}, multiple linear regression~\cite{siegmundPerformanceinfluenceModelsHighly2015}, classification and regression trees~\cite{sarkarCostEfficientSamplingPerformance,guo_2018_data}, Fourier learning~\cite{fourier_learning_2015,perLasso}, and deep neural networks~\cite{haDeepPerf2019,perfAL}.
The set of configurations for training can be sampled from the configuration space using a variety of different sampling techniques. All sampling strategies aim at yielding a representative sample, either by covering main effects of configuration options and interactions among them~\cite{siegmundPredictingPerformanceAutomated2012}, or sampling uniformly from the configuration space~\cite{ohFindingNearoptimalConfigurations2017,kaltenecker_distance-based_2019}.

Most approaches share the perspective of treating a configurable software system as a black-box model at application-level granularity. Recent work has incorporated feature location techniques to guide sampling effort towards relevant configuration options~\cite{velez_2020_configcrusher_jase,velez_comprex_2021} or model non-functional properties at finer granularity~\cite{weber_white_2021}.

\subsection{Workload-dependent Performance Modeling}

In performance and load testing, it its a common practice to select a workload that is reflective of the software system in question~\cite{ceesay2020,papadopoulos2021}. This can be achieved by constructing workloads, among others, from usage patterns~\cite{calzarossa2016}, or by increasing the workload coverage by using a mix of different workloads rather than a single one~\cite{jiang2015survey}.
The benefit of using more than one specific workload for performance models is illustrated by work of \citeauthor{liao_2020_using_emse}~\cite{liao_2020_using_emse}. For two different versions of a software system, they learn performance models based on disjoint sets of workloads. From the comparison of both performance models, they were able to identify performance shifts as the workload bias was averaged out.

Workload-specific differences in the performance behavior of configurable software systems have been documented before~\cite{jamishidi_transfer_2017,alves_sampling_2020}. \citeauthor{jamishidi_transfer_2017} observe that performance influences across different environments remain largely congruent and differences only affect small numbers of configuration options, such that differences can be learned efficiently via transfer learning~\cite{jamishidi_transfer_2017,jamshidi_learning_2018,jamshidi_transfer_gp_2017,ding_bayesian_2020}. Here, the bias is explicitly learned to adapt an already existing --- and potentially also biased --- performance model. \citeauthor{alves_sampling_2020} have observed dissimilar performance distributions for the video transcoder \textsc{x264} over different  workloads.

To illustrate the influence that workloads can have on
configuration performance, let us revisit the introductory example from Figure~\ref{fig:intro}. In addition to being dependent on the configuration, the execution of \texttt{insertRows} also depends on its workload,  the array of insertion statements. If the array size exceeds 50 (line~50), by design all insertions are handled by another method, \texttt{insertBulkRows}. Consequently, the code section depending on option \texttt{AUTOCOMMIT} also depends on the workload size as it is skipped for arrays with more than 50 items. 
To model the performance of any execution of method \texttt{insertRows}, we have to consider the influence of the configuration options, the workload, and the interactions between both. If we assume that each insertion takes a constant amount of time, we expect the method execution time to be proportional to the number of insertion statements. For the individual configuration options, however, the workload size can distort this conception. We illustrate this facet in Figure~\ref{fig:intro}~(right), where the performance influence of both configuration options is shown as a function of the number of insertions. Here, the influence of option \texttt{AUTOCOMMIT} becomes negligible for workloads with more than 50 insertion statements. That is, any configuration  with this option enabled will behave differently depending on the workload. At large, a performance model for this database system learned with a workload of either less or more than 50 insertions cannot generalize to the respective other scenario.


\subsection{Mapping Features to Code}\label{sec:feature_location}
The problem of determining which code implements which functionality in a software system is known as \emph{feature location}~\cite{rubin_feature_2013}. To reason about the degree to what a software feature (e.g., code conditioned by a configuration option) is covered under a workload, a mapping from features to code is necessary. 
%and can be obtained from data and control-flow analysis as well as dynamic analyses.

Most feature location approaches either employ static or dynamic program analysis to infer a mapping between features and code. As for static analyses, variables that encode configuration options (e.g., \texttt{AUTOCOMMIT} in Listing~\ref{fig:intro}) are tainted and, from tracing dataflow and controlflow, one can infer code sections tainted by such variables~\cite{velez_2020_configcrusher_jase,lillack_2018_lotrack_tse,luo_2019_cova}.
While static analyses can yield precise results, scalability is often limited by the exploration of all possible execution paths. To mitigate this shortcoming, dynamic taint analysis taints variables similarly to the static approaches, but only follows one execution path~\cite{bell_phosphor_2014,velez_comprex_2021,splat_kim_2013}. From few executions of different configurations, one can extract feature-specific code. In the context of the example in Listing~\ref{fig:intro}, two runs with \texttt{DUPLICATE\_CHECK} either enabled or disabled, respectively, would allow us to determine the lines of code depending on this option.


Aside from program analysis, a more tame, but also less precise approach is to use code coverage information, such as execution traces.
The rationale is that by exercising feature code, for instance via enabling configuration options or running corresponding tests, its location can be inferred from code coverage differences. Applications of such an approach have been studied not only for feature location~\cite{wong_integrated_2005,sulir_annotation_2015,michelon_spectrum_2021,perez_framing_2016}, but root in program comprehension~\cite{wilde_early_1996,wilde_reconnaissance_1995,sherwood_reducing_nodate,perez_diagnosis_2014,castro_pangolin_2019} and fault localization~\cite{agrawal_fault_1995,wong_faultloc_2016}. In the context of our example from Figure~\ref{fig:intro}, the feature code for options \texttt{DUPLICATE\_CHECK} (\colorbox{duplicatecheck}{Lines 2--6}) and \texttt{AUTOCOMMIT} (\colorbox{autocommit}{Lines 12–14}) could be inferred from the diff between execution traces where each option is once enabled and once disabled, respectively.


\section{Empirical Study}~\label{sec:study}
We have conducted an empirical study of a multitude of configurations and workloads for six configurable software systems. In the following, we describe the general experiment setup and study design as well as research questions and results.

\subsection{Experiment Setup}\label{sec:setup}
\paragraph*{Subject Systems and Workloads}
For our study, we have selected six configurable software systems implemented in Java. We decided to use Java for mainly two reasons: (1) practically, because we can forgo code modifications for instrumentation by using off-line instrumentation (cf.~Section~\ref{sec:profiling}) for code coverage measurement, and (2) strategically, because Java is one of the most widely used programming languages. We discuss this decision in more detail in the Section~\ref{sec:threats}. 
The full list of our subject systems and characteristics is presented in Table~\ref{tab:subject_systems}. Due to space limitations, we provide a more extensive description of the used workloads at the paper's companion Web site.
	
\jumper is a re-implementation of the LAME audio codec for MP3 in Java. In total, we have selected six WAVE audio files as a workload to encode to MP3. To diversify the selection of workloads, we indcluded audio files with different characteristics (sampling rate, number of channels, length etc.). 
\kanzi is a file compression tool. For this subject system, we selected nine workloads, including, among others, dedicated compression benchmarks (sets of files of different types) as well as binary and text files at different scales: a binary of the Linux kernel, a dump of its repository, XML and CSV data. 
\dconvert is a Java utility to scale images for use in Android apps into different formats and different resolutions. For our experiments, we selected a range of workloads, varying in file type (JPEG, PNG, PSD, and SVG) and size.
\batik is a Java utility to rasterize vector graphics, for which we selected a range of workloads varying in size.
\jadx is a decompiler and deobfuscator for Android applications, for which we selected a range of popular APK packages as workloads, varying in application domain and size.
\htwo is a relational database which can be integrated into Java applications or serve as a standalone. We used a selection of application-level benchmarks from the \textsc{OLTPBench}~\cite{difallah_oltp_2013}, a load generator for databases that allows for testing benchmarks, such as \textsc{TPC-C}. Overall, the selection of the workloads was driven by diversity to make use of different functionality of a subject system and by size to probe for scaling and utilization factors.
	
\begin{table*}[ht!]
		\centering
		\caption{Subject system characteristics}
		\input{tables/subject_systems.tex}
		\label{tab:subject_systems}
	\end{table*}

\paragraph*{Configuration Sampling}\label{sec:sampling}
For each subject system, we sampled a set of configurations. Exhaustive coverage of the configuration space is infeasible due to combinatorial explosion~\cite{henardCombining2015}. We combine several coverage-based sampling strategies and uniform random sampling into an \emph{ensemble} approach: To study the influence of single configuration options, we employ option-wise and negative option-wise sampling~\cite{siegmundPerformanceinfluenceModelsHighly2015}, where each option is enabled once, or all except one, respectively. In the same way, pair-wise sampling applies this schema to study influences of two-way interactions between configuration options. Interactions of higher degree can be found accordingly, which however, is computationally probibitively expensive~\cite{henardCombining2015}. Instead, we augment our sample set with a random sample that is, at least, twice the size of the coverage-based sample. To achieve a uniform random sample, we used \emph{distance-based sampling}~\cite{kaltenecker_distance-based_2019}. The variability models as well as sample sets can be found on the paper's companion Web site.
	
\paragraph*{Coverage Profiling}\label{sec:profiling}
To measure code coverage, we use the on-the-fly profiler \textsc{JaCoCo}. Unlike profilers that use source code instrumentation, with on-the-fly instrumentation, the executable binary remains unchanged. This way, we avoid altering and re-compiling the source code or injecting instrumentation code post compilation. From a practical perspective, on-the-fly instrumentation is more flexible and easier to accommodate. Nonetheless, either choice introduces some performance overhead. To avoid this overhead distorting our performance measurements, we conducted our coverage analysis in a separate run, for which no performance data was collected. 	
	
\paragraph*{Hardware Setup}
All experiments were conducted on three different compute clusters and each subject system was exclusively run on a single cluster. All machines in a compute cluster had the identical hardware setup: either with Intel~Core~i5-8259U CPUs at 2.3~GHz (\jumper and \kanzi),  Intel~Core~i7-8559U CPUs at 2.7~GHz (\dconvert, \batik, and \jadx) and 32~GB of RAM, respectively, or Intel~Xeon~E5-2630~v4 CPUs at 2.2~GHz with 256~GB of RAM (\htwo). All clusters ran a headless Debian 10 installation, the first two with kernel version \mbox{\texttt{4.19.0-14}}, the latter with version \mbox{\texttt{4.19.0-17}}. 
To minimize measurement noise, no additional user processes were running in the background and no other than necessary packages were installed.
	For all data points, we report the median across five repetitions (except for \htwo), which has shown to be a good trade-off between variance and measurement effort. For \htwo, we omitted the repetitions as in a pre-study since we found that across all benchmarks the throughput coefficient of variation (standard deviation divided by the arithmetic mean) was consistently below~5\,\%. 


\subsection{Workload-specific Configuration Performance}\label{sec:rq1}
It is well known that performance variation can arise from differences in the workload~\cite{benchmarking_book}. 
This has been observed as well for software systems in the presence of configuration options, such as for the video encoder x264~\cite{alves_sampling_2020}. \citeauthor{alves_sampling_2020} show that, under two different workloads, the configuration-specific performance behavior of x264 varies widely. 

%{\todo{fuzzy}\color{orange}For example, while the set of performance observations over a selection of configurations of \textsc{x264} observed by  exhibit little to no similarity to each other, for another series of subject systems, the influences of options (or their order) remained mostly stable, while only few shifted~\cite{jamshidi_transfer_gp_2017,jamishidi_transfer_2017}.}  The similarity between workload-specific performance distributions observed by \citeauthor{jamshidi_learning_2018} is key to adapt existing performance models to a new workload~\cite{jamshidi_learning_2018}. 

In a practical setting, the question arises whether, and if so, to what extent an existing workload-specific performance model is representative of the performance behavior of other workloads as well. 
That is, can a model estimating performance of different configuration be reused for the same software system, but learned under a different workload?
Depending on the degree of similarity, such a performance model could be either (1) re-used, (2) transformed at a reasonable cost, or (3) must be re-learned entirely, which entails a substantial measurement cost. To provide some context for the feasibility of model reuse or transformation, we formulate the following research question: 

\RQ{1}{To what extent does performance behavior vary across workloads?}

\paragraph*{Operationalization}
We address this research question by comparing workload-specific performance distributions obtained from our experiments (cf. Section~\ref{sec:study}). To this end, we investigate whether any two distributions obtained from two different workloads can be transformed into each other, or are not related in any way. As no single-valued metric can reliably describe the above properties, we employ an aggregate of multiple metrics: 

If the distributions can be transformed using linear transformation, such as scaling and shifting, we expect a high linear correlation, which we measure by Pearson’s correlation coefficient (\emph{Pearson's~r}). As an additional metric, we use Kendall’s rank correlation coefficient (\emph{Kendall's~$\tau$}) to test for further possible transformations. As rank correlation usually subsumes linear correlation, a low linear but high rank correlation indicates that only a non-linear transformation is possible. In the case where rank correlation does not indicate a relationship, we consider the distributions dissimilar. From these three metrics, we classify the pairs of workload-specific distributions into four different categories:
\vspace{1mm}

\begin{tabular}{p{4.4cm}l}
	 \textbf{Category} & \textbf{Criteria} \\
	{linearly transformable (LT)} & $r \geq 0.6$ \\
	{monotonically transformable (MT)} & $r < 0.6$, and $\tau \geq 0.6$ \\
	{dissimilar}  & (otherwise) \\%$p < 0.05$ and $\tau < 0.6$ \\
\end{tabular}

\paragraph*{Results}
We found that the performance behavior of the six configurable software systems varied widely. We illustrate the results in Table~\ref{tab:categorization}. For half of the software systems (\kanzi, \batik, and \jadx), in the majority of workload pairs, the resulting performance distributions could be transformed using a linear transformation. For \jumper, about more than half of the workload pairs are accountable to monotonic tranformation. For \dconvert and \htwo, the majority of workload comparisons resulted in dissimilar performance distributions, however a substantial amount was linearly transformable. For \jumper and \jadx, we found no case of non-transformable performance distributions, whereas the other four software systems exhibited dissimlar performance distributions under, at least, one workload. For \jadx, all performance distributions were linearly transformable.

In Figure~\ref{fig:diff_performance_similarity}, we provide a more detailed breakdown of which workloads resulted in performance distributions dissimilar to other ones in terms of a correlation matrix (Kendall's $\tau$) for each pair of workloads. Notable for \dconvert is the behavior for SVG workloads (and one JPG workload), which resulted in distributions dissimilar to the remaining workloads. For \htwo, the workloads \texttt{ycsb-2400} and \texttt{tpcc-8} are dissimilar to all other workloads, including their counterparts with a lower scalefactor (600 and 2, respectively).

\begin{table}
	\caption{Category counts of configuration-specific performance for workload pairs}
	\begin{tabular}{p{2.4cm}rrr}
		\toprule
		\textbf{Software System} & \textbf{LT} & \textbf{MT} & \textbf{dissimilar}\\
		\midrule
		\jumper &  7 (46.6\,\%) & \cellcolor{nicegreen!20}8 (53.3\,\%)& 0 (0\,\%)\\
		\kanzi &  \cellcolor{nicegreen!20}28 (77.8\,\%)& 4 (11.1\,\%) & 4 (11.1\,\%)\\
		\dconvert &  29 (43.9\,\%) & 0 (0\,\%) & \cellcolor{nicegreen!20}37 (56.0\,\%)\\
		\htwo &  11 (39.3\,\%) & 0 (0\,\%) & \cellcolor{nicegreen!20}17 (60.7\,\%)\\
		\batik &  \cellcolor{nicegreen!20}28 (50.9\,\%) & 8 (14.5\,\%) & 19 (34.5\,\%)\\
		\jadx  &  \cellcolor{nicegreen!20}120 (100\,\%) & 0 (0\,\%) & 0 (0\,\%)\\
		\bottomrule
	\end{tabular}
	\label{tab:categorization}
	
	{\vspace{2mm}
		{\footnotesize Abbreviations:$\quad$LT: \textit{Linearly transformable}, MT: \textit{Monotonically transformable}}
	\vspace{0.1cm}}
	
\end{table}

\begin{figure*}
	\centering
	\begin{subfigure}{0.33\textwidth}
		\centering
		\includegraphics[width=\linewidth]{images/rq1/kendall_batik.pdf}
		\caption{\batik}
	\end{subfigure}
	\begin{subfigure}{0.33\textwidth}
		\centering
		\includegraphics[width=\linewidth]{images/rq1/kendall_dconvert.pdf}
		\caption{\dconvert}
	\end{subfigure}
	\begin{subfigure}{0.33\textwidth}
		\centering
		\includegraphics[width=\linewidth]{images/rq1/kendall_h2.pdf}
		\caption{\htwo}
	\end{subfigure}
	\begin{subfigure}{0.33\textwidth}
		\centering
		\includegraphics[width=\linewidth]{images/rq1/kendall_jump3r.pdf}
		\caption{\jumper}
	\end{subfigure}
	\begin{subfigure}{0.33\textwidth}
		\centering
		\includegraphics[width=\linewidth]{images/rq1/kendall_jadx.pdf}
		\caption{\jadx}
	\end{subfigure}
	\begin{subfigure}{0.33\textwidth}
		\centering
		\includegraphics[width=\linewidth]{images/rq1/kendall_kanzi.pdf}
		\caption{\kanzi}
	\end{subfigure}
	\caption{Similarity matrix (Kendall's $\tau$) for performance distributions of different pairs of workloads.}
	\label{fig:diff_performance_similarity}
\end{figure*}
\vspace{1em}
\greybox{\textbf{Summary} (\RQref{1}): Not all software systems' performance behavior changes similarly across workloads. For \dconvert, performance behavior under SVG workloads is different to most other workloads, and for \htwo, performance behavior under workloads with higher scale factors is more likely to be dissimilar to behavior under other workloads.}


\subsection{Performance Model Similarity}\label{sec:rq2}
Our categorization of the differences between workload-specific performance distributions does help understand the degree of workload-dependent performance variation, but leaves out the role of configuration options. From a practical perspective, it is important to ask whether all configurations of a software system are equally likely to be affected by a workload choice or whether the selection of some configuration option increases or decreases this likelihood. If some configuration options show to be more sensitive to workload choices, practitioners can incorporate this knowledge into selecting configurations for regression testing or to adapt existing performance models~\cite{jamshidi_learning_2018}. To provide a clearer picture of the influence of workload variability in the presence of configuration options, we ask the following research question:

\RQ{2}{To what extent do influences of individual configuration options depend on the workload?}

\paragraph*{Operationalization}
We address this research question by learning and comparing performance models from the measurements of our experiments~\cite{dorn2020,siegmundPerformanceinfluenceModelsHighly2015,haDeepPerf2019,perfAL,guoVariabilityawarePerformancePrediction2013,sarkarCostEfficientSamplingPerformance,guo_2018_data,fourier_learning_2015,perLasso}. As a performance models effectively estimates the influence of individual configuration options on performance, we construct models not to predict unseen configurations but simply to explain differences observed in the performance distributions. So, for the construction of our performance models, we deliberately learn performance models using the entire sample set. 
To correct for multicollinearity~\cite{Daoud_2017} and to ensure that the obtained performance influences are interpretable, we drop several configuration options from the sample set, which has shown to be an effective practice~\cite{dorn2020}. For the training step, we exclude all mandatory configuration options since these by definition cannot contribute to performance variation. In addition, for each group of mutually exclusive configuration options, we discard one randomly selected group member.

Many different machine techniques have been used to obtain performance models, among others, based on linear regression~\cite{perLasso,siegmundPerformanceinfluenceModelsHighly2015,dorn2020}. For the purpose of answering \RQref{2}, we learn a prediction model using multiple linear regression (least-squares method). We limit the set of independent variables to individual options rather than higher-order interactions to be consistent with the feature location used for \RQref{3.2} where we determine option-specific, but not interaction-specific code segments
%{\color{orange}The overall number of actually influential configuration options and interactions is, by definition, relatively low~\cite{saltelli}}.

From the comparison of the performance model coefficients, we can answer \RQref{2} in detail and assess (1) how many configuration options, on average, change their influence, and (2) how many configuration options, in total, change their influence. For any pair of workload-specific performance models, we divide the performance model coefficients by the respective mean performance to make the model influences comparable across workloads. Henceforth, we will refer to these coefficients as \textit{relative performance influences.} {\color{blue!60!black}Since these relative performance influences have been divided by the population mean, we report these without any unit.}

\todo{replace tables with graphic}
\begin{table}
	\centering
	\caption{Absolute number of configuration options whose relative performance influence changed by at least by threshold $t$ under at least one workload configuration.}
	\begin{tabular}{lr|rrrrr}
		\toprule
		& &  \multicolumn{5}{c}{\textbf{Difference threshold $t$}} \\
		\textbf{Software System} & \textbf{\#\,Opt.} & 0.01 &  0.05 &  0.1 &  0.2 &  0.5 \\
		\midrule
		\dconvert & 23 &    14 &    12 &    7 &    6 &    1 \\
		\jumper & 36 &   30 &    29 &   29 &   25 &    9 \\
		\batik &  20 &  10 &     7 &    6 &    6 &    5 \\
		\kanzi & 29 &   26 &    26 &   25 &   20 &    9 \\
		\jadx & 20 &   19 &     9 &    5 &    4 &    2 \\
		\htwo & 17 &   16 &     5 &    3 &    1 &    0 \\
		\bottomrule
	\end{tabular}
	\label{tab:total_changes}
\end{table}
\begin{table}
	\caption{Average number of configuration options whose relative performance influence changed by, at least, by threshold $t$ across all workload combinations.}
	\begin{tabular}{lr|rrrrr}
		\toprule
		& & \multicolumn{5}{c}{\textbf{Difference threshold $t$}} \\
		\textbf{Software System} & \textbf{\#\,Opt.} & 0.01 &  0.05 &  0.1 &  0.2 &  0.5 \\
		\midrule
		\dconvert & 23 &  8.89 &  5.29 &  3.55 &  1.65 & 0.09 \\
		\jumper & 36 & 24.73 & 19.87 & 16.27 & 11.40 & 4.33 \\
		\batik & 20 &  7.22 &  4.47 &  3.62 &  2.95 & 1.18 \\
		\kanzi & 29 & 23.61 & 15.83 & 10.33 &  6.33 & 2.56 \\
		\jadx & 20 & 10.32 &  4.28 &  2.51 &  1.38 & 0.33 \\
		\htwo & 17 & 7.39 &  2.43 &  1.04 &  0.43 & 0.00 \\
		\bottomrule
	\end{tabular}
	\label{tab:average_changes}
\end{table}

\paragraph*{Results} 
{\color{blue}We show the distribution of the relative performance model coefficients in the box plots in Figure~\ref{fig:feature_influence_actross_workloads}. We see that, for all software systems, only a small number of configuration options actually has an influence on performance. While for most configuration options, we observe substantial workload-specific variation if they are influential options. Notably though, for \dconvert (option \texttt{png}) and \kanzi (e.g., option \texttt{BWT}), we see that otherwise non-influential options become influential. Moreover, for \batik, \dconvert, and \kanzi, we observe a large number of outlier, which indicates that for these software systems, some options' performance influence can be quite workload-specific.}
To further quantify the cases of such volatile configuration options, we break down the numbers of how many configuration options changed in Tables~\ref{tab:average_changes} and~\ref{tab:average_changes}. In Table~\ref{tab:average_changes}, we present the total number of configuration options whose relative performance influence was different in, at least, one comparison of workload-specific performance models. In lieu of a fixed absolute difference threshold, we report the number of configuration at five different thresholds. For \jumper and \kanzi, more than half of the configuration options' influences changed, at least, by $0.2$, whereas, for the remaining software systems, less than half of the options was affected.
%{\color{blue}To put this in a practical context: If under workload $w_1$, a performance influence was 60 seconds for a population mean performance of one minute, and 600 seconds under workload $w_2$ with a population mean performance of ten minutes, a change of $0.2$ would refer to either a 12 seconds or two minutes difference in the respective performance differences.}\todo{unverständlich}

In this vein, we show the \textit{average} number of configuration options whose influence changed, at least, by $t$ in Table~\ref{tab:average_changes}. We see that, across all software systems and thresholds, the avergae number of configuration options is consistently and substantially lower than the total number of configuration options from Table~\ref{tab:total_changes}. This indicates that not for all workload pair comparisons the same number of configuration options has changed their influence. Notable is that, for the systems for which we observed a large share of dissimilar performance distributions in \RQref{1}, \dconvert and \htwo, only few configuration options appear to be influenced by the choice of workloads.

\begin{figure*}
	\begin{minipage}{0.33\textwidth}
		%\centering
		\begin{subfigure}{\linewidth}
			\includegraphics[width=\linewidth]{images/rq2/relative_performance_batik.pdf}			
			\vspace{15mm}
		\end{subfigure}
	\end{minipage}
	\begin{minipage}{0.33\textwidth}
		\begin{subfigure}{\linewidth}
			\includegraphics[width=\linewidth]{images/rq2/relative_performance_dconvert.pdf}
			\vspace{0.0001mm}
		\end{subfigure}
	\end{minipage}
	\begin{minipage}{0.33\textwidth}
		\begin{subfigure}{\linewidth}
			\includegraphics[width=\linewidth]{images/rq2/relative_performance_h2.pdf}

		\end{subfigure}
	\end{minipage}

	\begin{minipage}{0.33\textwidth}
		\begin{subfigure}{\linewidth}
			\includegraphics[width=\linewidth]{images/rq2/relative_performance_jump3r.pdf}
			\vspace{6mm}
		\end{subfigure}
	\end{minipage}
	\begin{minipage}{0.33\textwidth}
		\begin{subfigure}{\linewidth}
			\includegraphics[width=\linewidth]{images/rq2/relative_performance_jadx.pdf}
		\end{subfigure}
	\end{minipage}
	\begin{minipage}{0.33\textwidth}
		\begin{subfigure}{\linewidth}
			\includegraphics[width=\linewidth]{images/rq2/relative_performance_kanzi.pdf}
			\vspace{1.19cm}
		\end{subfigure}
	\end{minipage}
	\caption{\color{blue}Relative performance influence distribution across over workloads per software system.}
	\label{fig:feature_influence_actross_workloads}
\end{figure*}

\vspace{1em}
\greybox{\textbf{Summary} \color{blue} (\RQref{2}) Most workload-specific variation affects influential configuration options, while some workloads let some options become influential. For the most software systems, less than 50\,\% configuration options' influence changed substantially depending on the workload. For systems with a high variation in performance behavior across workloads (cf.~\RQref{1}), only few configuration options' influence depends on the workload.}


\subsection{Code Coverage and Performance}\label{sec:rq3}
Our analysis of the effects of workload variation so far has taken only black-box measurements into account. In addition, we want to know whether the sensitivity of different configuration options (or interactions) to workload variation manifests itself only in performance differences, or if it can be understood at a finer granularity and travel back to the level of code. To draw parallels with software testing, where we determine which code is actually executed, we now extend our perspective on the problem with statement coverage data. 

%The rationale behind this is that performance emerges, among others\footnote{The hardware setup is plausibly a deciding factor for performance behavior as well~\cite{jamishidi_transfer_2017}, but was not varied in this experiment. We discuss this limitation in the threats to validity.}, from configuration and workload choice.

For illustration, let us revisit the introductory example from Figure~\ref{fig:intro}, where \colorbox{duplicatecheck}{Lines 2--6} depend on the selection of configuration option \texttt{DUPLICATE\_CHECK}. Once selected, its influence is proportional to the number of queries to handle. By contrast, the invocation of method \texttt{insertBulkRows} later on solely depends on the number of queries exceeding the minimum of 50 queries. While both examples exhibit workload-specific behavior, the execution in the former depends on the configuration option and the latter example depends on workload properties. 
For practitioners, understanding not only whether, but also, how configuration options interact with the workload is useful for assessing a performance model with regard to representativeness. 

To start translate our findings of performance variation under varying workloads into actionable strategies, we require insights about where and how workloads and configurations interact. For instance, if a performance model is based on code segments that are covered only under a specific workload, it incorporates some workload bias. Consequently, the model cannot make accurate predictions for performance behavior under another workload, where different code can be executed. Likewise, if performance-relevant code is covered under a variety of workloads, but each workload stressed this code segment differently, the performance-influence is likely biased by each individual workload, rendering each workload's performance model unrepresentative. Knowing where (for which options) and how (by option-code coverage or option code usage) configurations and workloads interact, paves an avenue towards actively searching for workloads that increase utilization or maximize coverage.

In the last part of this evaluation, we shed light on this aspect and explore the relation between the execution of configurable software systems and the observed performance and answer the following research question:

%\RQ{3}{To what extent do varying workloads influence both performance behavior and set of covered code lines?}
\RQ{3}{To what extent do individual options' influence on performance and option-specific code coverage depend on the workload?}

\begin{comment}
{\color{red}
\RQ{3.1}{To what extent do configuration-specific performance and configuration-specific code coverage depend on the workload?}
}
\end{comment}

\begin{comment}
{\color{red}
\paragraph*{Operationalization (RQ~3.1)} For each triple $(c, w_1, w_2)$, we compute the relative performance $\hat{\pi}_{w}(c)$, which is the observed performance $\pi_{w_i}(c)$ divided by the average performance under workload $w_i$, for both $w_1$ and $w_2$: 

\begin{equation}
	\hat{\pi}_{w_i}(c_i) =  \mid C \mid \cdot \frac{\pi_{w_i}(c_i)}{\sum_{c\in C} \pi_{w_i}(c)}
\end{equation}

Likewise, for configuration $c$, we compare the sets of covered lines under each of the two workloads. From all lines $S_{w_i}(c)$ covered under workload $w_i$ and configuration $c$, we substract the lines that are \emph{core code} of this configuration, which is covered under \emph{all} workloads $w_j \in W$:

\begin{equation}
\hat{S}_{w_i}(c)	 = S_{w_i}(c) \setminus \bigcap_{w_j\in W} S_{w_j}(c)
\end{equation}

For each tuple $(c, w_1, w_2)$ of a software system, we compute the ratio 
$\frac{\hat{\pi}_{w_1}(c) }{ \hat{\pi}_{w_2}(c) }$, where $\hat{\pi}_{w_1}(c) \leq \hat{\pi}_{w_2}(c)$, to indicate performance differences and the Jaccard set similarity of both coverage sets $\hat{S}_{w_1}(c)$ and $\hat{S}_{w_2}(c)$. 


We report the relationship between both the relative performance ratio and the coverage similarity to see whether, and if so, how both correlate. If a configuration exhibits little to no performance variation, but shows high variation in its code coverage, this indicates that the option-specific code does not contribute substantially to performance variation. Conversely, if the performance observations show a high degree of variation while largely similar code is executed, we can infer that, if at all, the code does only contribute to performance variation due to differences in how it is executed. While the case is unlikely for configurations that exhibit a linear relationship, we can infer that code coverage variation could serve as a proxy for performance variation. 

\paragraph*{Results}
We found that the configuration-specific code covered by different workloads can vary widely, but almost always resulted in groups similar to each other. We illustrate this pattern in Figure~\ref{fig:example_coverage_performance}. For each triple $(c, w_1, w_2)$ of a configuration and workload pair (here: \texttt{corona} and \texttt{mandelbrot} for \batik), the x-axis denotes the observed similarity in configuration code coverage and the y-axis represents the corresponding relative difference in performance. We see that performance variation (across the y-axis) is independent from variation in the code coverage. We found this pattern in all subject systems and workload combinations this pattern occured. We provide the full set of similar graphics in the supplementary material to this paper.
\vspace{1em}	
\greybox{\textbf{Summary} (\RQref{3.1}): \color{red}The code coverage of configuration-specific code does vary across workloads but does not show any relationship to variation in configuration-specific performance over workloads. {\color{blue!50!black} That is, we attribute workload-specific performance variation to code usage rather than code coverage.}}
}
\begin{figure}
	\centering
	\includegraphics[width=0.98\linewidth]{images/configuration_coverage_corona_mandelbrot.pdf}
	\caption{Representative example of the relation between configuration-specific code coverage and performance differences.}
	\label{fig:example_coverage_performance}
\end{figure}
\end{comment}
\paragraph*{Operationalization (RQ~3)}

\begin{comment}
From the results of \RQref{3.1}, we see that at a fine granularity  (configuration-specific code), we do not observe any relationship between configuration-specific code coverage and performance variation. To see, whether such a relationship is specific to individual configuration options rather than configurations, we abstract from single configurations to configuration options and relate the performance influences from \RQref{2} with the amount of option-dependent code covered under the respective workloads.
\end{comment}
To approach the research question, we compare each option's performance influence and option-specific code coverage across pairs of workloads. 
To determine which code can be attributed to individual configuration options, we employ a two-step process. First, we obtain a baseline of all code that depends on a configuration option within the scope of our entire benchmark selection. For each workload $w \in W$, we compute the set of code lines that depend on option $o \in O$. Let $C_{o}$ be the set of configurations with option $o$ selected, and $C_{\neg o}$ with option $o$ deselected. To obtain $S_{w, o}$, we follow a strategy similar to \textit{spectrum-based feature location}~\cite{michelon_spectrum_2021} (cf. Section~\ref{sec:feature_location}) and subtract the set of the code lines covered under $C_{\neg o}$ from those of $C_{o}$:

\begin{equation}%\todo{example from Figure~\ref{fig:intro}}
	S_{w, o} = \bigcup_{p \in C_{o}} S_{w}(p) ~ \setminus ~ \bigcup_{q \in C_{\neg o}} S_{w}(q)
\end{equation}

While $S_{w, o}$ yields an approximation of option-dependent code for a single workload, we aggregate the approximations for each workload $w \in W$ to obtain the set of lines that depend on a configuration option $o$ and are executed in, at least, one workload,~$S_{o}$. 

\begin{equation}
	S_{o} = \bigcup_{w \in W} S_{w, o}
\end{equation}

While this aggregated set is not a ground truth, it enables us to reason about differences in option-dependent code within the scope of our selected workloads. That is, the expressiveness of this baseline depends on the diversity of the workloads in question. We discuss this limitation in Section~\ref{sec:threats}. From the ratio of option-dependent code per workload to option-dependent code across workloads, $\mid S_{w_1, o}\mid/~{\mid S_{w_2, o}\mid}$, we can estimate the coverage of option-dependent code. From comparing the sets $S_{w_1, o}$ and $S_{w_2, o}$ for any two workloads $w_1$ and $w_2$, we can estimate a similarity between the option-code coverage.


\paragraph*{Results}
In Figure~\ref{fig:diff_performance_option_coverage}, we show, for all combinations of two workloads, the relationship between the similarity in option-specific code coverage on the x-axis and the difference between the option's relative performance influence (from the performance models obtained for \RQref{2}). Alongside the scatter plots for each software system, we provide histograms of the marginal distributions of both dimensions. 
{\color{red}
Notably, for \batik, \dconvert and \jumper, we observe both high and low similarity among workload-specific coverage of option-code. For \htwo and \jadx, most workloads appear to cover disjoint parts of the option-specific code, whereas for \kanzi, most workloads show high similarity in the coverage of option-specific code. 
Regarding performance influence variation, \dconvert and \jadx show a clear picture. For \dconvert, variation is completely independent from option-code coverage for \dconvert. For \jadx, most performance influence variation occurs when no similar code is executed. In the light of the results from \RQref{2}, where only few options for\jadx are influential and exhibit notable variation, the majority of perormance influence variation is accounted for by these options. A trend similar to \jadx can be seen for \jumper, although it is less pronounced as the performance influence differences for a code coverage similarity of 1 are still substantial. 
The same trenda s for \jadx, but in the opposite direction, can be seen for \htwo and \batik, where performance influence variation is highest for higher option-code coverage similarities. We cannot draw a clear picture for \kanzi, where we observe little variation in option-specific code coverage.

{The results for \dconvert and \jadx are the most extreme cases that we observe. For \jadx, we can infer that option-code of the influential options is either executed or not, depending on the workload since most substantial variation is present for workload pairs that share no option-specific code coverage. For \dconvert, on the other hand, at various levels of option-code coverage similarity of pairs of workloads, we observe performance influence variation. That is, the latter is likely dominated by \textit{how} the feature code is executed.}\\
}
\greybox{\textbf{Summary} (\RQref{3.2}):
{\color{blue!50!black}
	\color{red}Across the six software systems, we observe a wide variety of workload-specific effects on performance influences and option-specific code coverage. Depending on the workload, software system, and configuration option, option-code coverage (as in the case of \jadx) or option-code utilization (as in the case of \kanzi) may explain workload-specific performance variation.
}}

\begin{figure*}

	\begin{subfigure}{0.33\textwidth}
		\centering
		\includegraphics[width=\linewidth]{images/rq3.2/batik_rq3.2.pdf}
		%\caption{\batik}
	\end{subfigure}
	\begin{subfigure}{0.33\textwidth}
		\centering
		\includegraphics[width=\linewidth]{images/rq3.2/dconvert_rq3.2.pdf}
		%\caption{\dconvert}
	\end{subfigure}
	\begin{subfigure}{0.33\textwidth}
		\centering
		\includegraphics[width=\linewidth]{images/rq3.2/h2_rq3.2.pdf}
		%\caption{\htwo}
	\end{subfigure}
	\begin{subfigure}{0.33\textwidth}
		\centering
		\includegraphics[width=\linewidth]{images/rq3.2/jump3r_rq3.2.pdf}
		%\caption{\jumper}
	\end{subfigure}
	\begin{subfigure}{0.33\textwidth}
		\centering
		\includegraphics[width=\linewidth]{images/rq3.2/jadx_rq3.2.pdf}
		%\caption{\jadx}
	\end{subfigure}
	\begin{subfigure}{0.33\textwidth}
		\centering
		\includegraphics[width=\linewidth]{images/rq3.2/kanzi_rq3.2.pdf}
		%\caption{\kanzi}
	\end{subfigure}
	\caption{Differences in performance influence/importance vs differences in option code coverage}
	\label{fig:diff_performance_option_coverage}
\end{figure*}



{\color{blue!50!black}
\section{Discussion}
\todo{summary}
\subsection{Workload-aware Performance Models}
From RQ3, we can infer that it is more decisive for performance \textit{how} the software system is executed and not necessarily \textit{what} code. Since we have made this observation for option-specific code (RQ3.1), but also for configuration-specific code (3.2), it is plausible that the workload-specific influence on how the software system is executed accounts for workload-specific performance variation. That is, in order to make performance models generalizable to varying workloads, they must incorporate workload-specific characteristics from which the workload-specific execution emerges. 
Our introductory example from Figure~\ref{fig:intro} illustrates this facet: The number of insertions passed to the method directly determines how often the method commit is executed. From our selection of workloads, a rather trivial example is that, in general, larger workloads (by file size), resulted in higher execution times. A non-trivial example from our experiments is that, for the database system \htwo, the performance distributions for the workload \texttt{TPC-C} were dissimilar under two different scalefactors (i.e., could not easily be transformed). 
That is, varying workload characteristics and incorporating these as independent variables into performance models can be a promising strategy to pursue in order to obtain representative and generalizable performance models. 

\subsection{Workload-specific Execution}
In our experiments we use statement coverage to describe \emph{what} code is executed, but cannot make statements about \emph{how} the software system is executed, and can only infer that variation in the latter most likely accounts for workload-specific performance variation. Aside from incorporating workload characteristics as features in a workload-aware performance model, performance measurement at finer granularity, such as method or statement level, is a methodology that---in retrospect---might better have fitted our analysis. Work closest to such a methodology is the white-box profiling approach by \citeauthor{weber_white_2021}~\cite{weber_white_2021}. They use fine-grained profilers to assess at method-level, which factors, either configuration options or application context (e.g., method arguments), account for a method's performance. In the light of our findings from \RQref{3.1}, we believe that fine-grained performance measurements are a worthwhile strategy to obtain performance measurements at fine enough granularity to identify code sections whose performance is influenced by the workload.

\subsection{Implications for Transfer Learning}
The results from \RQref{1} and \RQref{2} illustrate that, even among a small number of configurable software systems, the configuration-specific performance behavior across workloads as well as the number of option-specific performance influences affected by the workload can vary widely. The substantial amount of dissimilar performance distributions is potentially challenging for transfer learning approaches adapting workload-specific performance models to new workloads. Approaches like the work by ~\citeauthor{jamshidi_learning_2018} exploit similarities between workload-specific performance distributions~\cite{jamshidi_learning_2018}, such as the option influences on performance shifted by a constant term or scaled by a factor proportional to workload size (cf. Figure ~\ref{fig:intro}). While these examples represent relatively simple scenarios that are easily accountable to affine transformations, and more complex non-linear relationships could be learned, we argue that it is not evident that transfer learning is an economical or feasible option to handle differences in workload-specific performance behavior in every case. Given the prevalence of different degree sof variation (cf. Table~\ref{tab:categorization}), one cannot assume that a configurable software system and workload pair provides exploitable similarities, and learning a new workload-specific performance model or workload-aware performance model from scratch may be the better choice.

}\section{Threats to Validity}\label{sec:threats}

\paragraph*{Internal Validity}\label{sec:internal_validity}
Threats to \emph{internal validity} include measurement noise which may distort our classification into categories (Section~\ref{sec:rq1}) and model construction (Section~\ref{sec:rq2}). We mitigate this threat by repeating each experiment five times and reporting the median as a robust measure. For \htwo, we confirmed a negligible measurement variation in a separate pre-study.
Another potential threat is that the coverage analysis with \mbox{\textsc{JaCoCo}} entails a noticable instrumentation overhead, which may can distort performance observations. We mitigate this threat by separating the experiment runs for coverage assessment and performance measurement. In the case of \htwo, the load generator of the \textsc{OLTPBench} framework~\cite{difallah_oltp_2013} ran on the same machine as the database since we were testing an embedded scenario, but thus introduced only negligible variation and overhead.
\paragraph*{External Validity}\label{sec:external_validity}
The selection of subject systems all written in Java poses a threat to \emph{external validity}. While our motivation for this selection is primarily practical in nature (cf. Section~\ref{sec:setup}), we mitigate this threat by selecting subject systems from a large variety of application domains (cf.~Table~\ref{tab:subject_systems}). Nonetheless, one cannot necessarily conclude that our results hold for other domains and programming languages. 
\paragraph{Construct Validity}\label{sec:construct_validity}
The profiler \textsc{JaCoCo} instruments Java byte code instructions rather than statements, which leads to some minor imprecision when mapping covered instructions back to the original statements. Transitively, this also affects the precision of our feature location (cf. Section~\ref{sec:rq3}). This drawback, however, is not limited to this particular profiler, but applies to most profilers operating on byte code. 
Superimposition in our feature location technique is sound, but, by design, incomplete, since no set of workloads is guaranteed to cover all feature code. We selected a variety of different workloads as a counter measure, yet our study is of exploratory nature. 

{\color{blue!50!black}
\section{Conclusion}
Software performance emerges from a multitude of factors. Configuration options have been widely studied as a key factor and used for learning performance models.  When learning configuration-dependent performance models, workloads as an orthogonal factor have so far been overlooked---most approaches rely on performance observations from a single workload. This raises the question of how generalizable existing models to other workloads. 
Understanding to what extent configuration and workload---individually and combined---are responsible for the performance variation a software system is key to evaluate whether one can transfer existing performance models are to new workloads, or has to learn performance models from scratch for new workloads.

In an empirical study of 29\,014 configurations and 55 workloads, we explore both how varying the workload affects the performance behavior and execution across 6 configurable software systems. Specifically, we investigate (1) whether the workload and configuration interact and in conjunction influence performance and, if so, (2) how this interaction takes place.
We enrich our black-box performance observations with statement coverage data to understand performance variation, and especially the corresponding execution, at a fine granularity.

Our findings suggest that the execution of option-specific code segments plays no substantial role in how workloads interact with configurations. However, variation in the execution of code is most likely accountable for workload-specific performance variation. We conclude that identifying performance-relevant workload characteristics and incorporating them as independent variables is a promising avenue towards obtaining generalizable and representative performance models.

We found that the workload can have a wide variety of effects on configuration-specific performance. Non-monotonic effects can challenge the feasibility of existing transfer learning approaches for performance models. In essence, one cannot assume that an existing  worklaod-specific performance model is easily transferable to a new workload with little effort.

In further work, we will incorporate further factors of variation, such as hardware variability, but also the evolution of configurable software systems, into our analysis.%. and ultimately enable selecting representative workloads for learning robust and generalizable performance models.
}