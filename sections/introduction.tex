
%\paragraph*{\color{purple}Context}
Most modern software systems can be customized via configuration options to meet user demands. Configuration options can enable desired functionality or tweak non-functional aspects of a software system, such as improving performance or energy consumption. The relationship of configuration choices and their influence on performance has been extensively studied in the literature~~\cite{dorn2020,siegmundPerformanceinfluenceModelsHighly2015,haDeepPerf2019,perfAL,guoVariabilityawarePerformancePrediction2013,sarkarCostEfficientSamplingPerformance,guo_2018_data,fourier_learning_2015,perLasso}. The backbone of performance estimation are prediction models that map a given configuration to the estimated performance value. Learning performance models relies on a training set of configuration-specific performance measurements. In state-of-the-art approaches observations usually employ only a single workload which aims at emulating a specific real-world application scenario.

The choice of the workload (i.e., the input fed to the software system) is known to influence the performance of configurable software systems in different ways as has been shown for the domains of SAT solvers~\cite{falkner_sat_solvers_2015,satzilla_2008}, compilation~\cite{ding_compilation_2015,plotnikov_compilation_2013}, video transcoding~\cite{maxiaguine_workload_2004,alves_sampling_2020}, data compression~\cite{khavari_compression_2019}, and code verification~\cite{koc_satune_2021}. Besides apparent interactions, such as performance scaling with the size of a workload, qualitative aspects can result in more complex and non-trivial performance interactions. For instance, \citeauthor{alves_sampling_2020} have shown that the video transcoder \textsf{x264} exhibits different performance distributions depending on the video source file~\cite{alves_sampling_2020}, and \citeauthor{liao_2020_using_emse} have confirmed this for stream processing applications~\cite{liao_2020_using_emse}. As varying the workload adds another layer of complexity to modeling performance, we cannot guarantee that performance models trained with a single workload \textit{generalize} to arbitrary workloads and make meaningful estimations. 

%\paragraph*{\color{purple}Motivation}
To address this limitation, in literature, two different directions have been pursued in the literature. First, performance models trained using a specific workload can be adapted to another specific workload~\cite{jamishidi_transfer_2017,jamshidi_learning_2018,jamshidi_transfer_gp_2017}. Second,  one can specify workload characteristics as further independent variables when modeling configuration-dependent performance~\cite{koc_satune_2021}.
The first strategy direction relies on transfer learning techniques, where, given an existing performance model, in a separate step only the difference to a new environment is learned. Such a transfer function encodes which configuration optionsâ€™ influence on performance is sensitive to workload variation. While transfer learning is an effective strategy that is not limited to varying workloads~\cite{jamshidi_learning_2018}, but can also be applied to different versions~\cite{jamishidi_transfer_2017,jamshidi_transfer_gp_2017,martin_transfer_2021}, or hardware setups~\cite{ding_bayesian_2020}, its main limitation is that the transfer function is specific to a difference of two environments.

In contrast to transfer learning, a more generalist approach is to consider the input fed to a software system as a further dimension for modeling performance. Here, a workload can be characterized by properties that---individually or in conjunction with software configuration options---influence performance. For such a strategy to work, one requires knowledge of the characteristics of a workload that influence performance. This strategy has been effectively tested for a  variety of application-domains, such as program verification~\cite{koc_satune_2021}. {\color{purple}\todo{\footnotesize the larger problem is that we often do not find or have at hand characteristics to model}However, the added complexity comes at significant cost. Besides the additional step of domain-specific workload characterization, screening the inflated configuration-workload problem space for relevant features incurs more measurements required to learn accurate performance models. }

%\paragraph*{\color{purple}Problem}
The existing body of research reflects the prevalence and importance of the workload influence on software systems~\cite{khavari_compression_2019,maxiaguine_workload_2004,plotnikov_compilation_2013,ding_compilation_2015,falkner_sat_solvers_2015,satzilla_2008,alves_sampling_2020}. {\color{purple}\todo{Aussage nicht ganz klar}All these approaches weed out configuration options that are insensitive to workload variation, so little is known about the quality and driving factors of the interplay between configuration options and workloads}. Our understanding of this cross-factor relationship lacks knowledge of the following aspects:

{\color{purple}\todo{Erklaeren sich noch nicht ganz so gut aus dem TExt zuvor}
\begin{compactitem}
	\item How difficult or complex is the transformation of a performance model to another workload? 
	\item What drives complexity in such transformations? 
	\item Are few or many configuration options sensitive to varying the workload?
	\item What are the driving factors of the interplay between configuration options? 
	\item Do some workloads cover feature code less exhautively than others, do workloads use feature code in different ways?
\end{compactitem}
}
To answer these questions, we have conducted a systematic study that sheds light on whether and how configuration options and workload choices interact with regard to performance. 
Specifically, we apply 29\,347 configurations and 55 workloads across six configurable software systems to obtain a broad picture of the interaction of configuration and workload when learning performance models and estimating a configuration's performance (i.e., response time). Aside from studying the sole effects of workload variation on performance behavior, we explore possible driving factors. To this end, we enrich performance observations with corresponding statement coverage data to understand workload variation at finer granularity.

Our findings suggest that {\color{red} \ldots. Our categorization can help practitioners and researchers to obtain representative performance models more efficiently as we provide a set of scenarios to test for.}

To summarize, we make the following contributions: 

\begin{compactitem}
	\item An empirical study of 29\,347 configurations and 55 workloads across six configurable software systems on whether and how the interaction of workloads with configuration options influences performance;
	
	\item {An catalogue of \color{red}X interaction scenarios alongside possible explanations on how to possibly detect them and incorporate them into modeling configuration-dependent performance;}
	
	\item A companion Web site\footnote{\url{https://add.url.here}} with supplementary material including performance and coverage measurements, experiment workloads and configurations, as well as additional visualizations left out due to space limitations.
\end{compactitem}


