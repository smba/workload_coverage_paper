\documentclass[10pt,conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.\usepackage{}

\input{preamble.tex}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
		T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\usepackage{blindtext}
\usepackage{cochineal}

\begin{document}
	
	\title{Analyzing the Impact of Workloads on Modeling the Performance of Configurable Software Systems (Reproduction Package)}
	
	\author{
		\IEEEauthorblockN{
			Stefan Mühlbauer
		}
		\IEEEauthorblockA{
			Leipzig University,\\
			Germany\\
			~
		}\\
		
		\IEEEauthorblockN{
			Johannes Dorn
		}
		\IEEEauthorblockA{
			Leipzig University,\\
			Germany
		}
		
		\and
		
		\IEEEauthorblockN{
			Florian Sattler
		}
		\IEEEauthorblockA{
			Saarland University,\\
			Saarland Informatics Campus, \\
			Germany
		}\\
		
		\IEEEauthorblockN{
			Sven Apel
		}
		\IEEEauthorblockA{
			Saarland University,\\
			Saarland Informatics Campus, \\
			Germany
		}
		
		\and
		
		\IEEEauthorblockN{
			Christian Kaltenecker\\
		}
		\IEEEauthorblockA{
			Saarland University,\\
			Saarland Informatics Campus, \\
			Germany
		}\\
		
		\IEEEauthorblockN{
			Norbert Siegmund
		}
		\IEEEauthorblockA{
			Leipzig University,\\
			Germany
		}
	}
	
	\author{
		\IEEEauthorblockN{
			Stefan Mühlbauer\IEEEauthorrefmark{1},
			Florian Sattler\IEEEauthorrefmark{2}, 
			Christian Kaltenecker\IEEEauthorrefmark{2},
			Johannes Dorn\IEEEauthorrefmark{1},
			Sven Apel\IEEEauthorrefmark{2} and
			Norbert Siegmund\IEEEauthorrefmark{1}
		}
		\IEEEauthorblockA{
			\IEEEauthorrefmark{1}Leipzig University, Germany,
			\IEEEauthorrefmark{2}Saarland University and Saarland Informatics Campus, Germany,
		}
	}
	
	\maketitle
	
	%\begin{IEEEkeywords}
	%	component, formatting, style, styling, insert
	%\end{IEEEkeywords}
	
	\maketitle
	\setcounter{tocdepth}{4}
	
	% abstract
	\begin{abstract}¸
	The artifacts described in this document refer to the study ``\textit{Analyzing the Impact of Workloads on Modeling the Performance of Configurable Software Systems}'' which explores how configuration options in conjunction with the workload can influence software performance.
	
	While the study presents representative selection of the results, in this document, we provide an overview of the experimental setup and complete available data set (software configurations, performance and coverage measurements). In addition, we provide an interactive dashboard for reproduction of analyses and visualizations. We make available all material via an archived software repository on \textit{figshare.com}.
	\end{abstract}

	\section{Research Overview}
	Software systems can be customized through configuration options to enable desired functionality or improve non-functional aspects, such as performance or energy consumption. To estimate the performance of such systems, prediction models map a given configuration to an estimated performance value~\cite{kaltenecker_interplay_2020}. These models are based on a training set of configuration-specific performance measurements, which usually rely on only a single workload. However, previous work shows that the choice of workload, besides configuration, can significantly influence the performance of software systems in various ways~\cite{alves_sampling_2020,lesoil_2021}.  
	
	We conduct a systematic study that explores the quality and driving factors of the interaction between configuration options and workloads and their impact on software performance~\cite{muhlbauer_workload_2023}. In total, we analyze 25\,258 configurations and and multiple workloads across nine real-world configurable software systems (\textsc{Java} and \textsc{C}/\textsc{C++}). We augment performance observations with code coverage information to explore driving causes for interactions between the workload and configuration options regarding their effect on software performance. 
	
	The study demonstrates that workloads can interact with configuration-specific performance, often in non-monotonous ways, which severely limits prediction models generalizability to arbitrary workloads. In the study, such interactions are barely uniform, but rather specific configuration options are sensitive to the workload. We identify workload-specific code coverage (i.e., code missed or covered only  under specific workloads) as a partial cause for workload sensitivity.
	
	\section{Artifacts}
	Because the original study presents a representative selection of results~\cite{muhlbauer_workload_2023} and, to enable \textit{repoducibility} of our findings, we provide study artifacts (experimental data and an interactive dashboard for analyses and visualizations) via an archived software repository\footnote{\url{https://foo.bar}}.

	\subsection{Experiment Data}
	The experimental data provided comprises the raw measurements (performance and code coverage) as well as resources (variability models, configurations, and workloads) used.
	
\subsubsection{Software Systems and Configurations}
	We have selected for the study nine configurable software systems, of which we provide source code and build instructions. Each study subject exhibits configuration options along with constraints that we captured and provide as variability models in DIMACS (conjunctive normal form) notation. Configuration samples are drawn for each subject system using an ensemble of sampling strategies from our sampling library \textsc{SPLConqueror}\footnote{\url{https://www.se.cs.uni-saarland.de/projects/splconqueror/}} and provided as CSV files.
	
	\subsubsection{Workloads}
	Along with different configurations, we selected workloads for each subject system. Generally, we selected workloads that are inputs fed to and processed by the respective subject system (e.g., different files to compress or vector graphics to rasterize). We make available these resources via the companion repository. {\color{orange}Where redistribution is not possible due to licensing (e.g., proprietary images or apps), we provide proper references}. 
	
	\subsubsection{Performance Measurements}
	For all data points (pairs of software configuration and workloads) we assess performance using either the utility \textsc{GNU time} (execution time) or \textsc{OLTPBench}~\cite{difallah_oltp_2013} (throughput). All measurements were orchestrated using the workload manager \textsc{Slurm}\footnote{https://slurm} and conducted on compute clusters with a headless \textsc{Debian} installation and identical hardware setup across each cluster, respectively. We provide all performance observations as CSV files via the companion repository.
	
	\subsubsection{Code Coverage Measurements}
	For all data points, we conducted a separate run to collect code coverage information. For \textsc{Java} subject systems, we employed the on-the-fly profiler \textsc{JaCoCo}\footnote{https://jacoco}, whereas for \textsc{C/C++} systems, we collected coverage information from executing binary build with instrumentation code using LLVM\footnote{https://llvm-cov}. We provide all line coverage information (per data point) via the companion repository.
		
	\subsection{Interactive Dashboard}
	
	%\clearpage
	%\color{black}
	\bibliographystyle{IEEEtran}
	\bibliography{literature}
\end{document}
